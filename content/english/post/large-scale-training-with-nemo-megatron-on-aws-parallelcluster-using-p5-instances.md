---
title: "Large scale training with NeMo Megatron on AWS ParallelCluster using P5 instances"
date: 2024-05-29T00:00:00-0700
# post thumb
images:
    - "images/post/Large-scale-training-with-NeMo-Megatron-on-AWS-ParallelCluster-using-P5-instances-1120x630.png"
#author
author: "Matt Vaughn"
# description
description: " (reposted from AWS HPC Blog)"
# Taxonomies
categories: [ "AWS ParallelCluster", ]
tags: [ "Slurm",  "Machine Learning",  "HPC",  "Artificial Intelligence",  "ParallelCluster",  "Compute",  "hpcblog", ]
type: "regular" # available type (regular or featured)
draft: false
---

This post was contributed by Akshit Arora (NVIDIA), Peter Dykas (NVIDIA), Aman Shanbhag (AWS), Sean Smith (AWS), Pierre-Yves (AWS) Today we’ll take you on a step-by-step guide to help you to create a cluster of p5.48xlarge instances, using AWS ParallelCluster to launch GPT training through the NeMo Megatron framework, using Slurm. We’ve put detailed information […]

<a href="https://aws.amazon.com/blogs/hpc/large-scale-training-with-nemo-megatron-on-aws-parallelcluster-using-p5-instances/" class="btn btn-primary btn-lg active" role="button" aria-pressed="true" style="margin-top: 8px;">Read the Post on the AWS Blog Channel</a>