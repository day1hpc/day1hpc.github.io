---
title: "Large scale training with NVIDIA NeMo Megatron on AWS ParallelCluster using P5 instances"
date: 2024-05-29T00:00:00-0700
# post thumb
images:
    - "images/post/Large-scale-training-with-NeMo-Megatron-on-AWS-ParallelCluster-using-P5-instances-1-1120x630.png"
#author
author: "Matt Vaughn"
# description
description: " (reposted from AWS HPC Blog)"
# Taxonomies
categories: [ "AWS ParallelCluster", ]
tags: [ "Compute",  "Artificial Intelligence",  "Slurm",  "HPC",  "Machine Learning",  "ParallelCluster",  "hpcblog", ]
type: "regular" # available type (regular or featured)
draft: false
---

Launching distributed GPT training? See how AWS ParallelCluster sets up a fast shared filesystem, SSH keys, host files, and more between nodes. Our guide has the details for creating a Slurm-managed cluster to train NeMo Megatron at scale.

<a href="https://aws.amazon.com/blogs/hpc/large-scale-training-with-nemo-megatron-on-aws-parallelcluster-using-p5-instances/" class="btn btn-primary btn-lg active" role="button" aria-pressed="true" style="margin-top: 8px;">Read the Post on the AWS Blog Channel</a>