---
title: "Scaling your LLM inference workloads: multi-node deployment with TensorRT-LLM and Triton on Amazon EKS"
date: 2024-12-02T00:00:00-0800
# post thumb
images:
    - "images/post/Scaling-your-LLM-inference-workloads-multi-node-deployment-with-TensorRT-LLM-and-Triton-on-Amazon-EKS-1120x630.png"
#author
author: "Matt Vaughn"
# description
description: " (reposted from AWS HPC Blog)"
# Taxonomies
categories: []
tags: [ "Modeling",  "Artificial Intelligence",  "Machine Learning",  "Technical How-to",  "HPC",  "hpcblog", ]
type: "regular" # available type (regular or featured)
draft: false
---

LLMs are scaling exponentially. Learn how advanced technologies like Triton, TRT-LLM and EKS enable seamless deployment of models like the 405B parameter Llama 3.1. Letâ€™s go large.

<a href="https://aws.amazon.com/blogs/hpc/scaling-your-llm-inference-workloads-multi-node-deployment-with-tensorrt-llm-and-triton-on-amazon-eks/" class="btn btn-primary btn-lg active" role="button" aria-pressed="true" style="margin-top: 8px;">Read the Post on the AWS Blog Channel</a>