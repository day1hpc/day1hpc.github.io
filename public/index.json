

















































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































[{"categories":["AWS ParallelCluster","Amazon Elastic Fabric Adapter","Amazon NICE DCV","Life Sciences"],"contents":"CryoEM is an awesome research tool, but it also comes with some challenges. The software stack is complex, and the hardware it needs is sometimes exotic and hard to get. Afterwards, there\u0026rsquo;s a massive dataset and a serious set of challenges to moving it around, and managing it. (No one wants to get stuck baby sitting someone else\u0026rsquo;s petabyte dataset unless they have to).\nThe team at KEK in Japan took all these challenges head on and solved them in a very different way (spoiler: it involved the cloud). But as is often the case, when teams in Japan solve a big HPC problem, it\u0026rsquo;s usually something that\u0026rsquo;s going to get picked up around the world. And KEK are very much those kinds of people.\nToday is just one part in a series of Four HPC Tech Shorts dedicated to the work KEK did (yes, it\u0026rsquo;s that awesome).\nPart 1 - describes the problem, and the extra challenge of doing work in a world of lock downs due to Covid. Part 2 - describes the software and infrastructure solution they came up with, and shows you how it works. Part 3 - discusses the impact this had on their operations, and its special role in forming a distributed CryoEM network across the whole country. A mesh of centers sharing knowledge, resources, and talent - really efficiently. Part4 - the final episode is a deep dive into the arsenal of benchmark data they’ve amassed, which the CryoEM junkies watching this channel can use to make their own decisions about what’s going to be the best CPU and GPU combinations to use for your workloads, at the resolutions you’re working at.\nLike we said: there\u0026rsquo;s a ton of stuff for us all to learn from our friends at KEK in Japan.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"October 20, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dDVZUhXrjHk_hu246b84a59c68ceb0068dbe2789d6fbb9_15121_460x200_fill_box_smart1_3.png","permalink":"/post/how-kek-changed-how-everyone-in-japan-does-cryoem-part-3-of-4.html","title":"How KEK changed how everyone in Japan does CryoEM (Part 3 of 4)"},{"categories":["AWS ParallelCluster","Amazon Elastic Fabric Adapter","Amazon NICE DCV","Life Sciences"],"contents":"CryoEM is an awesome research tool, but it also comes with some challenges. The software stack is complex, and the hardware it needs is sometimes exotic and hard to get. Afterwards, there\u0026rsquo;s a massive dataset and a serious set of challenges to moving it around, and managing it. (No one wants to get stuck baby sitting someone else\u0026rsquo;s petabyte dataset unless they have to).\nThe team at KEK in Japan took all these challenges head on and solved them in a very different way (spoiler: it involved the cloud). But as is often the case, when teams in Japan solve a big HPC problem, it\u0026rsquo;s usually something that\u0026rsquo;s going to get picked up around the world. And KEK are very much those kinds of people.\nToday is just one part in a series of Four HPC Tech Shorts dedicated to the work KEK did (yes, it\u0026rsquo;s that awesome).\nPart 1 - describes the problem, and the extra challenge of doing work in a world of lock downs due to Covid. Part 2 - describes the software and infrastructure solution they came up with, and shows you how it works. Part 3 - discusses the impact this had on their operations, and its special role in forming a distributed CryoEM network across the whole country. A mesh of centers sharing knowledge, resources, and talent - really efficiently. Part4 - the final episode is a deep dive into the arsenal of benchmark data they’ve amassed, which the CryoEM junkies watching this channel can use to make their own decisions about what’s going to be the best CPU and GPU combinations to use for your workloads, at the resolutions you’re working at.\nLike we said: there\u0026rsquo;s a ton of stuff for us all to learn from our friends at KEK in Japan.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"October 20, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/0Sf_N5eIt-c_hu49e2e0334b26f2b7b700f061ae3679a0_16299_460x200_fill_box_smart1_3.png","permalink":"/post/keks-arsenal-of-cryoem-benchmark-data-a-detailed-walk-through-part-4-of-4.html","title":"KEK's arsenal of CryoEM benchmark data - a detailed walk through (Part 4 of 4)"},{"categories":null,"contents":"Welcome to Day 1 Welcome to day1hpc.com - a community site built and curated by the Developer Relations team in HPC Engineering at AWS. Our job is to be the interface between our HPC engineering teams and the people in the HPC community who want to use AWS to create powerful tools for solving hard problems.\nTo do that, we\u0026rsquo;ll spend a lot of our time explaining how these services work, and getting your feedback so we can improve them, continuously. You’ll find a lot of things on this site to help you understand AWS a little better, all with the aim of getting you where to want to go, faster.\nWe called the site Day1Hpc for a couple of reasons. One is to pay homage to our employer (Amazon Web Services) where Day 1 means \u0026ldquo;focusing on customers, creating long term value over short-term corporate profit, and making many bold bets\u0026rdquo;. It\u0026rsquo;s the patient capital (human and otherwise) that has allowed us to experiment with a lot of ideas for solving problems in different ways.\nThe other reason is that it\u0026rsquo;s still Day 1 for HPC in the cloud. We see endless possibilities to improve the productivity of every scientist or engineer who uses HPC as a tool in their exploration. And we\u0026rsquo;re optimistic about where that\u0026rsquo;s going to lead us all.\nIf you see any of us at a conference, please say ‘hi’ and let us know how we can help.\nYou can find us on Twitter at @TechHpc, on YouTube in the HPC Tech Shorts Channel, or blogging to the world from the AWS HPC Blog Channel. Please reach out and let us know what you’re finding hard, or easy.\n\u0026ndash; Boof, Angel, Matt\nWe\u0026rsquo;re hiring So if you want to know what it\u0026rsquo;s like to work in our engineering teams, take it from the people who are alreasy here \u0026hellip;\n","date":"October 17, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/welcomeday1-ident-large_hu55207aec9ed68bb5ddf84e22b0bccde4_538643_460x200_fill_box_smart1_3.png","permalink":"/post/about.html","title":"About Us"},{"categories":["products"],"contents":" AWS Batch AWS Batch is an always-on scheduler that lets you easily and efficiently run thousands of container jobs. Workflow builders love it for scaling their workloads, from machine learning to genomics. It scales from one job to millions and takes away the chore of spinning up fleets of compute instances and keeping them busy.\nBatch is a different way of running workloads compared to \u0026rsquo;normal\u0026rsquo; HPC practices you might be used to. Before you dive in on batch, it\u0026rsquo;s worth comparing it to ParallelCluster. Angel wrote a great blog post about choosing between AWS Batch or AWS ParallelCluster.\nMajor features you\u0026rsquo;ll want to know about Introducing fair-share scheduling for AWS Batch New console features including container insights in AWS Batch Batch can now use Fargate for a truly serverless experience AWS Batch\u0026amp;rsquo;s new Faster Scaling features Introducing support for per-job Amazon EFS volumes in AWS Batch Fair share scheduling to maximize user happiness in AWS Batch Announcements BioContainers are now available in Amazon ECR Public Gallery Workflow engines love Batch If you\u0026rsquo;re working with Nextflow natively, then you\u0026rsquo;ll probably love finding out about the AWS Genomics CLI which does pretty much all the boring set up work for you and sets you up for running Nextflow piplines in around half an hour (from a standing start).\nGenomics Workflows on AWS - Nextflow Nextflow\u0026#39;s getting started guide for AWS Batch Nextflow\u0026#39;s Summit in 2022 - lots of great talks. If you\u0026rsquo;re working with Nextflow natively, then you\u0026rsquo;ll probably love finding out about the AWS Genomics CLI which does pretty much all the boring set up work for you and sets you up for running Nextflow piplines in around half an hour (from a standing start).\nGenomics Workflows on AWS - Cromwell Cromwell on AWS - blog post by Mark Schreiber about an improved integration. Metaflow\u0026#39;s Season 2 blockbuster on AWS Batch Skills AWS Batch Dos and Don’ts: Best Practices in a Nutshell\nWhat\u0026#39;s the difference between canceling and terminating a job in AWS Batch?\nRearchitecting AWS Batch managed services to leverage AWS Fargate\nUnderstanding the AWS Batch termination process\nUsing AWS Batch Console Support for Step Functions Workflows\nAWS Batch updates: higher compute utilization, AWS PrivateLink support, and updatable compute environments\nEncoding workflow dependencies in AWS Batch\nUse cases Optimize Protein Folding Costs with OpenFold on AWS Batch Protein folding in the cloud - a protein primer with Brian Loyal AlphaFold vs OpenFold - accelerating time to result in protein folding Analyzing Genomic Data using Amazon Genomics CLI and Amazon SageMaker Accelerating drug discovery with Amazon EC2 Spot Instances Running 20k simulations in 3 days to accelerate early stage drug discovery with AWS Batch miniWDL workflows with 100% cloud elasticity, and no DevOps geekery Nextflow Tower and how it makes it easy to manage a lot of infrastructure quickly. Genomics workflow set made easy with AWS Genomics CLI Getting Started with NVIDIA Clara Parabricks on AWS Batch using AWS CloudFormation Efficient and cost-effective rendering pipelines with Blender and AWS Batch Data Science workflows at insitro: how redun uses the advanced service features from AWS Batch and AWS Glue\nData Science workflows at insitro: using redun on AWS Batch\nBayesian ML Models at Scale with AWS Batch [Blog Post] || [Video] - with the data science team from Ampersand in New York.\nScalable and Cost-Effective Batch Processing for ML workloads with AWS Batch and Amazon FSx\nOptimize your Monte Carlo simulations using AWS Batch ","date":"October 17, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/batch-ident-large_hue6c0cca57bdd9e63bc5cba55f2a6b672_681693_460x200_fill_box_smart1_3.png","permalink":"/batch.html","title":"AWS Batch"},{"categories":["EFA"],"contents":" Elastic Fabric Adapter (EFA) The AWS Elastic Fabric Adapter (EFA) is a high-performance networking interface for Amazon EC2 instances that enables you to run applications requiring high levels of inter-node communications at scale on AWS.\nIt\u0026rsquo;s natively supported by Intel MPI, Open MPI, MVAPICH, and NCCL and is available on across CPU and GPU architectures including our own AWS Graviton processors.\nHow does it work? Amazon EC2 compute infrastructure is very much not like a ‘normal’ supercomputer (whatever that is). We don’t start with a blank page every few years and design the next big system. It’s a little more like a city where we build on what’s there already, renovate occasionally, and push for bigger and better and faster, while keeping the lights on and the traffic flowing at all times.\nWhile this leads to different design decisions, it also leads to interesting discoveries. It turns out that most HPC codes are more sensitive to networks that can deliver large amounts of data reliability and quickly between communicators. And they\u0026rsquo;re far less sensitive to individual packet latency than we all thought.\nEFA is a great example of how we\u0026rsquo;ve been able to keep packets flowing and solve some complex problems HPC users face in a novel way - without losing any performance for HPC and ML applications.\nDeeper Dive EFA-enabled Amazon EC2 instances - there are dozens of instances types that now support EFA. That gives you a lot of options to customize a cluster queue specifically for your workloads. A paper at IEEE Spectrum about EFA and SRD - a deep dive into the design decisions and characteristics of EFA and SRD. EFA manual setup guide - if you need to install the EFA stack from scratch yourself, this guide will help you. There\u0026#39;s more than one way to build a network - blog post explaining many of the EFA design decisions. EFA has gone mainstream - Most new Amazon EC2 instance families carry EFA-enabled interfaces. Deep dive on EFA and SRD - 36 mins - A deep dive with one of our Principal Engineers into the thought process and design decisions that lead to EFA. Speeds\u0026#39;n\u0026#39;Feeds Event - 10-mins - a fast-paced EFA update from January \u0026lsquo;22 on all the latest news and developments in EFA and - especially - new instance support. NCCL on EFA - 15m - An insight into how machine learning workloads are supported on EFA from the engineering team who support the libfabric interface to NCCL. EFA section of hpcworkshops.com - the same workshops delivered by our Solution Architects every year at SuperComputing and ISC. How to make sure you\u0026#39;re job is using EFA - How to debug scenarios where you\u0026rsquo;re not sure if your MPI is running over EFA or TCP. The performance difference can be dramatic, so these simple steps will help you figure it out. EFA Performance Customers tell us their workloads scale on EFA the same way they do on-site using traditional interconnects. We see the same thing in the lab, too.\nThere are a number of performance studies studies covering a wide range of HPC codes. To stay in touch with this side of what we do, we suggest following HPC Tech Shorts and our HPC Blog Channel.\nNumerical Weather Prediction Large-scale CFD fire simulations for Amazon.com Scaling CFD by breaking down a workflow, to speed it up Simcenter STAR-CCM\u0026#43; GROMACS performance optimizations The impact of network conditions on application performance is complicated EFA in TV broadcasting - an unexpected use case In HPC, we\u0026rsquo;re used to the tools and techniques we create flowing into the rest of the industry, solving lots of problems once thought too hard, and unlocking new possibilities for everyone. This is what happened when the team thaty looks after Holywood asked us for help with solving a networking problem.\nThis story will take you into the world of broadcast video, andf explains why we have EFA enabled on some smaller than normal instance sizes. It started with some difficult problems presented to us by customers in the entertainment industry, and led to an invention called the Cloud Digital Interface (CDI).\nHow EFA enables uncompressed live video for TV broadcast What supercomputers, scientists and TV stations have in common - 13min - background on why this problem exists in broadcast. ","date":"October 17, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/efa-ident-large_hu621fd3d67d9e9f55b880dd262bde5d3e_704646_460x200_fill_box_smart1_3.png","permalink":"/efa.html","title":"AWS Elastic Fabric Adapter"},{"categories":["AWS ParallelCluster"],"contents":" AWS ParallelCluster ParallelCluster is a tool for creating and managing HPC clusters on AWS, whether they\u0026rsquo;re powered by x86 CPUs, AWS Gravitons (our Arm-based processors), or powerful GPUs for machine-learning applications.\nParallelCluster brings together advanced compute, storage and networking into a single console. It adds great features like desktop visualization (with NICE DCV) for any cluster, and integrates deeply with Slurm to provide cloud-native scaling for all your applications.\nParallelCluster has a graphical interface (and a CLI) to model and control all the resources you need for your HPC applications, and an API you can use for sophisticated management of workflows.\nDeployment There are two ways to get ParallelCluster::\nWe suggest using PCluster Manager, which is a visual interface for designing and deploying your clusters. PCluster Manager has great features that help you integrate fast filesytems (like Lustre), get one-click desktops connections using NICE DCV, and controlling your spend using Slurm Accounting. PCluster Manager can be deployed to your AWS account very quickly, by following the tutorial here.\nIf you\u0026rsquo;re familiar with AWS already, and just want the CLI, you can install it on almost any computer with python installed. The procedure is documented here, but pretty much starts with:\n$ pip3 install \u0026#34;aws-parallelcluster\u0026lt;3.0\u0026#34; --upgrade --user Learning about ParallelCluster First, you should make sure you\u0026rsquo;re comfortable that ParallelCluster is the right path for your workloads in AWS. There\u0026rsquo;s a great blog post which helps to explain the difference between AWS Batch or AWS ParallelCluster.\nIntroducing AWS ParallelCluster 3 - blog ParallelCluster 3 - built by customers - 17min - Tech Short video with the product team, talking about all the new features of ParallelCluster v3. PCluster Manager - A new GUI for building and managing clusters - a tour through the management console for ParallelCluster. ParallelCluster 3\u0026#39;s config file - a look at the new config file syntax as an example of infrastructure as code. Introducing AWS ParallelCluster 3 - a blog describing this new major version of ParallelCluster and it\u0026rsquo;s main features. Multiuser support via Active Directory Custom AMIs in ParallelCluster 3 Expanded filesystems support in v3.2 Slurm-based memory-aware scheduling Scheduler Migration Migrating between schedulers doesn\u0026rsquo;t need to be hard. Since SGE is no longer supported by the community, we worked with the team at SchedMD to create a tutorial series to help everyone understand what tools are available in Slurm, to make the shift easier.\nEasing your migration from SGE to Slurm in AWS ParallelCluster 3 - a blog post kicking off the tutorial with several handy cheat sheets.\nTech Shorts series in 4-parts - hands on tutorials with examples, delivery by experts from SchedMD and AWS.\nCommand Line Tools || Job Scripts || Array Jobs || Slurm Accounting\nMigration for legacy ParallelCluster v2 customers Infrastructure as code - Understand ParallelCluster 3\u0026#39;s config Migrating to AWS ParallelCluster v3 – updated CLI interactions ParallelCluster 3 Configuration Converter Use cases ParallelCluster delivers you a canonical Beowulf cluster experience on AWS, but with added twists like elasticity, and support for fast storage and networking, built-in by design. That means you can run virtually any workload you like on AWS and expect to see great results.\nThe tabs here have videos and blog posts describing different use-cases for ParallelCluster.\nRunning large-scale CFD fire simulations on AWS for Amazon.com Cost-optimization on Spot Instances using checkpoint for Ansys LS-DYNA How Thermo Fisher Scientific Accelerated Cryo-EM using AWS ParallelCluster Running cost-effective GROMACS simulations using Amazon EC2 Spot Instances with AWS ParallelCluster Quantum Chemistry Calculation with FHI-aims code on AWS Running the Harmonie numerical weather prediction model on AWS Supporting climate model simulations to accelerate climate science Numerical weather prediction on AWS Graviton2 Simulating 44-Qubit quantum circuits using AWS ParallelCluster ","date":"October 17, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/pc-ident-large_hu4f22fb1266d6f8882a39a1725cd62c7f_767743_460x200_fill_box_smart1_3.png","permalink":"/parallelcluster.html","title":"AWS ParallelCluster"},{"categories":["products"],"contents":" NICE DCV NICE DCV is a high performance, low latency pixel streaming protocol that encrypts and transports pixels to devices, putting your desktop closer to your results. You can access, manipulate, and share business-critical information, regardless of your location, over local networks or the internet.\nDCV makes very frugal use of scarce bandwidth, because it\u0026rsquo;s super-lean, uses data-compression techniques and has always quickly adopted cutting-edge technologies (this is HPC, after all, we leave nothing on the table when it came to exploiting new gadgets).\nA desktop \u0026hellip; in a web browser? DCV also has a Web SDK which means you can offer your end users virtual desktops embedded in browser tabs (check out the talk from Netflix about how they did just that for their global network of editors).\nAll of this is what makes DCV a light-weight visualization package that can stream pixels over almost any network. So lean, in fact, that with reasonable bandwidth most users can\u0026rsquo;t tell that the data and the server are hundreds, or sometimes thousands of miles away.\nWhat\u0026rsquo;s behind DCV? DCV is another example of great tech that was born in HPC, but has expanded to a lot of places we never imagined - including desktop gaming.\nPushing pixels with NICE DCV - a short post explaining where DCV came from and how it solves some hard problems in novel ways. Supercomputing visualization good enough for the most demanding gamers - 25min - an insight into some of DCV\u0026rsquo;s tricks (like the QUIC transport layer) that minimize jitter and make the remote desktop feel like it\u0026rsquo;s local. It worked so well, it\u0026rsquo;s now used for gaming. An introduction to DCV - 25min - What is DCV, and how is it availble? What\u0026rsquo;s the impact of DCV's pixel streaming on my AWS Bill? - [Video 23min] || [Blog Post] - it turns out to be small, but in this talk, we walk through all the possibilities and use cases, so you know how what to expect. HPC use for DCV PCluster Manager - A new GUI for building and managing clusters - 20min - a chat with Sean Smith showing us how to use PCluster Manager to design and build a cluster. How Netflix used DCV (and AWS) to distribute their creative workforce (and saved our sanity) - 26min - a talk from Michelle Brenner of Netflix describing how DCV is embedded in their video production workflows (which we all benefit from). DCV in gaming Most of the techniques that DCV uses to minimize the effects of internet jitter and unpredictable latency, work really well for cloud gaming, too.\nPushing pixels with NICE DCV - a short post explaining where DCV came from and how it solves some hard problems in novel ways. Supercomputing visualization good enough for the most demanding gamers - 25min - an insight into some of DCV\u0026rsquo;s tricks (like the QUIC transport layer) that minimize jitter and make the remote desktop feel like it\u0026rsquo;s local. It worked so well, it\u0026rsquo;s now used for gaming. Getting started with DCV It\u0026rsquo;s easy to get started with DCV:\nDCV Preconfigured AMI - this AMI is in AWS Marketplace, and is available for Windows and Linux server versions - includiong options for G4 and G5 Amazon EC2 instances, which are GPU enabled. Preconfigured DCV EC2 instance with CloudFormation - you can launch using an AWS CloudFormation template, which we provide. Install DCV Manually - you can install manually, which makes more sense if you\u0026rsquo;re plannign to use DCV in a more taylored configuration. ","date":"October 17, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/DCV_hua3a206b2264587e0127eeeb415a17fe0_699679_460x200_fill_box_smart1_3.png","permalink":"/nice-dcv.html","title":"NICE DCV"},{"categories":["AWS ParallelCluster","Amazon Elastic Fabric Adapter","Amazon NICE DCV","Life Sciences"],"contents":"CryoEM is an awesome research tool, but it also comes with some challenges. The software stack is complex, and the hardware it needs is sometimes exotic and hard to get. Afterwards, there\u0026rsquo;s a massive dataset and a serious set of challenges to moving it around, and managing it. (No one wants to get stuck baby sitting someone else\u0026rsquo;s petabyte dataset unless they have to).\nThe team at KEK in Japan took all these challenges head on and solved them in a very different way (spoiler: it involved the cloud). But as is often the case, when teams in Japan solve a big HPC problem, it\u0026rsquo;s usually something that\u0026rsquo;s going to get picked up around the world. And KEK are very much those kinds of people.\nToday is just one part in a series of Four HPC Tech Shorts dedicated to the work KEK did (yes, it\u0026rsquo;s that awesome).\nPart 1 - describes the problem, and the extra challenge of doing work in a world of lock downs due to Covid. Part 2 - describes the software and infrastructure solution they came up with, and shows you how it works. Part 3 - discusses the impact this had on their operations, and its special role in forming a distributed CryoEM network across the whole country. A mesh of centers sharing knowledge, resources, and talent - really efficiently. Part4 - the final episode is a deep dive into the arsenal of benchmark data they’ve amassed, which the CryoEM junkies watching this channel can use to make their own decisions about what’s going to be the best CPU and GPU combinations to use for your workloads, at the resolutions you’re working at.\nLike we said: there\u0026rsquo;s a ton of stuff for us all to learn from our friends at KEK in Japan.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"October 11, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/rdF1AzfadOY_hu58ae954f5290b8d57d7ccf9fc4b16a96_15839_460x200_fill_box_smart1_3.png","permalink":"/post/keks-novel-solution-for-cryoems-software-and-infra-part-2-of-4.html","title":"KEK's novel solution for CryoEM's software and infra (Part 2 of 4)"},{"categories":["AWS ParallelCluster","Amazon Elastic Fabric Adapter","Amazon NICE DCV","Life Sciences"],"contents":"CryoEM is an awesome research tool, but it also comes with some challenges. The software stack is complex, and the hardware it needs is sometimes exotic and hard to get. Afterwards, there\u0026rsquo;s a massive dataset and a serious set of challenges to moving it around, and managing it. (No one wants to get stuck baby sitting someone else\u0026rsquo;s petabyte dataset unless they have to).\nThe team at KEK in Japan took all these challenges head on and solved them in a very different way (spoiler: it involved the cloud). But as is often the case, when teams in Japan solve a big HPC problem, it\u0026rsquo;s usually something that\u0026rsquo;s going to get picked up around the world. And KEK are very much those kinds of people.\nToday is one part in a series of Four HPC Tech Shorts dedicated to the work KEK did (yes, it\u0026rsquo;s that awesome).\nPart 1 - describes the problem, and the extra challenge of doing work in a world of lock downs due to Covid. Part 2 - describes the software and infrastructure solution they came up with, and shows you how it works. Part 3 - discusses the impact this had on their operations, and its special role in forming a distributed CryoEM network across the whole country. A mesh of centers sharing knowledge, resources, and talent - really efficiently. Part4 - the final episode is a deep dive into the arsenal of benchmark data they’ve amassed, which the CryoEM junkies watching this channel can use to make their own decisions about what’s going to be the best CPU and GPU combinations to use for your workloads, at the resolutions you’re working at.\nLike we said: there\u0026rsquo;s a ton of stuff for us all to learn from our friends at KEK in Japan.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"October 6, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/u3oFHPFOL68_hu699ecd3c1d7a972a15623813d7750e4d_16264_460x200_fill_box_smart1_3.png","permalink":"/post/the-challenges-of-cryoem-with-our-friends-from-kek-in-japan-part-1-of-4.html","title":"The Challenges of CryoEM with our friends from KEK in Japan (Part 1 of 4)"},{"categories":[],"contents":" Learn about AI/ML on AWS HPC ","date":"October 1, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/efa-ident-large_hu621fd3d67d9e9f55b880dd262bde5d3e_704646_460x200_fill_box_smart1_3.png","permalink":"/ai-ml.html","title":"AI/ML on AWS HPC"},{"categories":[],"contents":" Learn about CAE/CFD on AWS HPC ","date":"October 1, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/efa-ident-large_hu621fd3d67d9e9f55b880dd262bde5d3e_704646_460x200_fill_box_smart1_3.png","permalink":"/cae-cfd.html","title":"CAE/CFD on AWS HPC"},{"categories":[],"contents":" Learn about climate, environment, and weather on AWS HPC Weather simulation is a reliably difficult workload for almost any HPC architecture and is often used as a litmus test by many customers before they look at a novel technologies or different systems. Customers have asked us frequently about our performance, and that’s been even more the case since we launched the Elastic Fabric Adapter and several new HPC-oriented instance families.\nThe Hpc6a is built on AMD\u0026rsquo;s Milan, and the AWS Graviton is our Arm64-based processor. Both perform excellently with a variety of weather and climate codes, as you can see in the blog posts, videos and discussions linked on this page.\nNumerical Weather Prediction An overview of Numerical Weather Prediction (NWP) workloads on AWS HPC-optimized instances. We test three popular NWP codes: WRF, MPAS, and FV3GFS on the Hpc6a.\nNAVGEM on the Cloud: Computational Evaluation of Cloud HPC with a Global Atmospheric Model - Prompted by DoD priorities for modernization, cost savings, and redundancy, this project compared the performance of the NAVGEM on an in-house supercomputer system against several cloud offerings including AWS. This talk was recorded at the HPC User Forum in the fall of 2019.\nRunning the Harmonie weather model on AWS - We worked together with the Danish Meteorological Institute (DMI) to port and run a full numerical weather prediction (NWP) cycling dataflow with the Harmonie Numerical Weather Prediction (NWP) model to AWS.\nThe role of AWS Graviton in Weather Arm a world-leading forecast model with AWS Graviton and Lambda - the UK MetOffice is a a well-recognized scientific leader in weather, climate and environmental forecasts and severe weather warnings for the protection of life and property. They describe their experience of gathering their SurfaceNet data gathering of over 1 billion observations a year using AWS Lamba and their switch to using AWS Graviton. Numerical weather prediction on AWS Graviton2 - [Blog] [Video] - an exploration of how WRF performs on AWS Graviton2, delivering suprising performance results. Cloud as a training tool Training forecasters to warn severe hazardous weather on AWS ","date":"October 1, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/efa-ident-large_hu621fd3d67d9e9f55b880dd262bde5d3e_704646_460x200_fill_box_smart1_3.png","permalink":"/climate-environment-weather.html","title":"Climate/Environment/Weather on AWS HPC"},{"categories":[],"contents":" Learn about Financial Services on AWS HPC ","date":"October 1, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/efa-ident-large_hu621fd3d67d9e9f55b880dd262bde5d3e_704646_460x200_fill_box_smart1_3.png","permalink":"/financial-services.html","title":"Financial Services on AWS HPC"},{"categories":[],"contents":" Learn about Life Sciences on AWS HPC ","date":"October 1, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/efa-ident-large_hu621fd3d67d9e9f55b880dd262bde5d3e_704646_460x200_fill_box_smart1_3.png","permalink":"/life-sciences.html","title":"Life Sciences on AWS HPC"},{"categories":[],"contents":" Some resources for learning about AWS HPC ","date":"October 1, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/efa-ident-large_hu621fd3d67d9e9f55b880dd262bde5d3e_704646_460x200_fill_box_smart1_3.png","permalink":"/resources.html","title":"Resources"},{"categories":["AWS ParallelCluster","Amazon Elastic Fabric Adapter","Amazon NICE DCV","Life Sciences"],"contents":"Amazon EC2 is the engine that powers HPC in the cloud. There\u0026rsquo;s quite a lot that\u0026rsquo;s new if you\u0026rsquo;ve never seen it before. in this Level 1 module in the Tech Shorts Foundations Series, we aim to demystify EC2 for the HPC community.\nWe\u0026rsquo;ll expand the number of videos in this series over time - but if there\u0026rsquo;s a topic that\u0026rsquo;s really interesting to you that we\u0026rsquo;re overlooking, don\u0026rsquo;t hesitate to contact us.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"September 29, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/1y5Ix2HS8sw_hu39379d3026c0f1b311ba9e8ef744a140_12739_460x200_fill_box_smart1_3.png","permalink":"/post/understanding-ec2-for-hpc-users.html","title":"Understanding EC2 for HPC users"},{"categories":["Climate/Environment/Weather"],"contents":"In this post, we will provide an overview of Numerical Weather Prediction (NWP) workloads, and the AWS HPC-optimized services for it. We’ll test three popular NWP codes: WRF, MPAS, and FV3GFS.\nRead the full post at the AWS HPC Blog.\n","date":"September 27, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-131-featured-img_hu7ca89d54f8be2693115bef0553af4543_162685_460x200_fill_box_smart1_3.png","permalink":"/post/getting-the-best-price-performance-for-numerical-weather-prediction-workloads-on-aws.html","title":"Getting the Best Price Performance for Numerical Weather Prediction Workloads on AWS"},{"categories":["AWS Batch"],"contents":"AWS service teams continuously improve the underlying infrastructure and operations of managed services, and AWS Batch is no exception. The AWS Batch team recently moved most of their job scheduler fleet to a serverless infrastructure model leveraging AWS Fargate. I had a chance to sit with Devendra Chavan, Senior Software Development Engineer on the AWS Batch team, to discuss the move to AWS Fargate and its impact on the Batch managed scheduler service component.\nRead the full post at the AWS HPC Blog.\n","date":"September 21, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-144-featured_hu449daaa352d55d2fa0acc8c15912388d_156760_460x200_fill_box_smart1_3.png","permalink":"/post/rearchitecting-aws-batch-managed-services-to-leverage-aws-fargate.html","title":"Rearchitecting AWS Batch managed services to leverage AWS Fargate"},{"categories":["AWS ParallelCluster","Amazon Elastic Fabric Adapter","Amazon NICE DCV","Life Sciences"],"contents":"We get it: AWS has too much stuff going on for any reasonably busy human to catch up with. So we\u0026rsquo;re going back to the foundations of AWS to help the HPC community come up to speed on the stuff that matters to them most.\nToday we\u0026rsquo;re starting a whole new series in Tech Shorts called Tech Shorts Foundations.\nWe\u0026rsquo;ll start high level and gradually peel back the layers so you can get as deep as you like in the areas you\u0026rsquo;re interested in the most.\nThink of this as your map to navigate AWS and get to the HPC places quickly, without having to get lost on the way there. Your journey starts here.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"September 15, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/u1Djjv8A5Do_hue4409c8c7bf4de87ec8c487628806179_15368_460x200_fill_box_smart1_3.png","permalink":"/post/welcome-to-tech-shorts-foundations.html","title":"Welcome to tech Shorts Foundations"},{"categories":["AWS ParallelCluster","Amazon Elastic Fabric Adapter","Amazon NICE DCV","Life Sciences"],"contents":"We get it: AWS has too much stuff going on for any reasonably busy human to catch up with. So we\u0026rsquo;re going back to the foundations of AWS to help the HPC community come up to speed on the stuff that matters to them most.\nToday we\u0026rsquo;re starting a whole new series in Tech Shorts called Tech Shorts Foundations. We\u0026rsquo;ll still have shows about advanced topics, new features and performance analysis. But there\u0026rsquo;s now two tracks in Tech Shorts, which will develop over time.\nWe\u0026rsquo;ll start high level and gradually peel back the layers so you can get as deep as you like in the areas you\u0026rsquo;re interested in the most.\nThink of this as your map to navigate AWS and get to the HPC places quickly, without having to get lost on the way there. Your journey starts here.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"September 15, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/KHx22oJSNso_huece75e9623ff7b1b76e8a9c97af309c3_15813_460x200_fill_box_smart1_3.png","permalink":"/post/intro-to-aws-for-hpc-people-tech-short-foundations-level-1.html","title":"Intro to AWS for HPC People - Tech Short Foundations Level 1"},{"categories":["AWS ParallelCluster"],"contents":"This post will help you understand the tools available to ease the stress of migrating your cluster (and your users) from SGE to Slurm, which is necessary since the HPC community is no longer supporting SGE’s open-source codebase.\nRead the full post at the AWS HPC Blog.\n","date":"September 14, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/CleanShot-2022-06-29-at-14.58.17_hu8be735c4ee9c61f4c1bf3120903330b5_305784_460x200_fill_box_smart1_3.png","permalink":"/post/easing-your-migration-from-sge-to-slurm-in-aws-parallelcluster-3.html","title":"Easing your migration from SGE to Slurm in AWS ParallelCluster 3"},{"categories":["Financial Services"],"contents":"Understanding deal and portfolio risk and capital requirements is a computationally expensive process that requires the execution of multiple financial forecasting models every day and in often in real time. This post describes how it works at RenaissanceRe, one of the world’s leading reinsurance companies.\nRead the full post at the AWS HPC Blog.\n","date":"September 6, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/CleanShot-2022-08-03-at-12.43.36-1260x628_hu9416d1964e336829b9d162c2cfc5de48_238964_460x200_fill_box_smart1_3.png","permalink":"/post/a-serverless-architecture-for-high-performance-financial-modelling.html","title":"A serverless architecture for high performance financial modelling"},{"categories":["AWS ParallelCluster"],"contents":"A key part of the development of quantum hardware and quantum algorithms is simulation using existing classical architectures and HPC techniques. In this blog post, we describe how to perform large-scale quantum circuits simulations using AWS ParallelCluster with QuEST, the Quantum Exact Simulation Toolkit. We demonstrate a simple and rapid deployment of computational resources up to 4,096 compute instances to simulate random quantum circuits with up to 44 qubits. We were able to allocate as many as 4096 EC2 instances of c5.18xlarge to simulate a non-trivial 44 qubit quantum circuit in fewer than 3.5 hours.\nRead the full post at the AWS HPC Blog.\n","date":"August 30, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/CleanShot-2022-08-03-at-12.25.24-1260x627_hua88377418e292ecf1238b85b72cce226_306958_460x200_fill_box_smart1_3.png","permalink":"/post/simulating-44-qubit-quantum-circuits-using-aws-parallelcluster.html","title":"Simulating 44-Qubit quantum circuits using AWS ParallelCluster"},{"categories":["Life Sciences"],"contents":"In this blog, we showcase the first version of Open Omics and benchmark three applications that are used in processing NGS data – sequence alignment tools BWA-MEM, minimap2, and single cell ATAC-Seq on Xeon-based Amazon Elastic Compute Cloud (Amazon EC2) Instances.\nRead the full post at the AWS HPC Blog.\n","date":"August 23, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dalle-hpc-01_hub722fbb4e61ec1893436c76a61744c34_863441_460x200_fill_box_smart1_3.png","permalink":"/post/accelerating-genomics-pipelines-using-intels-open-omics-acceleration-framework-on-aws.html","title":"Accelerating Genomics Pipelines Using Intel’s Open Omics Acceleration Framework on AWS"},{"categories":[],"contents":"In this final part of this three-part blog series on building predictive models at scale in AWS, we will use the synthetic dataset and the models generated in the previous post to showcase the model updating and sensitivity analysis capabilities of the aws-do-pm framework.\nRead the full post at the AWS HPC Blog.\n","date":"August 11, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpc-132-p3-hreader_hu449daaa352d55d2fa0acc8c15912388d_153913_460x200_fill_box_smart1_3.png","permalink":"/post/building-a-scalable-predictive-modeling-framework-in-aws-part-3.html","title":"Building a Scalable Predictive Modeling Framework in AWS – Part 3"},{"categories":[],"contents":"In the first part of this three-part blog series, we introduced the aws-do-pm framework for building predictive models at scale in AWS. In this blog, we showcase a sample application for predicting the life of batteries in a fleet of electric vehicles, using the aws-do-pm framework.\nRead the full post at the AWS HPC Blog.\n","date":"August 10, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpc-132-p2-hreader_hu366d00aee6445a9ad9f0a17656234076_138265_460x200_fill_box_smart1_3.png","permalink":"/post/building-a-scalable-predictive-modeling-framework-in-aws-part-2.html","title":"Building a Scalable Predictive Modeling Framework in AWS – Part 2"},{"categories":[],"contents":"Predictive models have powered the design and analysis of real-world systems such as jet engines, automobiles, and powerplants for decades. These models are used to provide insights on system performance and to run simulations, at a fraction of the cost compared to experiments with physical hardware. In this first post of three, we described the motivation and general architecture of the open-source aws-do-pm framework project for building predictive models at scale in AWS.\nRead the full post at the AWS HPC Blog.\n","date":"August 9, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpc-132-p1-hreader_hu69cefb4ef533e82e86750a06ab440e8b_168783_460x200_fill_box_smart1_3.png","permalink":"/post/building-a-scalable-predictive-modeling-framework-in-aws-part-1.html","title":"Building a Scalable Predictive Modeling Framework in AWS – Part 1"},{"categories":["AWS ParallelCluster","Amazon Elastic Fabric Adapter","Amazon NICE DCV","Life Sciences"],"contents":"Spack has already been removing the ugly work from building HPC codes, but with the announcement of the Spack Binary Cache at ISC'22, build and deploy times for these complicated applications will drop by 95% or more in most cases.\nGreg Becker from Livermore came along to show us how it works, and discuss what\u0026rsquo;s behind it.\nYou can find a blog post about the announcement here: hpc.news/binaryCache\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"August 8, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/CilsaBBycZY_hu586140159a601b6ac3ac396c783da8f5_18122_460x200_fill_box_smart1_3.png","permalink":"/post/get-your-hpc-codes-installed-and-running-in-minutes-using-spacks-binary-cache.html","title":"Get your HPC codes installed and running in minutes using Spack's Binary Cache"},{"categories":["CAE/CFD","AWS ParallelCluster"],"contents":"In this blog post, we discuss the AWS solution that Amazon’s construction division used to conduct large-scale CFD fire simulations as part of their Fire Strategy solutions to demonstrate safety and fire mitigation strategies. We outline the five key steps taken that resulted in simulation times that were 15-20x faster than previous on-premises architectures, reducing the time to complete from up to twenty-one days to less than one day.\nRead the full post at the AWS HPC Blog.\n","date":"August 3, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-87-header_hu7ca89d54f8be2693115bef0553af4543_154803_460x200_fill_box_smart1_3.png","permalink":"/post/running-large-scale-cfd-fire-simulations-on-aws-for-amazoncom.html","title":"Running large-scale CFD fire simulations on AWS for Amazon.com"},{"categories":["AWS ParallelCluster","Amazon Elastic Fabric Adapter","Amazon NICE DCV","Life Sciences"],"contents":"ParallelCluster can now mount lots and lots of file systems that you\u0026rsquo;ve previously created in your AWS account, in addition to the scratch filesystem you can ask it to create for you when you launch your cluster. And as of today, ParallelCluster supports OpenZFS as one of those filesystems, along with Netapp ONTAP - which will help you get access to data on your enterprise filesystems, too.\nOlly Perks and Austin Cherian describe all this in detail, as part 1 of a 2-part series covering the new features of ParallelCluster 3.2.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"July 28, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/2JOoMv-K1FY_hu3e921e106bf9fc7123d292420fd51f5e_14043_460x200_fill_box_smart1_3.png","permalink":"/post/new-file-systems-support-in-parallelcluster-32-part-1-of-2.html","title":"New file systems support in ParallelCluster 3.2 (Part 1 of 2)"},{"categories":["AWS ParallelCluster","Amazon Elastic Fabric Adapter","Amazon NICE DCV","Life Sciences"],"contents":"If you\u0026rsquo;ve ever had to iteratively guess how much memory is left in a bunch of compute nodes in order to get your memory-hungry jobs running, then this feature will save your sanity.\nIt\u0026rsquo;s a new integration between ParallelCluster and Slurm that lets you specify how much RAM your jobs need, and gives Slurm the ability to figure out how to place your jobs in order to achieve that - not just counting cores, which is the default behavior for most schedulers.\nOlly Perks and Austin Cherian describe this in detail, as part of a 2-part series covering the new features of ParallelCluster 3.2 (part 1 covered new file systems support and you can find it here: https://youtu.be/2JOoMv-K1FY).\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"July 28, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/3YP8CbckYQA_hu18d312ce145c7551d9f594c404acacc8_16737_460x200_fill_box_smart1_3.png","permalink":"/post/memory-aware-scheduling-with-slurm-in-parallelcluster-32-part-2-of-2.html","title":"Memory aware scheduling with Slurm in ParallelCluster 3.2 (Part 2 of 2)"},{"categories":["AWS ParallelCluster"],"contents":"AWS ParallelCluster version 3.2 introduces support for two new Amazon FSx filesystem types (NetApp ONTAP and OpenZFS). It also lifts the limit on the number of filesystem mounts you can have on your cluster. We’ll show you how, and help you with the details for getting this going right away.\nRead the full post at the AWS HPC Blog.\n","date":"July 28, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/CleanShot-2022-07-28-at-09.45.46-1260x626_hu4954fc20bc6d1732c8efd9001a93542f_321310_460x200_fill_box_smart1_3.png","permalink":"/post/expanded-filesystems-support-in-aws-parallelcluster-32.html","title":"Expanded filesystems support in AWS ParallelCluster 3.2"},{"categories":["AWS ParallelCluster"],"contents":"AWS ParallelCluster version 3.2 now supports memory-aware scheduling in Slurm to give you control over the placement of jobs with specific memory requirements. In this blog post, we’ll show you how it works, and explain why this will be really useful to people with memory-hungry workloads.\nRead the full post at the AWS HPC Blog.\n","date":"July 28, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/CleanShot-2022-07-28-at-09.39.30-1260x630_hu7493e97492077c91f54145fc9425b4c9_243441_460x200_fill_box_smart1_3.png","permalink":"/post/slurm-based-memory-aware-scheduling-in-aws-parallelcluster-32.html","title":"Slurm-based memory-aware scheduling in AWS ParallelCluster 3.2"},{"categories":[],"contents":"Lawrence Livermore National Laboratory (LLNL) and AWS are joining forces to provide a training opportunity for emerging HPC tools and application. RADIUSS (Rapid Application Development via an Institutional Universal Software Stack) is a broad suite of open-source software projects originating from LLNL. Together we are hosting a tutorial series to give attendees hands-on experience with these cutting-edge technologies. Find out how to participate in these events in this blog post.\nRead the full post at the AWS HPC Blog.\n","date":"July 22, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-146-header_hu449daaa352d55d2fa0acc8c15912388d_151650_460x200_fill_box_smart1_3.png","permalink":"/post/call-for-participation-radiuss-tutorial-series.html","title":"Call for participation: RADIUSS Tutorial Series"},{"categories":["AWS ParallelCluster","Amazon Elastic Fabric Adapter","Amazon NICE DCV","Life Sciences"],"contents":"In part 1, we covered all the aspects of designing and creating ParallelClusters in PCluster Manager. Today we delve into some more advanced topics like debugging your stack when something goes wrong, managing access to the cluster via the UI and CLI, and visualization, which is something that\u0026rsquo;s just super hard to do anywhere else.\nSean Smith shows us how this is all a lot easier with the right tools. if you want to install PCluster Manager in your account, head over to hpc.news/pclustermanager and deploy one of the one-click lauchable stacks to get going. There\u0026rsquo;s even an episode with Charlie (from our engineering team) showing how to install it and get it set up (it\u0026rsquo;s here: https://youtu.be/Z1vlpJYb1KQ).\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"July 19, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/-sCTPyXnkOM_hu7c716726b745863f1ea626799f1a8608_17364_460x200_fill_box_smart1_3.png","permalink":"/post/clusters-in-the-cloud-made-easier-with-pcluster-manager-part-2.html","title":"Clusters in the Cloud made easier with PCluster Manager (Part 2)"},{"categories":["AWS Batch","Life Sciences"],"contents":"In this blog post, we demonstrate how to leverage the AWS Genomics Command line and Amazon SageMaker to analyze large-scale exome sequences and derive meaningful insights. We use the bioinformatics workflow manager Nextflow, it’s open source library of pipelines, NF-Core, and AWS Batch.\nRead the full post at the AWS HPC Blog.\n","date":"July 19, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dalle-hpc-01_hub722fbb4e61ec1893436c76a61744c34_863441_460x200_fill_box_smart1_3.png","permalink":"/post/analyzing-genomic-data-using-amazon-genomics-cli-and-amazon-sagemaker.html","title":"Analyzing Genomic Data using Amazon Genomics CLI and Amazon SageMaker"},{"categories":["AWS ParallelCluster","Amazon Elastic Fabric Adapter","Amazon NICE DCV","Life Sciences"],"contents":"Today is the first part of a 2-part series and we\u0026rsquo;re covering all the aspects of designing and creating ParallelClusters using PCluster Manager, which is a graphical user interface tool for making all the hard bits of cluster management way less messy and so much easier. All the documentation in the world isn\u0026rsquo;t as useful as this one tool.\nIn part 2, we\u0026rsquo;ll dive into some advanced topics like debugging your cluster build when something goes wrong, and we\u0026rsquo;ll show you some fancy access methods, and tools for managing jobs - and visualizing the results.\nSean Smith shows us how this is all a lot easier with the right tools. if you want to install PCluster Manager in your account, head over to hpc.news/pclustermanager and deploy one of the one-click lauchable stacks to get going. There\u0026rsquo;s even an episode with Charlie (from our engineering team) showing how to install it and get it set up (it\u0026rsquo;s here: https://youtu.be/Z1vlpJYb1KQ).\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"July 14, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/PChP3FQWeJQ_hu4b2bece4864f065f8abf6a39c325127a_17351_460x200_fill_box_smart1_3.png","permalink":"/post/clusters-in-the-cloud-made-easier-with-pcluster-manager-part-1-of-2.html","title":"Clusters in the Cloud made easier with PCluster Manager (Part 1 of 2)"},{"categories":["AWS ParallelCluster","Life Sciences"],"contents":"In this blog post, we’ll walk you through the process of building a successful Cryo-EM benchmarking pilot using AWS ParallelCluster, Amazon FSx for Lustre, and cryoSPARC (from Structura Biotechnology) and explain some of our design decisions along the way.\nRead the full post at the AWS HPC Blog.\n","date":"July 12, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/CleanShot-2022-07-12-at-14.52.54-1260x630_hue19d69081e7c40cb90d473cd3c4aff67_336348_460x200_fill_box_smart1_3.png","permalink":"/post/how-thermo-fisher-scientific-accelerated-cryo-em-using-aws-parallelcluster.html","title":"How Thermo Fisher Scientific Accelerated Cryo-EM using AWS ParallelCluster"},{"categories":["AWS Batch"],"contents":"This blog post explains how to run parallel rendering workloads and produce an animation in a cost and time effective way using AWS Batch and AWS Step Functions. AWS Batch manages the rendering jobs on Amazon Elastic Compute Cloud (Amazon EC2), and AWS Step Functions coordinates the dependencies across the individual steps of the rendering workflow. Additionally, Amazon EC2 Spot instances can be used to reduce compute costs by up to 90% compared to On-Demand prices.\nRead the full post at the AWS HPC Blog.\n","date":"July 5, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/CleanShot-2022-06-17-at-16.15.49_hu8be735c4ee9c61f4c1bf3120903330b5_385146_460x200_fill_box_smart1_3.png","permalink":"/post/efficient-and-cost-effective-rendering-pipelines-with-blender-and-aws-batch.html","title":"Efficient and cost-effective rendering pipelines with Blender and AWS Batch"},{"categories":["AI/ML","AWS ParallelCluster","Amazon Elastic Fabric Adapter","Amazon NICE DCV","Life Sciences"],"contents":"Machine Learning is a huge workload, and one of the most demanding when it comes to scaling to thousands (and thousands) of CPUs. Some of the largest workloads customers run in the cloud are deep learning models, which require huge numbers of GPUs and saturate the networks connecting them.\nTo make all that work on AWS, NVIDIA\u0026rsquo;s collectives communications library (NCCL) relies on libfabrics to speak to the EFA hardware that makes up EC2\u0026rsquo;s high performance interconnect.\nRashika Kheria leads the team in Annapurna that handles this interface, ensuring your models, using all your favorite frameworks, scale really nicely to as far as your imagination allows (well, maybe a little further). She came to Tech Shorts to tell us how that works.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"June 30, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/kDtHpRB5luw_huec860fbaf7dcd9809006ef11c2e058c4_18163_460x200_fill_box_smart1_3.png","permalink":"/post/nccl-on-efa-makes-the-ml-world-go-around-in-the-cloud.html","title":"NCCL on EFA makes the ML world go around in the cloud"},{"categories":["AWS Batch","Life Sciences"],"contents":"In this blog post, we’ll show how you can run NVIDIA Parabricks on AWS Batch leveraging AWS CloudFormation templates. Parabricks is a GPU-accelerated tool for secondary genomic analysis. It reduces the runtime of variant calling on a 30x human genome from 30 hours to just 30 minutes, and leverages AWS Batch to provide an interface that scales compute jobs across multiple instances in the cloud.\nRead the full post at the AWS HPC Blog.\n","date":"June 28, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dalle-hpc-01_hub722fbb4e61ec1893436c76a61744c34_863441_460x200_fill_box_smart1_3.png","permalink":"/post/getting-started-with-nvidia-clara-parabricks-on-aws-batch-using-aws-cloudformation.html","title":"Getting Started with NVIDIA Clara Parabricks on AWS Batch using AWS CloudFormation"},{"categories":["AI/ML","AWS ParallelCluster","Amazon Elastic Fabric Adapter","Amazon NICE DCV","Life Sciences"],"contents":"The Ampersand Data Science team had a challenge: their Bayesian statistical models needed more than half a million core-hours of runtime, regularly, if they were to get an answer fast enough for it to be useful to their customers.\nScaling to a million core or more isn\u0026rsquo;t really a challenge now (thanks to Amazon EC2). The hard part is all the code pipelines and plumbing - and the management of the entire thing when it\u0026rsquo;s in flight.\nDaniel Gerlanc (Senior Director for Data Science) and Jeffrey Enos (Senior Machine Learning Engineer) swung by the Tech Shorts virtual watercooler to tell us how it worked, what was most surprising, and which bits made all the difference.\nThere\u0026rsquo;s also a blog that was posted last week which talks to some of this too. Worth a read: https://aws.amazon.com/blogs/hpc/bayesian-ml-models-at-scale-with-aws-batch/\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"June 24, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/CcqeeRyx93k_hu96559df0e5c2bfaf9dad18bac7ee3caa_16144_460x200_fill_box_smart1_3.png","permalink":"/post/bayesian-models-and-half-a-million-cores-whatre-you-waiting-for.html","title":"Bayesian models and half a million cores - what're you waiting for?"},{"categories":["AWS Batch"],"contents":"In this blog post, we help you understand the AWS Batch job termination process and how you may take actions to gracefully terminate a job by capturing SIGTERM signal inside the application. It provides you with an efficient way to exit your Batch jobs. You also get to know about how job timeouts occur, and how the retry operation works with both traditional AWS Batch jobs and array jobs.\nRead the full post at the AWS HPC Blog.\n","date":"June 21, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-121-header-v2_hu7ca89d54f8be2693115bef0553af4543_151353_460x200_fill_box_smart1_3.png","permalink":"/post/understanding-the-aws-batch-termination-process.html","title":"Understanding the AWS Batch termination process"},{"categories":["AWS Batch","AWS ParallelCluster","Amazon Elastic Fabric Adapter","Amazon NICE DCV","Life Sciences"],"contents":"The AWS Batch team recently added container insights and advanced logging features to the Batch console, which is making a LOT of people very happy.\nWe decided to catch up with the dev team in Seattle who worked on this, and one of my favotire Amazonians - David Chambers - joined me to help us all understand what this means to Batch users and to taker us for a drive to sere how easy it is.\nThis is our first ever in-person Tech Short, which is something we\u0026rsquo;re really keen to build on, now that travel is starting to happen more. Hope you like it.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"June 16, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/uv4jJ7XIAfs_hubdac7c58f97bc20d0868125ec956fe89_14094_460x200_fill_box_smart1_3.png","permalink":"/post/new-console-features-including-container-insights-in-aws-batch.html","title":"New console features including container insights in AWS Batch"},{"categories":["AI/ML","AWS Batch"],"contents":"Ampersand is a data-driven TV advertising technology company that provides aggregated TV audience impression insights and planning on 42 million households, in every media market, across more than 165 networks and apps and in all dayparts (broadcast day segments). The Ampersand Data Science team estimated that building their statistical models would require up to 600,000 physical CPU hours to run, which would not be feasible without using a massively parallel and large-scale architecture in the cloud. AWS Batch enabled Ampersand to compress their time of computation over 500x through massive scaling while optimizing their costs using Amazon EC2 Spot. In this blog post, we will provide an overview of how Ampersand built their TV audience impressions (“impressions”) models at scale on AWS, review the architecture they have been using, and discuss optimizations they conducted to run their workload efficiently on AWS Batch.\nRead the full post at the AWS HPC Blog.\n","date":"June 14, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-124-header_hub4e0ea0f7b8733e2ec745f7fa262889e_138966_460x200_fill_box_smart1_3.png","permalink":"/post/bayesian-ml-models-at-scale-with-aws-batch.html","title":"Bayesian ML Models at Scale with AWS Batch"},{"categories":["AWS ParallelCluster","Life Sciences"],"contents":"In this blog post, we cover how to run GROMACS – a popular open source designed for simulations of proteins, lipids, and nucleic acids – cost effectively by leveraging EC2 Spot Instances within AWS ParallelCluster. We also show how to checkpoint GROMACS to recover gracefully from possible Spot Instance interruptions.\nRead the full post at the AWS HPC Blog.\n","date":"June 9, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-69-header_hua668e1514f0dd00acb5cf1256b1a7032_155358_460x200_fill_box_smart1_3.png","permalink":"/post/running-cost-effective-gromacs-simulations-using-amazon-ec2-spot-instances-with-aws-parallelcluster.html","title":"Running cost-effective GROMACS simulations using Amazon EC2 Spot Instances with AWS ParallelCluster"},{"categories":["AWS ParallelCluster","Amazon Elastic Fabric Adapter","Amazon NICE DCV","Life Sciences"],"contents":"Olly Perks - our resident Arm64 architecture expert and all round HPC software guy - walks us through the quite extensive support for HPC developers who are planning to work on AWS Graviton processors.\nThere\u0026rsquo;s a lot of software and it comes from a number of sources, including open source and commercial groups.\nThis is part 2 of a two-part series - the first part was published last week, abd you can find it here: https://youtu.be/qFrpmgvN9Xs\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"June 8, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/CMSKuq04GUo_hu0149f473ce5685f00b48733f08ed5499_13539_460x200_fill_box_smart1_3.png","permalink":"/post/the-arm64-developer-environments-part-2-of-2.html","title":"The Arm64 developer environments - Part 2 of 2"},{"categories":["AWS ParallelCluster","Amazon Elastic Fabric Adapter","Amazon NICE DCV","Life Sciences"],"contents":"Olly Perks - our resident Arm64 architecture expert and all round HPC software guy - walks us through the quite extensive support for HPC developers who are planning to work on AWS Graviton processors.\nThere\u0026rsquo;s a lot of software and it comes from a number of sources, including open source and commercial groups.\nThis is part 1 of a two-part series. The other part will be out early next week.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"June 2, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/qFrpmgvN9Xs_hu3454b9e6896ab820acbfc1c3cd8c8dd4_13890_460x200_fill_box_smart1_3.png","permalink":"/post/the-arm64-developer-environments-part-1-of-2.html","title":"The Arm64 developer environments - Part 1 (of 2)"},{"categories":["AWS ParallelCluster"],"contents":"Today we’re excited to announce the availability of a new public Spack Binary Cache. In a collaboration, between AWS, E4S, Kitware, and the Lawrence Livermore National Laboratory (LLNL), Spack users now have access to a public build cache hosted on Amazon S3. The use of this Binary Cache will result in up to 20x faster install times for common Spack packages.\nRead the full post at the AWS HPC Blog.\n","date":"May 31, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpblog-129-header_hu2ac7faf005d8f3e44a8c36641cf2d7f8_225231_460x200_fill_box_smart1_3.png","permalink":"/post/introducing-the-spack-rolling-binary-cache-hosted-on-aws.html","title":"Introducing the Spack Rolling Binary Cache hosted on AWS"},{"categories":["AWS ParallelCluster","Amazon Elastic Fabric Adapter","Amazon NICE DCV","Life Sciences"],"contents":"This is the second part of a two-part set about the Winter Invitational Student Cluster Competition, which the AWS HPC team supported as one of four mentor teams.\nThis isn\u0026rsquo;t just about the competition or the results. It\u0026rsquo;s about the skills that are important to getting a job, learning new things and solving tough problems. And it\u0026rsquo;s about working together with other people.\nIf you\u0026rsquo;re able to support these kinds of events, you should give it a try. it\u0026rsquo;s super rewarding.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"May 19, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/cZpDVD1k6Lo_huca843d194b4a59647c785f7292727d44_14053_460x200_fill_box_smart1_3.png","permalink":"/post/winter-invitational-episode-ii-the-algorithms-strike-back.html","title":"Winter Invitational Episode II - The Algorithms Strike Back"},{"categories":["AWS ParallelCluster","Amazon Elastic Fabric Adapter","Amazon NICE DCV","Life Sciences"],"contents":"This is the first part of a two-part set about the Winter Invitational Student Cluster Competition (which the AWS HPC team supported as one of four mentor teams). The competition targeted academic institutions that we need to succeed to bring some balance to the HPC force.\nDan Olds, the Chief Research Officer of Intersect360, and the unofficial historian (and historical figure) of Student Cluster Comps, joined us to walk us through the structure of the comp, and feed us some of the drama leading up to an extraordinary result in 2022.\nThis isn\u0026rsquo;t just about the competition or the results. It\u0026rsquo;s about the skills that are important to getting a job, learning new things and solving tough problems while working together with other people.\nIf you\u0026rsquo;re able to support these kinds of events, you should give it a try. it\u0026rsquo;s super rewarding. And if you\u0026rsquo;re wondering what it\u0026rsquo;s like to compete, watch these episodes, and you might be inspired.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"May 17, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/vW_pMzMjBig_hu115bc33a8518c527e6c899356c75e354_15459_460x200_fill_box_smart1_3.png","permalink":"/post/winter-invitational-episode-i-new-hopefuls.html","title":"Winter Invitational Episode I - New Hopefuls"},{"categories":["AWS Batch"],"contents":"This post covers the different ways you can encode a dependency between basic and array jobs in AWS Batch. We also cover why you may want to encode dependencies outside of Batch altogether using a workflow system like AWS Step Functions or Apache Airflow.\nRead the full post at the AWS HPC Blog.\n","date":"May 11, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-31-header-1260x630_huf7956106e02f089741086e79a139176d_298330_460x200_fill_box_smart1_3.png","permalink":"/post/encoding-workflow-dependencies-in-aws-batch.html","title":"Encoding workflow dependencies in AWS Batch"},{"categories":["AI/ML","AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"The AWS Graviton 3 will be launched this year as part of the new C7g instance. Inside this chip are some interesting innovations that already have a lot of HPC and AI/ML customers interested.\nWe sat down with Olly Perks, who recently joined AWS HPC Engineering from Arm, to discuss what\u0026rsquo;s most interesting. This is the first of a series of Tech Shorts covering the Graviton 3 architecture. We\u0026rsquo;ll deep dive on techniques and tools for getting the most out of these CPUs.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"May 5, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/A3PXJtWcczI_hu7e722cf83d92946609e6e306e50f4f22_17504_460x200_fill_box_smart1_3.png","permalink":"/post/what-makes-the-aws-graviton-3-so-interesting-to-hpc-and-aiml-customers.html","title":"What makes the AWS Graviton 3 so interesting to HPC and AI/ML customers?"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"YellowDog (yellowdog.co) cut their teeth in the media and entertainment industry working on orchestrating really large workloads for CGI and FX companies. They\u0026rsquo;ve generalized the platform and now they\u0026rsquo;re able to do the same thing for nearly any kind of workload in the cloud (or hybrid on-prem/cloud scenarios).\nThey understand the complexities of doing this, and have features and tooling for dealing with the nitty gritty (rationing licenses for ISV applications, data movement and staging etc etc).\nAlan Parry, their Director of Engineering showed us how easy it is.\nIn the show, we mentioned a blog post, which you can find here: https://hpc.news/hawking901\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"April 29, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/5O-Zdl740fw_hu78eddf88e3ed3c8ceb53611c18123c8e_16759_460x200_fill_box_smart1_3.png","permalink":"/post/yellowdog-make-scaling-to-crazy-large-workloads-easy.html","title":"YellowDog make scaling to crazy large workloads easy"},{"categories":["AWS Batch"],"contents":"In this post, I cover some of the recent updates to AWS Batch, including improvements to job placement, addition of AWS PrivateLink support, and the new capabilities to update your AWS Batch compute environments.\nRead the full post at the AWS HPC Blog.\n","date":"April 25, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpc-blog-125-header-1260x630_hu586066fb4b8eb0aee0c8c11c079b1036_257642_460x200_fill_box_smart1_3.png","permalink":"/post/aws-batch-updates-higher-compute-utilization-aws-privatelink-support-and-updatable-compute-environments.html","title":"AWS Batch updates: higher compute utilization, AWS PrivateLink support, and updatable compute environments"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"Protein folding simulations can consume huge CPU time on supercomputers. AlphaFold changed that by applying ML techniques to the task. Then the problem became \u0026lsquo;how do I run AlphaFold easily?\u0026rsquo;.\nThe Healthcare AI/ML team at AWS figured that out, built a really accessible solution and open-sourced it to GitHub, which is where you can grab it today.\nBrian Loyal and Ujjwal Ratan from the AI/ML specialist team in our global HCLS org joined us to help understand how this fits, why it works, and described some of the awesome science it supports.\nThey blogged about it here: https://hpc.news/curie264 The CloudFormation stack is here: https://github.com/aws-samples/aws-batch-architecture-for-alphafold\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"April 20, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/58ijuqcHS7Y_hu9577b95184e724c31d0a0c4a08d3c164_14433_460x200_fill_box_smart1_3.png","permalink":"/post/protein-folding-alphafold-at-scale-using-a-notebook-and-the-cloud.html","title":"Protein folding (AlphaFold) at SCALE using a notebook and the cloud"},{"categories":["Life Sciences"],"contents":"Somatic variants are genetic alterations which are not inherited but acquired during one’s lifespan, for example those that are present in cancer tumors. In this post, we will demonstrate how to perform somatic variant calling from matched tumor and normal genome sequence data, as well as tumor-only whole genome and whole exome datasets using an NVIDIA GPU-accelerated Parabricks pipeline, and compare the results with baseline CPU-based workflows.\nRead the full post at the AWS HPC Blog.\n","date":"April 20, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-114-header-1260x630_hua147a7752417c27699b65c243e7d5ace_226642_460x200_fill_box_smart1_3.png","permalink":"/post/benchmarking-nvidia-clara-parabricks-somatic-variant-calling-pipeline-on-aws.html","title":"Benchmarking NVIDIA Clara Parabricks Somatic Variant Calling Pipeline on AWS"},{"categories":["AI/ML","Life Sciences"],"contents":"Drug discovery is an expensive proposition, with a $2.6 billion cost over 10 years and just a 12% success rate. AI promises to significantly improve the success rate by finding small molecule hits for undruggable targets. On the forefront of using AI in drug discovery is Atomwise, with its AtomNet® platform. In this blog, we will lay out the challenges of the drug discovery process, and show how AI/ML startups are solving these challenges using solutions from Atomwise, AWS, and WEKA.\nRead the full post at the AWS HPC Blog.\n","date":"April 12, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dalle-hpc-01_hub722fbb4e61ec1893436c76a61744c34_863441_460x200_fill_box_smart1_3.png","permalink":"/post/ai-based-drug-discovery-with-atomwise-and-weka-data-platform.html","title":"AI-based drug discovery with Atomwise and WEKA Data Platform"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"Amazon Braket is AWS\u0026rsquo;s Quantum Computing service which gives you access to several Quantum Computers using a serverless programming model and \u0026hellip; YOU ACTUALLY GET TO USE REAL QUANTUM COMPUTERS without needing a $50M budget and a cryo facility.\nThis is therefore one of the weirdest Tech Shorts we\u0026rsquo;ve done. We even run a Quantum version of \u0026lsquo;Hello, world\u0026rsquo;. No kidding.\nYou can get more information about the service at aws.amazon.com/braket\nBut : why not try it yourself?\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"April 8, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/8AYNmOeecHE_hu5c3b595f218661b5c0f74d41cde4d2e3_18516_460x200_fill_box_smart1_3.png","permalink":"/post/spooky-action-at-a-distance-as-a-service-quantum-computing.html","title":"Spooky-action-at-a-distance As a Service (Quantum Computing)"},{"categories":["CAE/CFD"],"contents":"Organizations such as Amazon Prime Air and Joby Aviation use Simcenter STAR-CCM+ for running CFD simulations on AWS so they can reduce product manufacturing cycles and achieve faster times to market. In this post today, we describe the performance and price analysis of running Computational Fluid Dynamics (CFD) simulations using Siemens SimcenterTM STAR-CCM+TM software on AWS HPC clusters.\nRead the full post at the AWS HPC Blog.\n","date":"April 5, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-49-fig1_hu8612b64ebb750eea2dbaab01990efbd8_118493_460x200_fill_box_smart1_3.png","permalink":"/post/simcenter-star-ccm-price-performance-on-aws.html","title":"Simcenter STAR-CCM+ price-performance on AWS"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"EnginFrame makes life easy for scientists and engineers so they can use HPC resources without having to understand the complexity.\nBUT: EnginFrame also makes the local sysadmin a hero by giving them the ability to embed into simple scripts the decisions that lead to determining how and where and when a job gets run.\nThe use cases are almost endless:\ndecide whether your job bursts to the cloud based on congestion conditions in your on\u0026ndash;prem queues figure out if data needs to be moved first before a job can be run decide based on job parameters whether a job is fit for execution in the AWS spot market, or should be run on-prem, or run in on-demand queues. The workshop we discussed is here: https://hpc.news/efConnectorWorkshop\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"March 31, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/a6R0FqsYGy0_huc35cd6e18b4caf562e996d53c256f357_18927_460x200_fill_box_smart1_3.png","permalink":"/post/controlling-hybrid-workflows-between-on-premises-and-cloud-with-enginframe.html","title":"Controlling hybrid workflows between on-premises and cloud with EnginFrame"},{"categories":["Life Sciences","AWS Batch"],"contents":"Matt Rasmussen, VP of Software Engineering at insitro, expands on his first post on redun, insitro’s data science tool for bioinformatics, to describe how redun makes use of advanced AWS features. Specifically, Matt describes how AWS Batch’s Array Jobs is used to support workflows with large fan-out, and how AWS Glue’s DynamicFrame is used to run computationally heterogenous workflows with different back-end needs such as Spark, all in the same workflow definition.\nRead the full post at the AWS HPC Blog.\n","date":"March 31, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-116-p2-fig2-831x630_huc10da2d850eca5ca75c40f40dddb1f65_297616_460x200_fill_box_smart1_3.png","permalink":"/post/data-science-workflows-at-insitro-how-redun-uses-the-advanced-service-features-from-aws-batch-and-aws-glue.html","title":"Data Science workflows at insitro: how redun uses the advanced service features from AWS Batch and AWS Glue"},{"categories":["Life Sciences","AWS Batch"],"contents":"Matt Rasmussen, VP of Software Engineering at insitro describes their recently released, open-source data science framework, redun, which allows data scientists to define complex scientific workflows that scale from their laptop to large-scale distributed runs on serverless platforms like AWS Batch and AWS Glue. I this post, Matt shows how redun lends itself to Bioinformatics workflows which typically involve wrapping Unix-based programs that require file staging to and from object storage. In the next blog post, Matt describes how redun scales to large and heterogenous workflows by leveraging AWS Batch features such as Array Jobs and AWS Glue features such as Glue DynamicFrame.\nRead the full post at the AWS HPC Blog.\n","date":"March 31, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-116-p1-fig1_hu420fc37b9229afdcb7e3eb859b0cc008_186131_460x200_fill_box_smart1_3.png","permalink":"/post/data-science-workflows-at-insitro-using-redun-on-aws-batch.html","title":"Data Science workflows at insitro: using redun on AWS Batch"},{"categories":["AWS Batch","AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"The AWS Genomics CLI (or AGC) seriously removes all the grunt work involved in setting up bioinformatics pipelines to run in the cloud. We know that Snakemake, Cromwell, NextFlow and miniWDL all work happily in the cloud on AWS Batch, but AGC means you don\u0026rsquo;t have to know to set all that stuff up - it does it for you.\nYou can have completely separate tool chains using completely different workflow languages all running at the same time, on the same infrastructure (if you like), sharing data and tooling.\nLee Pang from the dev team that built this came along to show us how it works, and - most importantly - how easy it is to get productive. Zero to hero in less than 30 mins - it\u0026rsquo;s really impressive.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"March 25, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/30cfBPdzykA_hucd1a41940a97d74c3d4d6e5ee1ef9206_17663_460x200_fill_box_smart1_3.png","permalink":"/post/genomics-workflow-set-made-easy-with-aws-genomics-cli.html","title":"Genomics workflow set made easy with AWS Genomics CLI"},{"categories":["Life Sciences"],"contents":"Quantum physics and high-performance computing have slashed research times for a consortium of researchers led by Qubit Pharmaceuticals. This post describes the discovery of chemical substances that may lead to new COVID-19 treatments in only six months using cloud technology.\nRead the full post at the AWS HPC Blog.\n","date":"March 22, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dalle-hpc-01_hub722fbb4e61ec1893436c76a61744c34_863441_460x200_fill_box_smart1_3.png","permalink":"/post/creating-a-digital-map-of-covid-19-virus-for-discovery-of-new-treatment-compounds.html","title":"Creating a digital map of COVID-19 virus for discovery of new treatment compounds"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"Customers have been asking us to make it easy to configure ParallelCluster to use a corporate directory server, so adding a user to the cloud HPC stack becomes as easy as adding them to (say) the Engineering Group in Active Directory or LDAP.\nWe\u0026rsquo;re happy to say we\u0026rsquo;ve done that, and released a blog about it a couple weeks ago. In today\u0026rsquo;s show, Giacomo Marciani (one of the developers who built this feature) came along to explain how it works and give us a demo, so you can get to the important steps fast.\nDuring the show we spoke about a blog and a tutorial. They\u0026rsquo; right here:\nBlog: https://hpc.news/turing313 Tutorial: https://hpc.news/hawking513 If you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"March 17, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/wvd6bFieht0_hu65a19074f67b8bf4b8c8092852109491_14315_460x200_fill_box_smart1_3.png","permalink":"/post/multi-user-in-parallelcluster-with-ldap-and-ms-active-directory.html","title":"Multi-user in ParallelCluster with LDAP and MS Active Directory"},{"categories":["AWS ParallelCluster"],"contents":"The AWS ParallelCluster version 3 CLI differs significantly from ParallelCluster version 2. This post provides some guidance on mapping between versions to help you with migrating to ParallelCluster 3. We also summarize new CLI features in ParallelCluster 3 to expose the things you just couldn’t do previously.\nRead the full post at the AWS HPC Blog.\n","date":"March 17, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dalle-hpc-01_hub722fbb4e61ec1893436c76a61744c34_863441_460x200_fill_box_smart1_3.png","permalink":"/post/migrating-to-aws-parallelcluster-v3-updated-cli-interactions.html","title":"Migrating to AWS ParallelCluster v3 – Updated CLI interactions"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"We launched Amazon FSx for OpenZFS at re:invent in December, and well let\u0026rsquo;s just say it\u0026rsquo;s been more than a bit popular :-)\nZFS and it\u0026rsquo;s friends have been a long time favorite of the HPC community, so we asked Delwin Olivan, the Snr Product Manager for the service to come and show us how easy it is to get big scale, and big performance with very little effort.\nAuto-backups and one-click snapshots are exactly as they\u0026rsquo;re described on the lid. Which is awesome.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"March 10, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/AaAy6_bpI9Q_hu97359e5845aaa97c5f87a360da79ff4d_13404_460x200_fill_box_smart1_3.png","permalink":"/post/openzfs-created-quickly-with-checkpoints-backups-and-big-performance.html","title":"OpenZFS created quickly with checkpoints, backups and big performance"},{"categories":["AWS Batch","AWS ParallelCluster"],"contents":"It’s an understatement that AWS has a lot of services (more than 200 at the time of this post!). We’re usually the first to point out that there’s more than one way to solve a problem. HPC is no different in this regard, because we offer a choice: customers can run their HPC workloads using AWS […]\nRead the full post at the AWS HPC Blog.\n","date":"March 10, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-78-header-1260x630_hu282297331ac3b5e88c7453d6f6f3f537_322830_460x200_fill_box_smart1_3.png","permalink":"/post/choosing-between-aws-batch-or-aws-parallelcluster-for-your-hpc-workloads.html","title":"Choosing between AWS Batch or AWS ParallelCluster for your HPC Workloads"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"Sometimes when we\u0026rsquo;re looking for performance we lose the forrest for all those trees - we miss the huge improvements we can do to unglamorous parts of the overall workflow, while we obsess on the pieces that look hard. It\u0026rsquo;s an engineer thing, I think.\nIn today\u0026rsquo;s Tech Short, Neil Ashton shows us exactly this kind of example from the world of CFD (using OpenFOAM, but this lesson applies generally) - and shows us how to break the problem down in order to speed it all up - and pretty easily, too.\nThe workshops we reference in the discussion are all listed here: https://aws.amazon.com/hpc/cfd/\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"March 3, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/F8YzcuYr7YI_hu1be96599c0b12c8df0c2bf502fc64e07_15019_460x200_fill_box_smart1_3.png","permalink":"/post/scaling-cfd-a-lot-by-breaking-down-a-workflow-to-speed-things-up.html","title":"Scaling CFD a lot by breaking down a workflow, to speed things up"},{"categories":["CAE/CFD"],"contents":"OpenFOAM is one the most widely used Computational Fluid Dynamics (CFD) packages and helps companies in a broad range of sectors (automotive, aerospace, energy, and life-sciences) to conduct research and design new products. In this post, we’ll discuss six practical things you can do as an OpenFOAM user to run your simulations faster and more cost effectively.\nRead the full post at the AWS HPC Blog.\n","date":"March 2, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-14-header-1260x630_hu99ff055de8fac9344e283c13f37168f9_337346_460x200_fill_box_smart1_3.png","permalink":"/post/getting-the-best-openfoam-performance-on-aws.html","title":"Getting the best OpenFOAM Performance on AWS"},{"categories":["Financial Services"],"contents":"We worked with our financial services customers to develop an open-source, scalable, cloud-native, high throughput computing solution on AWS — AWS HTC-Grid. HTC-Grid allows you to submit large volumes of short and long running tasks and scale environments dynamically. In this first blog of a two-part series, we describe the structure of HTC-Grid and its objective to provide a configurable blueprint for HPC grid scheduling on the cloud.\nRead the full post at the AWS HPC Blog.\n","date":"February 28, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-57-fig-2-981x630_hu852d505bf1855a20676a4a6dbadd9645_309677_460x200_fill_box_smart1_3.png","permalink":"/post/cloud-native-high-throughput-grid-computing-using-the-aws-htc-grid-solution.html","title":"Cloud-native, high throughput grid computing using the AWS HTC-Grid solution"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"The Amazon Fsx for Lustre team have managed to turn what used to be a multi-year process to buy and build a Lustre filesystem, into a simple process with a launch button on the front.\nIt\u0026rsquo;s hard to overstate the pain that this saves, but the most interesting things to ponder are the consequences of it. Jordan Dolman from the FSx Lustre team shows us some of the more unusual things you can do with Fsx Lustre that you just couldn\u0026rsquo;t think about doing any other way.\nStorage is one of the foundational layers we covered in the AWS HPC Speeds\u0026rsquo;n\u0026rsquo;Feeds event in early Feb 2022. if you missed it, head over to https://hpc.news/SpeedsFeeds to watch the replay.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"February 24, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/0AVdf3jKuvo_hu0dbd67d46653fbf8fe94cef3aa258d20_8640_460x200_fill_box_smart1_3.png","permalink":"/post/using-lustre-to-build-very-fast-file-systems-with-amazon-fsx-for-lustre.html","title":"Using Lustre to build very fast file systems with Amazon Fsx for Lustre"},{"categories":["AWS Batch"],"contents":"Introduction Monte Carlo methods are a class of methods based on the idea of sampling to study mathematical problems for which analytical solutions may be unavailable. The basic idea is to create samples through repeated simulations that can be used to derive approximations about a quantity we’re interested in, and its probability distribution. In this […]\nRead the full post at the AWS HPC Blog.\n","date":"February 23, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dalle-hpc-01_hub722fbb4e61ec1893436c76a61744c34_863441_460x200_fill_box_smart1_3.png","permalink":"/post/optimize-your-monte-carlo-simulations-using-aws-batch.html","title":"Optimize your Monte Carlo simulations using AWS Batch"},{"categories":[],"contents":"This post by Roberto Meda and Salvo Maccarone covers how you can configure NICE EnginFrame to leverage OKTA as an identity service provider to support SAML 2.0 single sign on authentication and several other features like multi-factor verification, API access management and multi-device support.\nRead the full post at the AWS HPC Blog.\n","date":"February 17, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dalle-hpc-01_hub722fbb4e61ec1893436c76a61744c34_863441_460x200_fill_box_smart1_3.png","permalink":"/post/integrating-okta-identity-service-provider-with-nice-enginframe.html","title":"Integrating OKTA identity service provider with NICE EnginFrame"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"Charlie Gruenweld, our newest Principal Engineer in the HPC org, took advantage of the new API in ParallelCluster and a few dozen years of web development revolutions to build a snappy UI for creating and managing clusters that makes it nearly impossible to go wrong - and gives you immediate access to some of ParallelCluster\u0026rsquo;s most powerful features.\nIn less than 20 minutes, you\u0026rsquo;ll grok how to create a cluster, run jobs, spawn elastic (disposable) nodes, and jump into a full graphical desktop to run visual applications on the cluster itself.\nThis is the project that brings it all together. I hope you enjoy this as much as I did.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"February 16, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/Z1vlpJYb1KQ_hua9fbe5893d8da4d7f566d2ca78335705_11027_460x200_fill_box_smart1_3.png","permalink":"/post/a-new-gui-for-building-and-managing-clusters-pcluster-manager.html","title":"A new GUI for building and managing clusters - PCluster Manager"},{"categories":["Life Sciences"],"contents":"We recently launched two new Amazon EC2 instance families based on Intel’s Ice Lake – the C6i and M6i. These instances provide higher core counts and take advantage of generational performance improvements on Intel’s Xeon scalable processor family architectures. In this post we show how GROMACS performs on these new instance families. We use similar methodologies as for previous posts where we characterized price-performance for CPU-only and GPU instances (Part 1, Part 2, Part 3), providing instance recommendations for different workload sizes.\nRead the full post at the AWS HPC Blog.\n","date":"February 16, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dalle-hpc-01_hub722fbb4e61ec1893436c76a61744c34_863441_460x200_fill_box_smart1_3.png","permalink":"/post/gromacs-performance-on-amazon-ec2-with-intel-ice-lake-processors.html","title":"GROMACS performance on Amazon EC2 with Intel Ice Lake processors"},{"categories":["AWS ParallelCluster"],"contents":"Today we’re announcing the release of AWS ParallelCluster 3.1 which now supports multiuser authentication based on Active Directory (AD). Starting with v3.1.1 clusters can be configured to use an AD domain managed via one of the AWS Directory Service options like Simple AD or AWS Managed Microsoft AD (MSAD). This blog post describes the new feature, and gives an example of a configuration block for ParallelCluster 3 configuration files.\nRead the full post at the AWS HPC Blog.\n","date":"February 10, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-115-header-1260x630_hu241b61614b9460f7af3b7a4d5fe13ee6_402816_460x200_fill_box_smart1_3.png","permalink":"/post/introducing-aws-parallelcluster-multiuser-support-via-active-directory.html","title":"Introducing AWS ParallelCluster multiuser support via Active Directory"},{"categories":["Climate/Environment/Weather"],"contents":"The Met Office is the UK’s National Meteorological Service, providing 24×7 world-renowned scientific excellence in weather, climate and environmental forecasts and severe weather warnings for the protection of life and property. They provide forecasts and guidance for the public, to our government and defence colleagues as well as the private sector. As an example, if you’ve been on a plane over Europe, Middle East, or Africa; that plane took off because the Met Office (as one of two World Aviation Forecast Centres) provided a forecast. This article explains one of the ways they use AWS to collect these observations, which has freed them to focus more on top quality delivery for their customers.\nRead the full post at the AWS HPC Blog.\n","date":"February 2, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dalle-hpc-01_hub722fbb4e61ec1893436c76a61744c34_863441_460x200_fill_box_smart1_3.png","permalink":"/post/how-to-arm-a-world-leading-forecast-model-with-aws-graviton-and-lambda.html","title":"How to Arm a world-leading forecast model with AWS Graviton and Lambda"},{"categories":[],"contents":"It’s often difficult to keep track of all the announcements AWS is making around HPC. Come and join us on Feb. 9th for a quick overview of the latest and greatest AWS HPC products and services launched over the past year. You will hear directly from the AWS HPC engineers and product managers who have built these exciting new offerings.\nRead the full post at the AWS HPC Blog.\n","date":"January 28, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/AdobeStock_323435178-2_hu13e5e8a3b187b767040c8014c07116cb_111850_460x200_fill_q90_box_smart1.jpeg","permalink":"/post/join-us-for-our-hpc-speeds-n-feeds-event-on-feb-9.html","title":"Join us for our HPC “Speeds n’ Feeds” event on Feb. 9"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"Slurm\u0026rsquo;s job management API, coupled with ParallelCluster\u0026rsquo;s own API for managing cluster infrastructure opens up a lot of doors to create new ways for your users to interact with HPC resources - in this case without having to leave their familiar Jupyter notebook environment.\nWe made this video as part of the launch of ParallelCluster 3 - we want to make it easy to migrate all your workflows from to Slurm, and we figured it would be easier if you heard abotu it from the source.\nSo we enlisted the help of SchedMD\u0026rsquo;s Director of Cloud Engineering, Nick Ihli to help us show you how it\u0026rsquo;s a lot easier than it might look.\nJoining Nick today is Josiah Bjorgaard from the AWS APN Solution Architecture team, who regularly works with Nick and the team from SchedMD to solve lots of customer problems.\nThis is the final part of a 5-part series where we covered:\nPart 1) Command line syntax and which Slurm commands map to familiar SGE ones. Part 2) Job Scripts - what\u0026rsquo;s the stuff you need to care about to adjust ytour scripts? What\u0026rsquo;s actually easier with Slurm? Part 3) Array Jobs - Slurm has a really elegant way to handle array jobs, and also has some really nice SGE-like commands that will fool you into thinking nothing actually changed. Part 4) Job Accounting - everyone who has more than 1 user should look at this, because it\u0026rsquo;ll help you manage what your users are doing, limit what damage they can do, and - most importantly - help you understand how you can tune the experience for them so it\u0026rsquo;s even better. Part 5) Slurm\u0026rsquo;s API for controlling jobs. Coupled with ParallelCluster\u0026rsquo;s API, you can create some really imaginative solutions for your site. \u0026lsquo;Infrastructure as Code\u0026rsquo; takes on a whole new meaning.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"January 27, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/NzTXEA4zl2E_hu8609efdbcc2e68c3924c557ee8481f41_10930_460x200_fill_box_smart1_3.png","permalink":"/post/how-to-explore-slurms-job-management-api-from-a-python-notebook-part-5.html","title":"How to explore Slurm's job management API from a Python notebook - (Part 5)"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"In a previous show (https://youtu.be/6gAwAK5IJ2w), we talked about \u0026lsquo;Infrastructure as Code\u0026rsquo;, or how the ParallelCluster 3 YAML config translated to an actual cluster running, ready to take jobs.\nToday, Austin Cherian joins us again to walk through standing up the cluster using that config, and how we can adjust the cluster on the fly if, for example, we needed to add some new node types into the compute fleets, like if we need some fat memory nodes, or a new GPU type.\nThe reference guide for doing dynamic updates is here: https://docs.aws.amazon.com/parallelcluster/latest/ug/using-pcluster-update-cluster-v3.html\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"January 20, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/Vwfx7ruTrUw_hu44fa3306f6a559fa80c402fab28912f1_18883_460x200_fill_box_smart1_3.png","permalink":"/post/parallelcluster-3-launch-and-cluster-operations.html","title":"ParallelCluster 3 - Launch and Cluster Operations"},{"categories":["AWS ParallelCluster"],"contents":"ParallelCluster 3 was a major release with several changes and a lot of new features. To help get you started migrating your clusters, we describe the config file converter tool which is part of the ParallelCluster (\u0026gt;= v3.0.1) command line interface (CLI).\nRead the full post at the AWS HPC Blog.\n","date":"January 20, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/pc3-header-1260x630_hu6d44aabb85e747b20df152cdde661f68_411804_460x200_fill_box_smart1_3.png","permalink":"/post/using-the-parallelcluster-3-configuration-converter.html","title":"Using the ParallelCluster 3 Configuration Converter"},{"categories":["AWS ParallelCluster"],"contents":"Processing large amounts of complex data often requires leveraging a mix of different Amazon EC2 instance types. These types of computations also benefit from shared, high performance, scalable storage like Amazon FSx for Lustre. A way to save costs on your analysis is to use Amazon EC2 Spot Instances, which can help to reduce EC2 costs up to 90% compared to On-Demand Instance pricing. This post will guide you in the creation of a fault-tolerant cluster using AWS ParallelCluster. We will explain how to configure ParallelCluster to automatically unmount the Amazon FSx for Lustre filesystem and resubmit the interrupted jobs back into the queue in the case of Spot interruption events.\nRead the full post at the AWS HPC Blog.\n","date":"January 19, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-77-header-1260x630_hu9c3d371a6bdbe8d4a8d81f23e4822cdd_339260_460x200_fill_box_smart1_3.png","permalink":"/post/using-spot-instances-with-aws-parallelcluster-and-amazon-fsx-for-lustre.html","title":"Using Spot Instances with AWS ParallelCluster and Amazon FSx for Lustre"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"On Jan 10, the new Hpc6a instance became generally available in two regions. This instance is built from the ground up for HPC and comes with a number of interesting innovations. Neil and Heidi from our application performance team came along to show results from real analyses of real codes running at tens of thousands of cores, with some of the hardest applications on Earth.\nThe instance details are here: https://aws.amazon.com/ec2/instance-types/hpc6/\n\u0026hellip; and the blog post talking about them is here: https://hpc.news/Hpc6aLaunch\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"January 13, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/oB7q-wKzdoA_hu2156695b8989b8086ffdf037c345ee8e_17553_460x200_fill_box_smart1_3.png","permalink":"/post/hpc-on-aws-just-got-faster-and-lower-cost-with-the-launch-of-hpc6a.html","title":"HPC on AWS just got faster and lower-cost with the launch of Hpc6a"},{"categories":["AWS ParallelCluster"],"contents":"This blog post shows how you can create and manage custom AMI images for AWS ParallelCluster 3 using the new AMI creation and management process, which is built using EC2 Image Builder.\nRead the full post at the AWS HPC Blog.\n","date":"January 13, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dalle-hpc-01_hub722fbb4e61ec1893436c76a61744c34_863441_460x200_fill_box_smart1_3.png","permalink":"/post/custom-amis-with-parallelcluster-3.html","title":"Custom AMIs with ParallelCluster 3"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"In December, we introduced EnginFrame with AWS HPC Connector, which is a new feature in EnginFrame to help customers leverage managed HPC resources in AWS as well as their on-prem systems - that means a single interface so administrators can make easy to use workflows available no matter where they live.\nIt means highly specialized users like scientists and engineers can use EnginFrame’s portal interface to run their important workflows without having to understand the detailed operation of infrastructure underneath. HPC is, after all, a tool used by humans. Their productivity is the real measure of success, and we think AWS HPC Connector will make a big difference to them.\nIn this talk, Fabrizio Chignoli (who leads the EnginFrame dev team) shows us how it all works and what the end user experience looks like.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"January 11, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/ei0ioOV1nps_hufdf84d889d3f911df566a895eed8a27a_17770_460x200_fill_box_smart1_3.png","permalink":"/post/make-your-hpc-users-highly-productive-using-enginframe-with-aws-hpc-connector.html","title":"Make your HPC users highly productive using EnginFrame with AWS HPC Connector"},{"categories":[],"contents":"This blog post shows you how to deploy an HPC cluster for Windows workloads. We have provided an AWS CloudFormation template that automates the creation process to deploy an HPC Pack 2019 Windows cluster. This will help you get started quickly to run Windows-based HPC workloads, while leveraging highly scalable, resilient, and secure AWS infrastructure. As an example, we show how to run a sample parametric sweep for EnergyPlus, an open source energy simulation tool maintained by the U.S. Department of Energy’s Building Technology Office.\nRead the full post at the AWS HPC Blog.\n","date":"January 11, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-59-header-1260x630_hucd550da5407e4c3f953e2d0a5ac612e6_491196_460x200_fill_box_smart1_3.png","permalink":"/post/running-windows-hpc-workloads-using-hpc-pack-in-aws.html","title":"Running Windows HPC Workloads using HPC Pack in AWS"},{"categories":["AWS Batch","Life Sciences"],"contents":"We have been working with a team of researchers at the Max Planck Institute, helping them adopt the AWS cloud for drug research applications in the pharmaceutical industry. In this post, we’ll focus on how the team at Max Planck obtained thousands of EC2 Spot Instances spread across multiple AWS Regions for running their compute intensive simulations in a cost-effective manner, and how their solution will be enhanced further using the new Spot Placement Score API.\nRead the full post at the AWS HPC Blog.\n","date":"January 5, 2022","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dalle-hpc-01_hub722fbb4e61ec1893436c76a61744c34_863441_460x200_fill_box_smart1_3.png","permalink":"/post/accelerating-drug-discovery-with-amazon-ec2-spot-instances.html","title":"Accelerating drug discovery with Amazon EC2 Spot Instances"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"reInvent \u0026lsquo;21 is over, but it was a busy week with new tech developments we\u0026rsquo;ve built for customers in pretty much every area of HPC: storage, networking and compute.\nWe talk with our HPC General Manager, Ian Colle, about ZFS, Lustre, Graviton3, DDR5, EFA at 400 Gb/s on dozens of instances \u0026hellip; and a lot more.\nIf you didn\u0026rsquo;t get to reinvent, or even if you did and couldn\u0026rsquo;t be in 10 places at once, this is a compact recap for you.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"December 14, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/3K11LHzKP7I_hu9d63e016ac2914cd75f04ae2868bd373_16381_460x200_fill_box_smart1_3.png","permalink":"/post/recap-on-all-the-hpc-developments-from-reinvent-21.html","title":"Recap on all the HPC developments from re:invent 21"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"CDI (Cloud Digital Interface) uses EFA to blast broadcast quality video, uncompressed, from instance to instance in the cloud as live video feeds its way through complex stacks of software and makes its way to your TV set.\nIt\u0026rsquo;s one of the more unusual use cases we\u0026rsquo;ve heard for a technology that we invented for keeping highly parallel scientific applications in sync when running in HPC environments in the cloud. Yes, that\u0026rsquo;s weird.\nEvan Statton from our media and entertainment tech team talks about how this happened and why it\u0026rsquo;s so useful.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"December 9, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/x3zCTVP_LKQ_hu2f9876a6458d59f7eb506b16023dd819_19218_460x200_fill_box_smart1_3.png","permalink":"/post/what-supercomputers-scientists-and-tv-stations-have-in-common.html","title":"What supercomputers, scientists and TV stations have in common"},{"categories":[],"contents":"Today we’re introducing AWS HPC Connector, a new feature in NICE EnginFrame that allows customers to leverage managed HPC resources on AWS. With this release, EnginFrame provides a unified interface for administrators to make hybrid HPC resources available to their users both on-premises and within AWS. In this post, we’ll provide some context around EnginFrame’s typical use cases, and show how you can use AWS HPC Connector to stand up HPC compute resources on AWS.\nRead the full post at the AWS HPC Blog.\n","date":"December 6, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-111-header-1260x630_hu97a7c7c7e0cb902bce7083d2a412e0b3_338240_460x200_fill_box_smart1_3.png","permalink":"/post/introducing-aws-hpc-connector-for-nice-enginframe.html","title":"Introducing AWS HPC Connector for NICE EnginFrame"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"Mike Lin is the creator and maintainer of miniWDL and has been working with our Genomics and ML teams to integrate it with Sagemaker Studio and AWS Batch. The result is a working environment that\u0026rsquo;s completely familiar and easy to use (it\u0026rsquo;s Jupyter, after all) and yet has access to all the elasticity of the cloud, but without ever needing to leave the familiar notebook interface to make that magic happen.\nIt\u0026rsquo;s a really cool example of bringing the tools to science, rather than forcing the scientists to understand yet another new tool.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"December 2, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/N-IlEZKa_-0_huf67225f685495577f3961f4d2129f75b_17323_460x200_fill_box_smart1_3.png","permalink":"/post/miniwdl-workflows-with-100-cloud-elasticity-and-no-devops-geekery.html","title":"miniWDL workflows with 100% cloud elasticity, and no DevOps geekery"},{"categories":["AWS Batch","AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"If you’re a frequent user of Batch queues, you have opinions about whether it’s scheduling is “fair” on your users.\nBut \u0026lsquo;fair\u0026rsquo; means different things to everyone - what’s needed are some levers and dials that allow you to figure out what ‘fair’ means for your organization.\nAWS Batch now has Fair Share Scheduling, and it comes with a whole lot of controls.\nWe’re joined this week on Tech Shorts by Aswin Damodar from the Batch engineering team, and Christian Kniep, our Snr Dev Advocate for HPC \u0026amp; Batch, to run through all these controls and what they mean.\nI’m betting this video is going to get watched a LOT. Fair-share scheduling is a complex topic, but the discussion makes it a lot easier to understand that reading the documentation probably will the first time.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"November 24, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/Ws_fvv1_Sv8_huaf89dd701db195aa00153f93b0d5b6d2_19687_460x200_fill_box_smart1_3.png","permalink":"/post/fair-share-scheduling-to-maximize-user-happiness-in-aws-batch.html","title":"Fair share scheduling to maximize user happiness in AWS Batch"},{"categories":["AWS Elastic Fabric Adapter"],"contents":"We’re going to take you into the world of broadcast video, and explain how it led to us announcing today the general availability of EFA on smaller instance sizes. For a range of applications, this is going to save customers a lot of money because they no longer need to use the biggest instances in each instance family to get HPC-style network performance. But the story of how we got there involves our Elastic Fabric Adapter (EFA), some difficult problems presented to us by customers in the entertainment industry, and an invention called the Cloud Digital Interface (CDI). And it started not very far from Hollywood.\nRead the full post at the AWS HPC Blog.\n","date":"November 24, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-75-f2_hu8b7ec91ac9d4cd4aacc59e67fa2f80c0_57024_460x200_fill_box_smart1_3.png","permalink":"/post/how-we-enabled-uncompressed-live-video-with-cdi-over-efa.html","title":"How we enabled uncompressed live video with CDI over EFA"},{"categories":["Life Sciences"],"contents":"This blog provides an overview of NVIDIA’s Clara Parabricks along with a guide on how to use Parabricks within the AWS Marketplace. It focuses on germline analysis for whole genome and whole exome applications using GPU accelerated bwa-mem and GATK’s HaplotypeCaller.\nRead the full post at the AWS HPC Blog.\n","date":"November 23, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-50-header-1260x630_huceda652d4a941768818f7ecb36678605_319125_460x200_fill_box_smart1_3.png","permalink":"/post/benchmarking-the-nvidia-clara-parabricks-germline-pipeline-on-aws.html","title":"Benchmarking the NVIDIA Clara Parabricks germline pipeline on AWS"},{"categories":["Life Sciences"],"contents":"OMass Therapeutics, a biotechnology company identifying medicines against highly validated target ecosystems, used Yellowdog on AWS to analyze and screen 337 million compounds in 7 hours, a task which would have taken two months using an on-premises HPC cluster. YellowDog, based in Bristol in the UK, ran the drug discovery application on an extremely large, multi-region cluster in AWS with the AWS ‘pay-as-you-go’ pricing model. It provided a central, unified interface to monitor and manage AWS Region selection, compute provisioning, job allocation and execution. The entire workload completed in 65 minutes, enabling scientists to start work on analysis the same day, significantly accelerating the drug discovery process. In this post, we’ll discuss the AWS and YellowDog services we deployed, and the mechanisms used to scale to 3.2m vCPUs using multiple EC2 instance types across multiple regions in 33 minutes, running at a 95% utilization rate.\nRead the full post at the AWS HPC Blog.\n","date":"November 23, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-102-header-1260x630_hu8f9413a4b239520286ea0a6637858494_468801_460x200_fill_box_smart1_3.png","permalink":"/post/running-a-32m-vcpu-hpc-workload-on-aws-with-yellowdog.html","title":"Running a 3.2M vCPU HPC Workload on AWS with YellowDog"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"As part of the launch of ParallelCluster 3, we want to make it easy to migrate all your workflows from to Slurm, but we know that it can seem daunting. So we enlisted the help of SchedMD\u0026rsquo;s Director of Cloud Engineering, Nick Ihli to help us show you how it\u0026rsquo;s a lot easier than it might look.\n(Seriously, you may need to change very little).\nOver the next 5 x Tech Shorts we\u0026rsquo;ll show:\nPart 1) Command line syntax and which Slurm commands map to familiar SGE ones. Part 2) Job Scripts - what\u0026rsquo;s the stuff you need to care about to adjust ytour scripts? What\u0026rsquo;s actually easier with Slurm? Part 3) Array Jobs - Slurm has a really elegant way to handle array jobs, and also has some really nice SGE-like commands that will fool you into thinking nothing actually changed. Part 4) Job Accounting - everyone who has more than 1 user should look at this, because it\u0026rsquo;ll help you manage what your users are doing, limit what damage they can do, and - most importantly - help you understand how you can tune the experience for them so it\u0026rsquo;s even better. Part 5) Slurm\u0026rsquo;s API for controlling jobs. Coupled with ParallelCluster\u0026rsquo;s API, you can create some really imaginative solutions for your site. \u0026lsquo;Infrastructure as Code\u0026rsquo; takes on a whole new meaning.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"November 18, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/TzTIN17CG-s_hu68488fc25ed5c68bbd3d2483217ec681_11614_460x200_fill_box_smart1_3.png","permalink":"/post/easy-migration-from-sge-to-slurm-part-4-slurm-accounting.html","title":"Easy migration from SGE to Slurm - Part 4 Slurm Accounting"},{"categories":["AWS ParallelCluster","AWS Batch","AWS Elastic Fabric Adapter"],"contents":"This year, we’ve launched a lot of new capabilities for HPC customers, making AWS the best place for the length and breadth of their workflows. EFA went mainstream and is now available in sixteen instance families for fast fabric capabilities for scaling MPI and NCCL codes. We’ve written deep-dive studies to explore and explain the optimizations that will drive your workloads faster in the cloud than elsewhere. We released a major new version of AWS ParallelCluster with its own API for controlling the cluster lifecycle. AWS Batch became deeply integrated into AWS Step Functions and now supports fair-share scheduling, with multiple levers to control the experience. Today we’re signaling the arrival of a new HPC-dedicated instance family – the Hpc6a – and an enhanced EnginFrame that will bring the best of the cloud and on-premises together in a single interface.\nRead the full post at the AWS HPC Blog.\n","date":"November 18, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/Hpc6a-and-EF-blog_hucb8505b8da7c05e00a2f7f220be00d68_180954_460x200_fill_box_smart1_3.png","permalink":"/post/coming-soon-dedicated-hpc-instances-and-hybrid-functionality.html","title":"Coming soon: dedicated HPC instances and hybrid functionality"},{"categories":["AWS ParallelCluster"],"contents":"HPC systems are traditionally access through a Command Line Interface (CLI) where the users submit and manage their computational jobs. Depending on their experience and sophistication, the CLI can be a daunting experience for users not accustomed in using it. Fortunately, the cloud offers many other options for users to submit and manage their computational jobs. In this blog post we will cover how to create a serverless API to interact with an HPC system in the the cloud built with AWS ParallelCluster.\nRead the full post at the AWS HPC Blog.\n","date":"November 17, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dalle-hpc-01_hub722fbb4e61ec1893436c76a61744c34_863441_460x200_fill_box_smart1_3.png","permalink":"/post/how-to-manage-hpc-jobs-using-a-serverless-api.html","title":"How to manage HPC jobs using a serverless API"},{"categories":["AWS ParallelCluster"],"contents":"The Slurm Workload Manager by SchedMD is a popular HPC scheduler and is supported by AWS ParallelCluster, an elastic HPC cluster management service offered by AWS. Traditional HPC workflows involve logging into a head node and running shell commands to submit jobs to a scheduler and check job status. Modern distributed systems often use representational […]\nRead the full post at the AWS HPC Blog.\n","date":"November 17, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-53-fig1_hud03abf5f6601bb57d2a60aa98050fe91_98332_460x200_fill_box_smart1_3.png","permalink":"/post/using-the-slurm-rest-api-to-integrate-with-distributed-architectures-on-aws.html","title":"Using the Slurm REST API to integrate with distributed architectures on AWS"},{"categories":["AWS ParallelCluster"],"contents":"In September, we announced the release of AWS ParallelCluster 3, a major release with lots of changes and new features. To help get you started migrating your clusters, we provided the Moving from AWS ParallelCluster 2.x to 3.x guide. We know moving versions can be a quite an undertaking, so we’re augmenting that official documentation with additional color and context on a few key areas. With this blog post, we’ll focus on the configuration file format changes for ParallelCluster 3, and how they map back to the same configuration sections for ParallelCluster 2.\nRead the full post at the AWS HPC Blog.\n","date":"November 15, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/pc3-header-1260x630_hu6d44aabb85e747b20df152cdde661f68_411804_460x200_fill_box_smart1_3.png","permalink":"/post/deep-dive-into-the-aws-parallelcluster-3-configuration-file.html","title":"Deep dive into the AWS ParallelCluster 3 configuration file"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"As part of the launch of ParallelCluster 3, we want to make it easy to migrate all your workflows from to Slurm, but we know that it can seem daunting. So we enlisted the help of SchedMD\u0026rsquo;s Director of Cloud Engineering, Nick Ihli to help us show you how it\u0026rsquo;s a lot easier than it might look.\n(Seriously, you may need to change very little).\nOver the next 5 x Tech Shorts we\u0026rsquo;ll show:\nPart 1) Command line syntax and which Slurm commands map to familiar SGE ones. Part 2) Job Scripts - what\u0026rsquo;s the stuff you need to care about to adjust ytour scripts? What\u0026rsquo;s actually easier with Slurm? Part 3) Array Jobs - Slurm has a really elegant way to handle array jobs, and also has some really nice SGE-like commands that will fool you into thinking nothing actually changed. Part 4) Job Accounting - everyone who has more than 1 user should look at this, because it\u0026rsquo;ll help you manage what your users are doing, limit what damage they can do, and - most importantly - help you understand how you can tune the experience for them so it\u0026rsquo;s even better. Part 5) Slurm\u0026rsquo;s API for controlling jobs. Coupled with ParallelCluster\u0026rsquo;s API, you can create some really imaginative solutions for your site. \u0026lsquo;Infrastructure as Code\u0026rsquo; takes on a whole new meaning.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"November 11, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/PVO7_fZAT0I_huda0e05e5851debaaa6e07579f89a8112_10513_460x200_fill_box_smart1_3.png","permalink":"/post/easy-migration-from-sge-to-slurm-part-3-array-jobs.html","title":"Easy migration from SGE to Slurm - Part 3 - Array Jobs"},{"categories":["AWS Elastic Fabric Adapter"],"contents":"We have recently launched three new Amazon EC2 instances types enabled with Elastic Fabric Adapter (EFA), our network interface for Amazon EC2 instances that enables customers to run applications requiring high levels of inter-node communications at scale on AWS. These bring our EFA-enabled count to sixteen different instance families covering a wide range of use cases. EFA is going mainstream and we are just getting started.\nRead the full post at the AWS HPC Blog.\n","date":"November 11, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/efa-mainstream-f1_huf657f80e4c5a3ce0eb3e41ee82684d27_451849_460x200_fill_box_smart1_3.png","permalink":"/post/efa-is-now-mainstream-and-thats-a-good-thing.html","title":"EFA is now mainstream, and that’s a Good Thing"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"As part of the launch of ParallelCluster 3, we want to make it easy to migrate all your workflows from to Slurm, but we know that it can seem daunting. So we enlisted the help of SchedMD\u0026rsquo;s Director of Cloud Engineering, Nick Ihli to help us show you how it\u0026rsquo;s a lot easier than it might look.\n(Seriously, you may need to change very little).\nOver the next 5 x Tech Shorts we\u0026rsquo;ll show:\nPart 1) Command line syntax and which Slurm commands map to familiar SGE ones. Part 2) Job Scripts - what\u0026rsquo;s the stuff you need to care about to adjust ytour scripts? What\u0026rsquo;s actually easier with Slurm? Part 3) Array Jobs - Slurm has a really elegant way to handle array jobs, and also has some really nice SGE-like commands that will fool you into thinking nothing actually changed. Part 4) Job Accounting - everyone who has more than 1 user should look at this, because it\u0026rsquo;ll help you manage what your users are doing, limit what damage they can do, and - most importantly - help you understand how you can tune the experience for them so it\u0026rsquo;s even better. Part 5) Slurm\u0026rsquo;s API for controlling jobs. Coupled with ParallelCluster\u0026rsquo;s API, you can create some really imaginative solutions for your site. \u0026lsquo;Infrastructure as Code\u0026rsquo; takes on a whole new meaning.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"November 9, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/HYMqq0L6fLU_hu11cd42374864b10a6cf2d2d4de2a838d_10151_460x200_fill_box_smart1_3.png","permalink":"/post/easy-migration-from-sge-to-slurm-part-2-job-scripts.html","title":"Easy migration from SGE to Slurm - Part 2 - Job Scripts"},{"categories":["AWS Batch"],"contents":"Today we are announcing fair-share scheduling (FSS) for AWS Batch, which provides fine-grain control of the scheduling behavior by using a scheduling policy. With FSS, customers can prevent “unfair” situations caused by strict first-in, first-out scheduling where high priority jobs can’t “jump the queue” without draining other jobs first. You can now balance resource consumption between groups of workloads and have confidence that the shared compute environment is not dominated by a single workload. In this post, we’ll explain how fair-share scheduling works in more detail. You’ll also find a link to a step-by-step workshop at the end of this post, so you can try it out yourself.\nRead the full post at the AWS HPC Blog.\n","date":"November 9, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-72-f6_hu35eb6ec92643d08d1d6e316e44146b34_1018794_460x200_fill_box_smart1_1.gif","permalink":"/post/introducing-fair-share-scheduling-for-aws-batch.html","title":"Introducing fair-share scheduling for AWS Batch"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"As part of the launch of ParallelCluster 3, we want to make it easy to migrate all your workflows from to Slurm, but we know that it can seem daunting. So we enlisted the help of SchedMD\u0026rsquo;s Director of Cloud Engineering, Nick Ihli to help us show you how it\u0026rsquo;s a lot easier than it might look.\n(Seriously, you may need to change very little).\nOver the next 5 x Tech Shorts we\u0026rsquo;ll show:\nPart 1) Command line syntax and which Slurm commands map to familiar SGE ones. Part 2) Job Scripts - what\u0026rsquo;s the stuff you need to care about to adjust ytour scripts? What\u0026rsquo;s actually easier with Slurm? Part 3) Array Jobs - Slurm has a really elegant way to handle array jobs, and also has some really nice SGE-like commands that will fool you into thinking nothing actually changed. Part 4) Job Accounting - everyone who has more than 1 user should look at this, because it\u0026rsquo;ll help you manage what your users are doing, limit what damage they can do, and - most importantly - help you understand how you can tune the experience for them so it\u0026rsquo;s even better. Part 5) Slurm\u0026rsquo;s API for controlling jobs. Coupled with ParallelCluster\u0026rsquo;s API, you can create some really imaginative solutions for your site. \u0026lsquo;Infrastructure as Code\u0026rsquo; takes on a whole new meaning.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"November 4, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/zCEN4GblrRs_hu44c9178d7d41f66d41bebfd672368ae4_11145_460x200_fill_box_smart1_3.png","permalink":"/post/easy-migrating-from-sge-to-slurm-part-1-command-line-tools.html","title":"Easy migrating from SGE to Slurm - Part 1 - Command Line Tools"},{"categories":["Financial Services"],"contents":"Many shared file systems are used in supporting read-intensive applications, like financial backtesting. These applications typically exploit copies of datasets whose authoritative copy resides somewhere else. For small datasets, in-memory databases and caching techniques can yield impressive results. However, low latency flash-based scalable shared file systems can provide both massive IOPs and bandwidth. They’re also easy to adopt because of their use of a file-level abstraction. In this post, I’ll share how to easily create and scale a shared, distributed POSIX compatible file system that performs at local NVMe speeds for files opened read-only.\nRead the full post at the AWS HPC Blog.\n","date":"November 4, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/scaling-read-fs-f1_huf5a5e4cadd8cbb459630478a29da4695_168803_460x200_fill_box_smart1_3.png","permalink":"/post/scaling-a-read-intensive-low-latency-file-system-to-10m-iops.html","title":"Scaling a read-intensive, low-latency file system to 10M+ IOPs"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","CAE/CFD","Life Sciences"],"contents":"Today we launched the new Amazon EC2 C6i instance family which is powered by the Intel Xeon Ice Lake processor and comes equipped with our Elastic Fabric Adapter.\nNeil Ashton and Nicola Venuti joined us to talk about CFD performance on this new instance family, and spent some time comparing network connectivity options, too.\nThis is a two part series of Tech Shorts:\nPart 1) Discuss C6i and how it\u0026rsquo;s put together. Look at OpenFOAM and Siemens Simcenter StarCCM+ Part 2) Ansys Fluent, under populated cores, and some on-premises comparisons for calibration.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"November 2, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/9w9-KhZLOnU_hu59e6b8103d818b2a81f6a11a65830d2e_14784_460x200_fill_box_smart1_3.png","permalink":"/post/cfd-performance-on-ice-lake-cpu-with-the-amazon-ec2-c6i-part-2.html","title":"CFD performance on Ice Lake CPU with the Amazon EC2 C6i (Part 2)"},{"categories":["AWS Batch","Life Sciences"],"contents":"In this blog post, we’ll describe an ensemble run of 20K simulations to accelerate the drug discovery process, while also optimizing for run time and cost. We used two popular open-source packages — GROMACS, which does a molecular dynamics simulations, and pmx, a free-energy calculation package from the Computational Biomolecular Dynamics Group at Max Planck Institute in Germany.\nRead the full post at the AWS HPC Blog.\n","date":"November 2, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dalle-hpc-01_hub722fbb4e61ec1893436c76a61744c34_863441_460x200_fill_box_smart1_3.png","permalink":"/post/running-20k-simulations-in-3-days-to-accelerate-early-stage-drug-discovery-with-aws-batch.html","title":"Running 20k simulations in 3 days to accelerate early stage drug discovery with AWS Batch"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","CAE/CFD","Life Sciences"],"contents":"Today we launched the new Amazon EC2 C6i instance family which is powered by the Intel Xeon Ice Lake processor and comes equipped with our Elastic Fabric Adapter.\nNeil Ashton and Nicola Venuti joined us to talk about CFD performance on this new instance family, and spent some time comparing network connectivity options, too.\nThis is a two part series of Tech Shorts:\nPart 1) Discuss C6i and how it\u0026rsquo;s put together. Look at OpenFOAM and Siemens Simcenter StarCCM+ Part 2) Ansys Fluent, under populated cores, and some on-premises comparisons for calibration.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"October 29, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/Ow7yP3AhyRY_hu3fa7f207516de9b30fc3da348e5818b4_13131_460x200_fill_box_smart1_3.png","permalink":"/post/cfd-performance-on-ice-lake-cpu-with-the-amazon-ec2-c6i-part-1.html","title":"CFD performance on Ice Lake CPU with the Amazon EC2 C6i (Part 1)"},{"categories":["Life Sciences","AWS Batch"],"contents":"Last year, we published the Genomics Secondary Analysis Using AWS Step Functions and AWS Batch solution as a companion solution to the Genomics Data Transfer, Analytics, and Machine Learning Using AWS Services whitepaper. Since then, many customers have used the secondary analysis solution to automate their bioinformatics pipelines in AWS. A common pain point expressed […]\nRead the full post at the AWS HPC Blog.\n","date":"October 27, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dalle-hpc-01_hub722fbb4e61ec1893436c76a61744c34_863441_460x200_fill_box_smart1_3.png","permalink":"/post/using-aws-batch-console-support-for-step-functions-workflows.html","title":"Using AWS Batch Console Support for Step Functions Workflows"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"Last month we talked about the evolution of HPC workloads in the financial world. We agreed with Alex Kimber, our global FSI HPC expert, to come back and talk about how to deconstruct these very complex, high-speed, task scheduling environments and map them onto AWS services. This is cool because it lets us scale them even further in a bunch of new ways. it also means we can solve some tricky organizational problems (like sharing infra and cross-charging between depts) without any fuss at all.\nAlex dives into this area in today\u0026rsquo;s discussion.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"October 21, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/4SfrmIyu1Ss_hu387dd67435bf43e4b5d72b85e4e3c554_19561_460x200_fill_box_smart1_3.png","permalink":"/post/how-to-deconstruct-fsi-grid-scheduling-and-mapping-it-onto-aws-services.html","title":"How to deconstruct FSI Grid scheduling and mapping it onto AWS services"},{"categories":["Financial Services"],"contents":"The Financial Services industry makes significant use of high performance computing (HPC) but it tends to be in the form of loosely coupled, embarrassingly parallel workloads to support risk modelling. The infrastructure tends to scale out to meet ever increasing demand as the analyses look at more and finer grained data. At AWS we’ve helped many customers tackle scaling challenges are noticing some common themes. In this post we describe how HPC teams are thinking about how they deliver compute capacity today, and highlight how we see the solutions converging for the future.\nRead the full post at the AWS HPC Blog.\n","date":"October 21, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dalle-hpc-01_hub722fbb4e61ec1893436c76a61744c34_863441_460x200_fill_box_smart1_3.png","permalink":"/post/the-convergent-evolution-of-grid-computing-in-financial-services.html","title":"The Convergent Evolution of Grid Computing in Financial Services"},{"categories":["NICE DCV"],"contents":"Recently, we talked about the advances NICE DCV has made to push pixels from cloud-hosted desktops or applications over the internet even more efficiently than before. Since we published that post on this blog channel, we’ve been asked by several customers whether all this efficient pixel-pushing could lead to outbound data charges moving up on their AWS bill. We decided to try it on your behalf, and share the details with you in this post. The bottom line? The charges are unlikely to be significant unless you’re doing intensive streaming (such as gaming) and other cost optimizations (like AWS Instance Savings Plans) that will have more impact on your bill.\nRead the full post at the AWS HPC Blog.\n","date":"October 19, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dcv-bitrates-header-1260x630_hu22578a892c243f27e70b57977fbf0c1c_569760_460x200_fill_box_smart1_3.png","permalink":"/post/putting-bitrates-into-perspective.html","title":"Putting bitrates into perspective"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"Swaine Chen had a problem: his new group at the Genomics Institute of Singapore needed common tooling involving zillions of utilities, scripts carrying algorithms and techniques, so they could migrate their workloads from their on-premises infrastructure and over to the cloud.\nHis solution was to build a golden AMI. So much better is that he automated the production of the AMI, and documented it in GitHub, for the world to see, and for the world to use.\nToday\u0026rsquo;s discussion shows just a glimpse of what his team has built, and you can get all the goodness in the hands-on training workshops he\u0026rsquo;s offering (for free) to anyone in the world who wants to sign up.\nIf you want to know more about the workshops, or sign up, go here: https://hpc.news/giswsregister\nIf you want to plunder all this goodness and use it for the forces of good (not evil!), you can see the workshops, grab the AMI and get busy by going here: https://hpc.news/gisworkshop\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"October 15, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/q6CwmdwItDA_hu20a721a6f822c3b3e840b97d75f79c5c_19850_460x200_fill_box_smart1_3.png","permalink":"/post/getting-started-with-bioinformatics-on-aws-with-swaine-chen-from-gis.html","title":"Getting started with bioinformatics on AWS with Swaine Chen from GIS"},{"categories":["Life Sciences","AWS ParallelCluster"],"contents":"This three-part series of posts cover the price performance characteristics of running GROMACS on Amazon Elastic Compute Cloud (Amazon EC2) GPU instances. Part 1 covered some background no GROMACS and how it utilizes GPUs for acceleration. Part 2 covered the price performance of GROMACS on a particular GPU instance family running on a single instance. […]\nRead the full post at the AWS HPC Blog.\n","date":"October 14, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/gromacs-gpu-p3-f4_huf10ce6dcdd4f26ba614c243a4f0c8f51_126951_460x200_fill_box_smart1_3.png","permalink":"/post/running-gromacs-on-gpu-instances-multi-node-price-performance.html","title":"Running GROMACS on GPU instances: multi-node price-performance"},{"categories":["Life Sciences"],"contents":"This three-part series of posts cover the price performance characteristics of running GROMACS on Amazon Elastic Compute Cloud (Amazon EC2) GPU instances. Part 1 covered some background no GROMACS and how it utilizes GPUs for acceleration. This post (Part 2) covers the price performance of GROMACS on a particular GPU instance family running on a […]\nRead the full post at the AWS HPC Blog.\n","date":"October 13, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/gromacs-gpu-p2-f4-1_hue7fe863deca52da27210be1ce278f7c3_78551_460x200_fill_box_smart1_3.png","permalink":"/post/running-gromacs-on-gpu-instances-single-node-price-performance.html","title":"Running GROMACS on GPU instances: single-node price-performance"},{"categories":["Life Sciences"],"contents":"Comparing the performance of real applications across different Amazon Elastic Compute Cloud (Amazon EC2) instance types is the best way we’ve found for finding optimal configurations for HPC applications here at AWS. Previously, we wrote about price-performance optimizations for GROMACS that showed how the GROMACS molecular dynamics simulation runs on single instances, and how it […]\nRead the full post at the AWS HPC Blog.\n","date":"October 12, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/gromacs-gpu-p1-f2_hu8dc4b7f69c0bcac6a573f6c5f5924933_81017_460x200_fill_box_smart1_3.png","permalink":"/post/running-gromacs-on-gpu-instances.html","title":"Running GROMACS on GPU instances"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"\u0026lsquo;Infrastructure as Code\u0026rsquo; has a weird meaning in HPC, because it says we can write a script which stands up the entire incredibly complex software stack that is an HPC cluster, complete with MPIs, math libraries, schedulers - and even a parallel file system and visualization server.\nAustin Cherian takes us for a walk through the new ParallelCluster 3 config file and shows how the syntax aligns with the significant elements of the cluster architecture. He also talks about a few quirks we found along the way.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"October 7, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/6gAwAK5IJ2w_hu02299a379b1d7761d5e22f738036d0b4_15325_460x200_fill_box_smart1_3.png","permalink":"/post/infrastructure-as-code-parallelcluster-3s-config.html","title":"Infrastructure as Code - ParallelCluster 3's config"},{"categories":["AWS Batch"],"contents":"AWS Batch is a service that enables scientists and engineers to run computational workloads at virtually any scale without requiring them to manage a complex architecture. In this blog post, we share a set of best practices and practical guidance devised from our experience working with customers in running and optimizing their computational workloads. The readers will learn how to optimize their costs with Amazon EC2 Spot on AWS Batch, how to troubleshoot their architecture should an issue arise and how to tune their architecture and containers layout to run at scale.\nRead the full post at the AWS HPC Blog.\n","date":"October 4, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/batch-best-practices-header-1260x630_hu55fc076c912b43ec2738a252b5267de1_416650_460x200_fill_box_smart1_3.png","permalink":"/post/aws-batch-dos-and-donts-best-practices-in-a-nutshell.html","title":"AWS Batch Dos and Don’ts: Best Practices in a Nutshell"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"RONIN is probably the easiest way for a research scientist or engineer to be able to quickly jump into the cloud on AWS and get stuff done. And they\u0026rsquo;ve made a regular habit of delighting their customers by making hard things easy.\nIn this Tech Short, Tara from RONIN shows us how to spin up a Linux machine and connect to it securely. All of this happens inside the safety and governance of RONIN\u0026rsquo;s security posture and it\u0026rsquo;s budget management guardrails.\nMore on all of this is available at www.ronin.cloud.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"September 30, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/2Pb3qZqotAA_hu2f664f3cae7375cbe1c287836e8211d4_10156_460x200_fill_box_smart1_3.png","permalink":"/post/launch-a-machine-in-ronin.html","title":"Launch a machine in RONIN"},{"categories":["AWS ParallelCluster","Climate/Environment/Weather"],"contents":"The Danish Meteorological Institute (DMI) is responsible for running atmospheric, climate and ocean models covering the kingdom of Denmark. We worked together with the DMI to port and run a full numerical weather prediction (NWP) cycling dataflow with the Harmonie Numerical Weather Prediction (NWP) model to AWS. You can find a report of the porting and operational experience in the ACCORD community newsletter. In this blog post, we expand on that report to present the initial timing results from running the forecast component of Harmonie model on AWS. We also present these as-is timing results together with as-is timings attained on the supercomputing systems based on Cray XC40 and Intel Xeon based Cray XC50.\nRead the full post at the AWS HPC Blog.\n","date":"September 30, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/harmonie-header-1260x630_hu55fc076c912b43ec2738a252b5267de1_387541_460x200_fill_box_smart1_3.png","permalink":"/post/running-the-harmonie-numerical-weather-prediction-model-on-aws.html","title":"Running the Harmonie numerical weather prediction model on AWS"},{"categories":["AWS ParallelCluster"],"contents":"A major portion of the costs incurred for running Finite Element Analyses (FEA) workloads on AWS comes from the usage of Amazon EC2 instances. Amazon EC2 Spot Instances offer a cost-effective architectural choice, allowing you to take advantage of unused EC2 capacity for up to a 90% discount compared to On-Demand Instance prices. In this post, we describe how you 0can run fault-tolerant FEA workloads on Spot Instances using Ansys LS-DYNA’s checkpointing and auto-restart utility.\nRead the full post at the AWS HPC Blog.\n","date":"September 27, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/ls-dyna-spot-header-1260x630_huaf73ef8523fa6bf06283d0d0c733e8d2_399615_460x200_fill_box_smart1_3.png","permalink":"/post/cost-optimization-on-spot-instances-using-checkpoint-for-ansys-ls-dyna.html","title":"Cost-optimization on Spot Instances using checkpoint for Ansys LS-DYNA"},{"categories":["AWS ParallelCluster"],"contents":"This article was contributed by Dr. Fabio Baruffa, Sr. HPC and QC Solutions Architect at AWS, and Dr. Jesús Pérez Ríos, Group Leader at the Fritz Haber Institute, Max-Planck Society. Introduction Quantum chemistry – the study of the inherently quantum interactions between atoms forming part of molecules – is a cornerstone of modern chemistry. […]\nRead the full post at the AWS HPC Blog.\n","date":"September 24, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dalle-hpc-01_hub722fbb4e61ec1893436c76a61744c34_863441_460x200_fill_box_smart1_3.png","permalink":"/post/quantum-chemistry-calculation-with-fhi-aims-code-on-aws.html","title":"Quantum Chemistry Calculation with FHI-aims code on AWS"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Financial Services","Life Sciences"],"contents":"HPC in Financial Services is interesting for a lot of reasons that have nothing to do with banking. It\u0026rsquo;s an embarrassingly parallel workload, typically made up from zillions of short\u0026rsquo;ish jobs (seconds to minutes). And the decisions it supports are pretty big ones: whole banks and entire economies might be impacted by the outcomes. It\u0026rsquo;s interesting because there are so many fun ways you can solve the problem of \u0026lsquo;get it done fast, cheaply, and soon enough to help the people who are making really big decisions\u0026rsquo;. And whilst you expect us to say this: the cloud truly is a real game changer for this workload.\nAlex Kimber wrote the HPC book at AWS for Financial Services, but what he talks about in this short discussion puts context around the whole thing \u0026hellip; He\u0026rsquo;ll be back to expand on several of the key points in a few weeks.\nIn the meantime, his FSI HPC whitepaper is here: https://d1.awsstatic.com/whitepapers/aws-financial-services-grid-computing.pdf\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"September 23, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/pCtyWCwVgz0_hu9f18261d12870640bbb30ebe48627734_15015_460x200_fill_box_smart1_3.png","permalink":"/post/hpc-in-financial-services-is-not-boring-and-has-some-interesting-problems-to-solve.html","title":"HPC in Financial Services is not boring, and has some interesting problems to solve."},{"categories":["Life Sciences"],"contents":"Computer-aided drug discovery (CADD) has been a key player in lowering the cost and speeding up the timeline for drug development. CADD uses high performance computing (HPC) resources to virtually screen databases with billions of molecules. It can speed up the searching of potential drug molecules, and filter out molecules and compounds that are unsuitable. OpenEye Scientific developed Orion®, a cloud-based molecular design platform for CADD. Orion provides computational chemists with virtually unlimited HPC resources. These include data visualization, collaboration, and workflow management tools that help them perform calculations more efficiently. In this post, we describe the Orion architecture on AWS, and it’s capabilities to address the challenges in drug development.\nRead the full post at the AWS HPC Blog.\n","date":"September 23, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dalle-hpc-01_hub722fbb4e61ec1893436c76a61744c34_863441_460x200_fill_box_smart1_3.png","permalink":"/post/virtual-screening-of-novel-active-drug-compounds-on-aws-with-orion.html","title":"Virtual Screening of Novel Active Drug Compounds on AWS with Orion®"},{"categories":[],"contents":"The Inter University Computing Centre (IUCC) in Israel and AWS have joined forces to train Researchers and Research Software Engineers (RSEs) in the use of AWS for High Performance Computing (HPC) at the PRACE Winter School, 7-9 December 2021, and we’re calling for interested groups to sign up and join us.\nRead the full post at the AWS HPC Blog.\n","date":"September 16, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dalle-hpc-01_hub722fbb4e61ec1893436c76a61744c34_863441_460x200_fill_box_smart1_3.png","permalink":"/post/call-for-participation-prace-winter-school.html","title":"Call for participation: PRACE Winter School"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"RONIN (www.ronin.cloud) is one of our favorite HPC partners, because they\u0026rsquo;ve done a really comprehensive job providing research customers the governance and guardrails they need around budget management and data security to make insanely easy to explore and experiment with workloads in the cloud. It helps that they pack hundreds of DevOps tasks into every click so you can launch HPC clusters in less time than it takes to buy your groceries online.\nTara Madhyastha is one of RONIN\u0026rsquo;s Principal Research Scientists based out of Seattle, and she gives us a peek at a new product built on RONIN Core called \u0026lsquo;RONIN Isolate\u0026rsquo;, which takes secure enclaves and project isolation to an obsessive new level to help customers with really extreme (and particular) security and compliance needs.\nIsolate launched this week, and you can find more about it at the RONIN Blo at https://blog.ronin.cloud/ronin/\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"September 15, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dQM9Gd36Hv8_huefc9ea753b2e9b9165e29fb60887fe2e_18012_460x200_fill_box_smart1_3.png","permalink":"/post/governance-and-guardrails-for-researchers-with-ronin-isolate.html","title":"Governance and guardrails for researchers with RONIN ISOLATE"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"Today we’re announcing AWS ParallelCluster 3.\nCustomers, integrators, and other builders have told us they want to build end-to-end “recipes” for HPC, spanning the whole gamut from infrastructure to middleware, libraries, and runtime codes. They also asked to interact with ParallelCluster programmatically to create interfaces and services for their users. We worked backwards from this feedback, using thousands of conversations with customers to create a shiny new version of ParallelCluster rebuilt from the ground up.\nIn 15 minutes, we walk through the main features with Nathan Stornetta our Snr Product Manager for ParallelCluster, who also explains what customer feedback lead to each idea.\nThis is a big release with loads of new features. But the most exciting part is what it sets us up for next.\nYou can find out more about this in our launch blog at: http://hpc.news/pc3day1 and also by checking out the workshop and documentation that are linked to in the bottom of that blog post.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"September 10, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/a-99esKLcls_hu6d4db7ed1b046c85a45abe6d381216e5_10772_460x200_fill_box_smart1_3.png","permalink":"/post/introducing-parallelcluster-3-hpc-in-the-cloud-built-by-customers.html","title":"Introducing ParallelCluster 3 - HPC in the Cloud built by customers"},{"categories":["CAE/CFD","AWS ParallelCluster","Life Sciences"],"contents":"Running HPC workloads, like computational fluid dynamics (CFD), molecular dynamics, or weather forecasting typically involves a lot of moving parts. You need a hundreds or thousands of compute cores, a job scheduler for keeping them fed, a shared file system that’s tuned for throughput or IOPS (or both), loads of libraries, a fast network, and […]\nRead the full post at the AWS HPC Blog.\n","date":"September 10, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/pc3-header-1260x630_hu6d44aabb85e747b20df152cdde661f68_411804_460x200_fill_box_smart1_3.png","permalink":"/post/new-introducing-aws-parallelcluster-3.html","title":"New: Introducing AWS ParallelCluster 3"},{"categories":["AWS ParallelCluster","Climate/Environment/Weather"],"contents":"The Amazon Sustainability Data Initiative (ASDI), AWS is donating cloud resources, technical support, and access to scalable infrastructure and fast networking providing high performance computing solutions to support simulations of near-term climate using the National Center for Atmospheric Research (NCAR) Community Earth System Model Version 2 (CESM2) and its Whole Atmosphere Community Climate Model (WACCM). In collaboration with ASDI, AWS, and SilverLining, a nonprofit dedicated to ensuring a safe climate, the National Center for Atmospheric Research (NCAR) will run an ensemble of 30 climate-model simulations on AWS. The climate runs will simulate the Earth system over the period of years 2022-2070 under a median scenario for warming and make them available through the AWS Open Data Program. The simulation work will demonstrate the ability to use cloud infrastructure to advance climate models in support of robust scientific studies by researchers around the world and aims to accelerate and democratize climate science.\nRead the full post at the AWS HPC Blog.\n","date":"September 3, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/asdi-header-1260x630_hub19ea4e5f83a9a54d6fe8480e5d8f170_675052_460x200_fill_box_smart1_3.png","permalink":"/post/supporting-climate-model-simulations-to-accelerate-climate-science.html","title":"Supporting climate model simulations to accelerate climate science"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"Since our NICE DCV high performance desktop and application streaming product does such a great job of making it feel like you have a cloud dat center sitting behind your laptop screen somewhere, lots of customers have asked \u0026lsquo;Won\u0026rsquo;t that impact my bill? You guys charge for data, right?\u0026rsquo;.\nThere\u0026rsquo;s no way to give a precise answer, but what we did do was to put together a range of usage scenarios from fairly pedestrian usage (doing work processing on a remote windows machine) through to high bit-rate gaming and video streaming. We tested them all and measured their consumption. And we show you the results. Bottom line: data charges are\u0026rsquo;t really going to nudge your billing by much unless you\u0026rsquo;re getting close to 4K video streaming.\nJyothi Venkatesh and Boof put DCV to the test, and we walk you through the scenarios and our analysis.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"September 2, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/-YBh4d_zKxc_hu57c246c95071e76817d94b4faf828600_16224_460x200_fill_box_smart1_3.png","permalink":"/post/whats-the-impact-of-dcvs-pixel-streaming-on-my-aws-bill.html","title":"What's the impact of DCV's pixel streaming on my AWS Bill?"},{"categories":[],"contents":"Playtech mathematicians and game designers need accurate, detailed game play simulation results to create fun experiences for players. While software developers have been able to iterate on code in an agile manner for many years, for non-analytical solutions, mathematicians have had to rely on slow CPU-bound Monte-Carlo simulations, waiting, as software engineers once did, many hours or overnight to get the results of their latest changes. These statistics are also required as evidence of game fairness in the highly regulated online gaming business. Playtech has developed an AWS Lambda Serverless based solution that provides massive burst compute performance that allows game simulations in minutes rather than hours. This post goes into the details of the architecture, as well as some examples of using the system in our development and operations.\nRead the full post at the AWS HPC Blog.\n","date":"September 1, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/mc-playtech-header-1260x630_hu22578a892c243f27e70b57977fbf0c1c_435829_460x200_fill_box_smart1_3.png","permalink":"/post/high-burst-cpu-compute-for-monte-carlo-simulations-on-aws.html","title":"High Burst CPU Compute for Monte Carlo Simulations on AWS"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"This is a talk that was delivered at the AWS/Arm Cloud Hackathon in July 2021.\nTodd Gamblin (the creator of Spack) and his colleague Greg Becker talk us through the essential skills and concepts needed to understand how to create and deploy Spack recipes to build scientific codes. Spack massively simplifies the task of building scientific applications, which are almost defined by their insane build methods and dependency hierarchies. We made extensive use of Spack in the hackathon, and were extremely grateful for their help.\nThe Summer Hackathon ran for a week from July 12-16 in 2021. It\u0026rsquo;s purpose was to assemble the HPC community around a common goal of beginning the porting/testing/tuning process for dozens of codes to use Arm-based processors, in this case, AWS\u0026rsquo;s Graviton2\u0026rsquo;s.\nDuring the week we had a series of talks from global leaders and experts in various fields from compilers to networking, profiling debugging and tooling. We\u0026rsquo;ve published all the talks here in the HPC Tech Shorts channel, in the hope that everyone gets some value from them.\nIf you want to join in our collective effort to boost the performance of HPC workloads on Arm, DM us at Twitter (@TechHpc) or head to a-hug.org/ and join the community.\n","date":"August 26, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/wLjbJQdirsc_huf9585a20fcdd63bf690b8cc651d13f73_16698_460x200_fill_box_smart1_3.png","permalink":"/post/how-to-spack-a-software-package.html","title":"How to Spack a software package"},{"categories":["Life Sciences"],"contents":"This post was written by Swapnil Bhatkar, Cloud Engineer, NREL in collaboration with Edward Eng Ph.D. and Micah Rapp Ph.D, both SEMC/NYSBC, and Evan Bollig Ph.D. and Aniket Deshpande, both AWS. Introduction Cryo-electron microscopy (Cryo-EM) technology allows biomedical researchers to image frozen biological molecules, such as proteins, viruses and nucleic acids, and obtain structures of […]\nRead the full post at the AWS HPC Blog.\n","date":"August 17, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dalle-hpc-01_hub722fbb4e61ec1893436c76a61744c34_863441_460x200_fill_box_smart1_3.png","permalink":"/post/stion-a-software-as-a-service-for-cryo-em-data-processing-on-aws.html","title":"Stion – a Software as a Service for Cryo-EM data processing on AWS"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"This is a talk that was delivered at the AWS/Arm Cloud Hackathon in July 2021.\nWill Lovett works for Arm on the Arm compiler, and talk about how compilers work, and how to best leverage them to get the result you\u0026rsquo;re looking for when porting a code or working on performance.\nThe Summer Hackathon ran for a week from July 12-16 in 2021. It\u0026rsquo;s purpose was to assemble the HPC community around a common goal of beginning the porting/testing/tuning process for dozens of codes to use Arm-based processors, in this case, AWS\u0026rsquo;s Graviton2\u0026rsquo;s.\nDuring the week we had a series of talks from global leaders and experts in various fields from compilers to networking, profiling debugging and tooling. We\u0026rsquo;ve published all the talks here in the HPC Tech Shorts channel, in the hope that everyone gets some value from them.\nIf you want to join in our collective effort to boost the performance of HPC workloads on Arm, DM us at Twitter (@TechHpc) or head to a-hug.org/ and join the community.\n","date":"August 12, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/vsCv2F9ICqs_hu980dce07d5b50472ee74b3185273ac88_17643_460x200_fill_box_smart1_3.png","permalink":"/post/armaws-cloud-hackathon-compilers-in-hpc-their-use-and-abuse.html","title":"Arm/AWS Cloud Hackathon - Compilers in HPC, their use and abuse"},{"categories":[],"contents":"Seismic imaging is the process of positioning the Earth’s subsurface reflectors. It transforms the seismic data recorded in time at the Earth’s surface to an image of the Earth’s subsurface. This is done by back-propagating data from time to space in a given velocity model. Kirchhoff depth migration is a well-known technique used in geophysics for seismic imaging. Kirchhoff time and depth migration produce an image with higher resolution and generate an image of the subsurface for a subset class of the data, providing valuable information about the petrophysical properties of the rocks and helps to determine how accurate the velocity model is. This blog post looks at the price-performance characteristics computing Kirchhoff migration methods on GPUs using Nvidia’s GPU-optimized code.\nRead the full post at the AWS HPC Blog.\n","date":"August 12, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpc-blog-header-price-perf-1-1260x630_hu55fc076c912b43ec2738a252b5267de1_411626_460x200_fill_box_smart1_3.png","permalink":"/post/price-performance-analysis-of-amazon-ec2-gpu-instance-types-using-nvidias-gpu-optimized-seismic-code.html","title":"Price-Performance Analysis of Amazon EC2 GPU Instance Types using NVIDIA’s GPU optimized seismic code"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"This is a talk that was delivered at the AWS/Arm Cloud Hackathon in July 2021.\nTom Deakin is a computer scientist at the University of Bristol, and speaks to us about how to build for portability, and what that really means in an era of so many hardware technology choices.\nThe Summer Hackathon ran for a week from July 12-16 in 2021. It\u0026rsquo;s purpose was to assemble the HPC community around a common goal of beginning the porting/testing/tuning process for dozens of codes to use Arm-based processors, in this case, AWS\u0026rsquo;s Graviton2\u0026rsquo;s.\nDuring the week we had a series of talks from global leaders and experts in various fields from compilers to networking, profiling debugging and tooling. We\u0026rsquo;ve published all the talks here in the HPC Tech Shorts channel, in the hope that everyone gets some value from them.\nIf you want to join in our collective effort to boost the performance of HPC workloads on Arm, DM us at Twitter (@TechHpc) or head to a-hug.org/ and join the community.\n","date":"August 10, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/w49zW0pOINY_hu0d7a94c765436fa608be3a6b90b46729_17871_460x200_fill_box_smart1_3.png","permalink":"/post/armaws-cloud-hackathon-performance-portability-with-tom-deakin.html","title":"Arm/AWS Cloud Hackathon - Performance Portability with Tom Deakin"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"This is a talk that was delivered at the AWS/Arm Cloud Hackathon in July 2021.\nJohn Linford is Arm\u0026rsquo;s director of HPC and in this talk he covers why you should evolve your profiling beyond using print(), and how much time and effort this can save you. He also dives into some reasons to avoid premature optimization.\nThe Summer Hackathon ran for a week from July 12-16 in 2021. It\u0026rsquo;s purpose was to assemble the HPC community around a common goal of beginning the porting/testing/tuning process for dozens of codes to use Arm-based processors, in this case, AWS\u0026rsquo;s Graviton2\u0026rsquo;s.\nDuring the week we had a series of talks from global leaders and experts in various fields from compilers to networking, profiling debugging and tooling. We\u0026rsquo;ve published all the talks here in the HPC Tech Shorts channel, in the hope that everyone gets some value from them.\nIf you want to join in our collective effort to boost the performance of HPC workloads on Arm, DM us at Twitter (@TechHpc) or head to a-hug.org/ and join the community.\n","date":"August 5, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/vE8Chj070NY_hu7dcd471090ea64f437de89c9b7831373_16731_460x200_fill_box_smart1_3.png","permalink":"/post/armaws-cloud-hackathon-profiling-without-printf.html","title":"Arm/AWS Cloud Hackathon - Profiling without printf()"},{"categories":[],"contents":"High Performance Computing (HPC) is known as a domain where applications are well-optimized to get the highest performance possible on a platform. Unsurprisingly, a common question when moving a workload to AWS is what performance difference there may be from an existing on-premises “bare metal” platform. This blog will show the performance differential between “bare metal” instances and instances that use the AWS Nitro hypervisor is negligible for the evaluated HPC workloads.\nRead the full post at the AWS HPC Blog.\n","date":"August 5, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/bm-nitro-header-1260x620_hu2579bbb06c25871f8c5275d2ffbe4fea_278332_460x200_fill_box_smart1_3.png","permalink":"/post/bare-metal-performance-with-the-aws-nitro-system.html","title":"Bare metal performance with the AWS Nitro System"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"This is a talk that was delivered at the AWS/Arm Cloud Hackathon in July 2021.\nJeff Hammond is a computational scientist at NVIDIA and walks through techniques for getting applications to scale in HPC.\nThe Summer Hackathon ran for a week from July 12-16 in 2021. It\u0026rsquo;s purpose was to assemble the HPC community around a common goal of beginning the porting/testing/tuning process for dozens of codes to use Arm-based processors, in this case, AWS\u0026rsquo;s Graviton2\u0026rsquo;s.\nDuring the week we had a series of talks from global leaders and experts in various fields from compilers to networking, profiling debugging and tooling. We\u0026rsquo;ve published all the talks here in the HPC Tech Shorts channel, in the hope that everyone gets some value from them.\nIf you want to join in our collective effort to boost the performance of HPC workloads on Arm, DM us at Twitter (@TechHpc) or head to a-hug.org/ and join the community.\n","date":"August 3, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/Hfrc_Pqm0oY_hu5ef443b3b79bd5a762a299a8455a8bc3_16726_460x200_fill_box_smart1_3.png","permalink":"/post/armaws-cloud-hackathon-talk-application-scaling-with-jeff-hammond.html","title":"Arm/AWS Cloud Hackathon Talk - Application Scaling with Jeff Hammond"},{"categories":["NICE DCV"],"contents":"NICE DCV, our high-performance, low-latency remote-display protocol, was originally created for scientists and engineers who ran large workloads on far-away supercomputers, but needed to visualize data without moving it. Pushing pixels over limited bandwidth across the globe has been the goal of the DCV team since 2007. DCV was able to make very frugal use of very scarce bandwidth, because it was super lean, used data-compression techniques and quickly adopted cutting-edge technologies of the time from GPUs (this is HPC, after all, we left nothing on the table when it came to exploiting new gadgets). This allowed the team to create a super light-weight visualization package that could stream pixels over almost any network. Fast forward to the 2020s, and a generation of gamers, artists, and film-makers all want to do the same thing as HPC researchers- only this time there are way more pixels, because we now have HD and 4k (and some people have multiple), and for most of them, it’s 60 frames per second, or it’s not worth having. Today we have around 12x the number of pixels, and around 3x the frame rate compared to TV of circa 2007. Fortunately, networking improved a lot in that time: a high-end user’s broadband connection grew around 60x in bandwidth, but the 120x growth in computing power really tipped the balance in favor of bringing remote streaming to the masses. Still, physics remains, meaning the latency forced on us by the curvature of the earth and the speed of light, is still a challenge. We still haven’t beaten physics, but we’re making up for it by building our own global fiber network and adding more machinery (and in local and wavelength zones) to get closer to more customers as soon as we can.\nRead the full post at the AWS HPC Blog.\n","date":"July 30, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dalle-hpc-01_hub722fbb4e61ec1893436c76a61744c34_863441_460x200_fill_box_smart1_3.png","permalink":"/post/pushing-pixels-with-nice-dcv.html","title":"Pushing pixels with NICE DCV"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"Netflix has a really ambitious goal to create more content than practically anyone, but ran into headwinds when Covid-19 forced us all to lock down: it became hard to recruit and support artists, editors and other creatives without finding some really clever solutions to putting powerful compute at their fingertips anywhere in the world : a perfect job for AWS\u0026rsquo;s NICE DCV and globally-distributed infrastructure.\nBut there\u0026rsquo;s another lesson here - when you listen to how Michelle describes the way Netflix works to support their most valuable assets (clever, talented people), it should strike you as a really good lesson for how we should prosecute our mission to make scientists and engineers more powerful and productive with all these same tools.\nYou can find Michelle on linkedIn (linkedin.com/in/michellebrenner/) if you want to know more, or you can join her working at Netflix by heading to job.netflix.com where they\u0026rsquo;re hiring like crazy.\nAnd you can find out lots more about DCV here: aws.amazon.com/hpc/dcv as well as tools to get started.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"July 29, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/PUAeBQ98Odc_huf92c6d3c98752adff5f46b4a4788c9b2_23419_460x200_fill_box_smart1_3.png","permalink":"/post/how-netflix-used-dcv-and-aws-to-distribute-their-creative-workforce-and-saved-our-sanity.html","title":"How Netflix used DCV (and AWS) to distribute their creative workforce (and saved our sanity)"},{"categories":["AI/ML","AWS Batch","Financial Services"],"contents":"Batch processing is a common need across varied machine learning use cases such as video production, financial modeling, drug discovery, or genomic research. The elasticity of the cloud provides efficient ways to scale and simplify batch processing workloads while cutting costs. In this post, you’ll learn a scalable and cost-effective approach to configure AWS Batch Array jobs to process datasets that are stored on Amazon S3 and presented to compute instances with Amazon FSx for Lustre.\nRead the full post at the AWS HPC Blog.\n","date":"July 23, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dalle-hpc-01_hub722fbb4e61ec1893436c76a61744c34_863441_460x200_fill_box_smart1_3.png","permalink":"/post/scalable-and-cost-effective-batch-processing-for-ml-workloads-with-aws-batch-and-amazon-fsx.html","title":"Scalable and Cost-Effective Batch Processing for ML workloads with AWS Batch and Amazon FSx"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"Nextflow Tower became necessary when lots of Nextflow users started to leave the confines of their existing environments in search of more compute, more storage, more capabilities. Nextflow made their workflows portable, but there was still a lot of relatively complex work to be done standing up cloud infrastructure, setting limits, choosing instances, pondering VPCs, Batch compute environments \u0026hellip; You get the idea.\nNextflow Tower massively simplifies that. It integrates deeply with all these environments (lots of clouds, lots of cluster types, and even in hybrid mode) such that a researcher with a good idea can go from laptop to server to cluster to massive compute farm, step-by-step as their idea and ambition grows.\nEvan Floden, the CEO of Seqera Labs and a core developer of Nextflow (along with his friend and business partner Paolo Di Tommaso) steps in to take us for a drive of Tower and explains some of the tech underpinning it.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"July 22, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/JOguxRohITA_hudd219cce90da852777669b63f8a421b5_22316_460x200_fill_box_smart1_3.png","permalink":"/post/nextflow-tower-and-how-it-makes-it-easy-to-manage-a-lot-of-infrastructure-quickly.html","title":"Nextflow Tower  and how it makes it easy to manage a lot of infrastructure quickly."},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"NF Core is an open source repository of Nextflow workflows that can be downloaded, shared, forked, updated, tested and validated. If you\u0026rsquo;ve ever heard that quote from Isaac Newtown about seeing further by standing on the shoulders of giants: well this is the ladder modern day scientists are using to climb up those very big shoulders. Phil Ewels from the SciLife lab at Stockholm University walks us through some of the history of NF Core and gives us a demo of how easy it is to use, and the cool tools he and his project team have built around to just make it \u0026hellip; a no brainer. Every domain of science should be looking at how this works, because every field of science performs a workflow on data - and those workflows are getting increasingly complicated. If you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"July 15, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/KtFogrsqIzg_hu41b1835ed26365ecd12685a762cbd118_21861_460x200_fill_box_smart1_3.png","permalink":"/post/nf-core-is-an-example-to-all-compute-intensive-scientific-fields-they-should-all-watch-this.html","title":"NF Core is an example to all compute-intensive scientific fields. They should all watch this."},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"AWS has a LOT of storage options (block, PIOPS, object, volume, loads of file sytems), pretty much all of which can be used for HPC. That\u0026rsquo;s because we\u0026rsquo;re not forced to come up with one single massively fast storage solution that can cope with a hurricane of worst-case usage - you\u0026rsquo;re not making one-way door decisions on infrastructure. Cloud infrastructure is a two-way door and you can change your mind as many times as you like.\nSo, yes, you can have 200 GByte/s Lustre, but if you\u0026rsquo;re doing embarrassingly-parallel workloads that just need to stream data through a CPU, you\u0026rsquo;re missing an optimization right there. You can have very specifically tuned storage for very specific workloads (and specific clusters or compute environments, too, if you like). And you can have very specifically lower costs associated with it if you do.\nOur Principal Developer Advocate, Angel Pizarro (@delagoya) walks us through the options, and helps you figure out how each of these native AWS services might form part of your environment.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"July 9, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/PY_X49SQWuo_hu037381cd69167c066d0e132322f7f55a_21897_460x200_fill_box_smart1_3.png","permalink":"/post/awss-hpc-storage-storage-options-choose-use-and-abuse-heres-how.html","title":"AWS's HPC Storage storage options - choose, use and abuse. Here's how."},{"categories":[],"contents":"Now that AWS Batch supports AWS Fargate, you can submit jobs any time you like, and AWS just spins up the resources you need when you need them, REALLY quickly (because that\u0026rsquo;s what Fargate is awesome at). You don\u0026rsquo;t need to keep any resources running other than precisely the ones you need right now, and you get results right away. No managing servers, or even worrying about them. That\u0026rsquo;s why it\u0026rsquo;s serverless :-)\nOur Principal Product Manager for AWS Batch, Steve Kendrex, explains how it all works and walks us through a live example.\nDuring the talk, we mentioned this page for more information on using Batch with Fargate, and that page is here: https://aws.amazon.com/batch/features/?nc=sn\u0026amp;loc=2\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"July 1, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/weKeR-qg_-4_hub58e2583027ec479589cfd395e4d0013_22135_460x200_fill_box_smart1_3.png","permalink":"/post/batch-can-now-use-fargate-for-a-truly-serverless-experience.html","title":"Batch can now use Fargate for a truly serverless experience."},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"In recent years, we’ve all been amazed at how large areas of scientific computing have moved to abstract their workflows away from the details of systems and cluster architecture. Bioinformatics has really lead the way in this regard - exploiting the fact that they came to large scale computing without decades of legacy code.\nNextflow is an open source project that has been hugely impactful in that movement, enabling a community leverage each other\u0026rsquo;s scientific work and - honestly - stand on each others\u0026rsquo; shoulders to reach further. We didn\u0026rsquo;t need a pandemic to drive that point home, but they played their part there too (more on that in a later show).\nToday we’re joined by Evan Floden, who is a principle author of Nextflow (along with other leaders like Paolo Di Tommaso who was one of Nextflow’s creators). Evan is also the CEO of Seqera Labs, the company he and Paolo formed to provide extra support to Nextflow users, and to take management of the workflows and integration with the underlying infrastructure to a whole new level.\nEvan shows us some of Nextflow’s latest tricks and gives us an insight to some of the cool things coming in Nextflow Tower - the Seqera Labs product.\nDuring the discussion he talks a lot about the community, not least of which is:\nnf-co.re - the NF Core project which is a library of shareable and editable workflows; nextflow.io itself Seqera.io (who, by the way, are hiring like crazy at the moment). If you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"June 24, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/DCSy0zVx86Y_hudc8fbb4c9696f9bac79f966481506035_21984_460x200_fill_box_smart1_3.png","permalink":"/post/nextflow-has-changed-the-way-science-does-computing-and-energized-a-community-we-need-this.html","title":"Nextflow has changed the way science does computing and energized a community. We need this."},{"categories":["AWS Elastic Fabric Adapter"],"contents":"AWS worked backwards from an essential problem in HPC networking (MPI ranks need to exchange lots of data quickly) and found a different solution for our unique circumstances, without trading off the things customers love the most about cloud: that you can run virtually any application, at scale, and right away. Find out more about how Elastic Fabric Adapter (EFA) can help your HPC workloads scale on AWS.\nRead the full post at the AWS HPC Blog.\n","date":"June 22, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/efa-header-1260x630_huaf73ef8523fa6bf06283d0d0c733e8d2_536182_460x200_fill_box_smart1_3.png","permalink":"/post/in-the-search-for-performance-theres-more-than-one-way-to-build-a-network.html","title":"In the search for performance, there’s more than one way to build a network"},{"categories":["AWS ParallelCluster","AWS Batch"],"contents":"Containers are rapidly maturing within the high performance computing (HPC) community and we’re excited to be part of the movement: listening to what customers have to say and feeding this back to both the community and our own product and service teams. Containerization has the potential to unblock HPC environments, so AWS ParallelCluster and container-native schedulers like AWS Batch are moving quickly to reflect the best practices developed by the community and our customers. This year is the seventh consecutive year we are hosting the ‘High Performance Container Workshop’ at ISC High Performance 2021 conference (ISC’21). The workshop will be taking place on July 2nd at 2PM CEST (7AM CST). The full program for the workshop is available on the High Performance Container Workshop page at https://hpcw.github.io/\nRead the full post at the AWS HPC Blog.\n","date":"June 18, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dalle-hpc-01_hub722fbb4e61ec1893436c76a61744c34_863441_460x200_fill_box_smart1_3.png","permalink":"/post/getting-started-with-containers-in-hpc-at-isc21.html","title":"Getting started with containers in HPC at ISC’21"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"Arm and AWS are jointly sponsoring a summer hackathon that\u0026rsquo;s being run by the Arm HPC User group (A-HUG - and definitely we send a hug to everyone out there). The goal of the hackathon is to get loads of codes with Spack recipes building, running and validating on Arm based systems and identifying the ones that don\u0026rsquo;t work or don\u0026rsquo;t perform well, so we - as a community - can find them and get to work fixing them.\nIn today\u0026rsquo;s show we cover the logistics of how the hackathon will work, and especially cover the software and hardware we\u0026rsquo;re going to be using. There are a lot of new tools (especially cool things like Spack and reFrame) that we think are awesome - we\u0026rsquo;re pretty sure you\u0026rsquo;ll agree after you see them in action - Olly gives us a demo with a real test case.\nIf you\u0026rsquo;re thinking about the hackathon, check out this discussion with Olly Perks from Arm and consider signing up for the event, which is about a month away (July 12-16).\nIf you want to know more, feel free to reach out to Olly or Boof using our twitter handles or DM’ing @TechHPC.\n","date":"June 17, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/NrsZvFsdxug_hufcadf277c1a167c344a7ab858f41ed83_14113_460x200_fill_box_smart1_3.png","permalink":"/post/arm-hpc-cloud-hackathon.html","title":"Arm HPC Cloud Hackathon"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"Some workloads generate a LOT of output files and sometimes quite suddenly. For codes like OpenFOAM, this is data that you may not need until later when you run a post-processing job.\nGiven the amount of data isn’t always predictable, there are a few ways to prepare for this deluge, but most of them involve pre-provisioning too much storage in advance (and hoping you guessed correctly).\nWe’ve never been fans of guessing like that - we think infrastructure should just expand when you need it.\nStephen Sachs from our HPC Performance Engineering team came up with a great technique for solving this. He’s built some cloud automation with CloudWatch into AWS ParallelCluster so it triggers a “drain” process (a shell script) that pushes all the output files into Amazon S3 whenever the local filesystem on a compute instance reaches 80%. It’s surprisingly easy to do.\nEven if this isn’t the exact problem you’re trying to solve it’s a great thought process because it takes much of the automation outside the scheduler, outside the cluster even, and makes it really easy to set up, and really visible and auditable. You can even see plots of the activity.\nDuring the discussion, a GitHub repo with the sample shell scripts that you can steal and repurpose to your needs. They’re here: https://github.com/stephenmsachs/s3-openfoam\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"June 10, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/oFYzPXNoMYo_hu9c05a3356729acdf063ba350daf83b96_20530_460x200_fill_box_smart1_3.png","permalink":"/post/cloudwatch-automation-to-keep-your-scratch-disks-humming-and-your-clusters-running.html","title":"CloudWatch automation to keep your scratch disks humming, and your clusters running."},{"categories":["AWS ParallelCluster","Amazon Elastic Fabric Adapter","Amazon NICE DCV","Life Sciences"],"contents":"In the search for HPC application performance, there’s more than one way to build a network.\nHPC applications (or “codes” as we usually call them) are often at the mercy of the network underpinning an HPC cluster. If your CPUs aren’t busy, it’s usually because something that’s meant to be feeding them data isn’t doing so at a fast enough rate. And often the culprit is the network.\nPeter Mendygral has a lot of years of experience looking at networks and is one of the co-authors of the GPCnet benchmark, which aims to have a more nuanced look at how networks deliver for (or fail) the clusters built on them. In todays’s discussion, Pete speaks about the real world conditions found on many network fabrics, and shows us that when they depart from the idealized scenarios that common micro benchmarks measure, the results are anything but stellar.\nThis helps to understand some of the motivations behind AWS’s Elastic Fabric Adapter (EFA) design and in particular, how the Scalable Reliable Datagram (SRD) it’s built on, solves problems very differently.\nDuring the discussion, we reference a paper about GPCnet. You can find it here. https://escholarship.org/content/qt17m1r82n/qt17m1r82n.pdf?t=q15wzn\u0026amp;v=lg\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"June 3, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/gtQeLmZloJo_huaf63b64e4caabc9d05c8ef92fa1fb971_18681_460x200_fill_box_smart1_3.png","permalink":"/post/the-impact-of-network-conditions-on-application-performance-is-complicated.html","title":"The impact of network conditions on application performance is complicated."},{"categories":["AWS ParallelCluster"],"contents":"In this blog post, we will explain how to launch highly available HPC clusters across an AWS Region. The solution is deployed using the AWS Cloud Developer Kit (AWS CDK), a software development framework for defining cloud infrastructure in code and provisioning it through AWS CloudFormation, hiding the complexity of integration between the components.\nRead the full post at the AWS HPC Blog.\n","date":"June 3, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/ha-hpc-header-1260x630_hu22578a892c243f27e70b57977fbf0c1c_399315_460x200_fill_box_smart1_3.png","permalink":"/post/building-highly-available-hpc-infrastructure-on-aws.html","title":"Building highly-available HPC infrastructure on AWS"},{"categories":["AWS ParallelCluster","Amazon Elastic Fabric Adapter"],"contents":"By popular request, we’re looking today at the EFA software stack and environment: how to make sure it’s set up correctly (so you get great performance from your codes), how to tell if it’s not, and how to fix that.\nAustin Cherian, our performance junkie from the HPC Developer Relations team in AWS Engineering joins us to deep dive into the nitty gritty of the stack and some useful techniques for debugging. We cover both Open MPI and Intel MPI, as well as checking the libfabric providers and the hardware enablement underneath.\nIn the discussion, we reference a lot of helpful pages, including:\nthe EFA homepage: https://aws.amazon.com/hpc/efa/ supported instances: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html#efa-instance-types the user guide: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html troubleshooting: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/troubleshooting-ena.html#disable-enhanced-networking-ena-instance-store If you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"May 28, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/Wq8EMMXsvyo_huaba48e3d4fb0c9df02df5210a8a3e782_19447_460x200_fill_box_smart1_3.png","permalink":"/post/howto-make-sure-efa-is-setup-correctly-and-what-to-do-if-it-isnt.html","title":"HOWTO make sure EFA is setup correctly (and what to do if it isn't)."},{"categories":["Life Sciences","AI/ML"],"contents":"Today, more than 290,000 researchers in France are working to provide better support and care for patients through modern medical treatment. To fulfill their mission, these researchers must be equipped with powerful tools. At AWS, we believe that technology has a critical role to play in medical research. Why? Because technology can take advantage of the significant amount of data generated in the healthcare system and in the research community to enable opportunities for more accurate diagnoses, and better treatments for many existing and future diseases. To support elite research in France, we are proud to be a sponsor of two French organizations: Gustave Roussy and Sorbonne University. AWS is providing them with the computing power and machine learning technologies needed to accelerate cancer research and develop a treatment for COVID-19.\nRead the full post at the AWS HPC Blog.\n","date":"May 28, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/dalle-hpc-01_hub722fbb4e61ec1893436c76a61744c34_863441_460x200_fill_box_smart1_3.png","permalink":"/post/accelerating-research-and-development-of-new-medical-treatments-with-hpc-on-aws.html","title":"Accelerating research and development of new medical treatments with HPC on AWS"},{"categories":["AWS ParallelCluster","Climate/Environment/Weather"],"contents":"(A complete teardown of WRF performance on x86 and AWS Graviton, from memory subsystems, compilers and MPI stacks).\nWeather simulation is a reliably difficult workload for almost any HPC architecture and is often used as a litmus test by many customers before they look too hard at a novel or different systems. Customers have asked us frequently about our performance for codes like WRF, and that’s been even more the case since we launched our Arm-based processor, the AWS Graviton2, in a range of EC2 instances.\nSo it’s exciting that Karthik Raman and Matt Koop (two leading engineers from our global HPC solution architecture team) dived deep to look at WRF’s performance across a range of instance types (both Intel and Graviton), with EFA (our fast fabric, as you might remember from last week) as well as investigating the impact of different MPIs and compilers.\nThe results were startling: our Graviton instances performed and scaled pretty much exactly inline with our Intel ones - but at a much lower price, which should be pleasing to everyone.\nWe sat down at the virtual water cooler with Karthik and Matt to dig into this and understand what the moving parts are that brought this about. We get into memory subsystems, MPIs, collective operations and a lot more.\nIn the discussion, we reference a lot of material published last week in a blog post at the HPC blog, here: https://aws.amazon.com/blogs/hpc/numerical-weather-prediction-on-aws-graviton2/\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"May 20, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/D2ppdRKSz5I_hue129e7298476f9cd2c9c6a1dfe316fd5_14240_460x200_fill_box_smart1_3.png","permalink":"/post/wrf-performance-teardown-on-graviton-vs-x86.html","title":"WRF performance teardown on Graviton vs x86"},{"categories":["Climate/Environment/Weather"],"contents":"Training users on how to use high performance computing resources — and the data that comes out as a result of those analyses — is an essential function of most research organizations. Having a robust, scalable, and easy-to-use platform for on-site and remote training is becoming a requirement for creating a community around your research mission. A great example of this comes from the NOAA National Weather Service Warning Decision Training Division (WDTD), which develops and delivers training on the integrated elements of the hazardous weather warning process within a National Weather Service (NWS) forecast office. In collaboration with the University of Oklahoma’s Cooperative Institute for Mesoscale Meteorological Studies (OU/CIMMS), WDTD conducts its flagship course, the Radar and Applications Course (RAC), for forecasters issuing warnings for flash floods, severe thunderstorms, and tornadoes. Trainees learn the warning process, the science and application of conceptual models, and technical aspects of analyzing radar and other weather data in the Advanced Weather Interactive Processing System (AWIPS). Read the full post at the AWS HPC Blog.\n","date":"May 18, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/RITC-CollabMap-1101x630_hu0cbbd4093107edc99a698c698cdb818a_417106_460x200_fill_box_smart1_3.png","permalink":"/post/training-forecasters-to-warn-severe-hazardous-weather-on-aws.html","title":"Training forecasters to warn severe hazardous weather on AWS"},{"categories":["AI/ML","AWS ParallelCluster","Amazon Elastic Fabric Adapter"],"contents":"AWS’s compute infrastructure is very much not like a ‘normal’ supercomputer (whatever that is). We don’t start with a blank page every few years and design the next big system. It’s more like a city where we have to build on what’s there already, renovate occasionally, and push for bigger and better and faster whilst keeping the lights on at all times.\nThat leads to a bunch of design decisions that drive our engineers in a very different direction and our Elastic Fabric Adapter is an example of just that. Brian Barrett (one of our Principal Engineers in the HPC team) joins us this week to talk about the genesis of EFA, how it works, and why it convinced us that we could do without specialist fabrics like Infiniband and still deliver the same (or better) application scaling performance that our HPC customers were pushing us for.\nDuring the discussion, we mentioned an IEEE paper which is worth reading for more insights into SRD: “A Cloud-Optimized Transport Protocol for Elastic and Scalable HPC” : https://ieeexplore.ieee.org/document/9167399\nAnd we also shamelessly tried to hire all of you for our EFA engineering team to work on some more difficult problems. That link is here: http://boofla.io/jobs\nMore about EFA is here: https://aws.amazon.com/hpc/efa/\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"May 13, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/IgPWzhIHX68_hu2bc5a802f003a26ca0cc6c37f7b4a4b9_17889_460x200_fill_box_smart1_3.png","permalink":"/post/how-efa-works-and-why-we-dont-use-infiniband-in-the-cloud.html","title":"How EFA works and why we don't use infiniband in the cloud."},{"categories":[],"contents":"Arm and AWS are calling all grad students and post-docs who want to gain experience advancing the adoption of the Arm architecture in HPC to join a world-wide community effort lead by the Arm HPC User’s Group (A-HUG). The event will take the form of a hackathon this summer and is aimed at getting open-source HPC codes to build and run fast on Arm-based processors, specifically AWS Graviton2. To make it a bit more exciting, A-HUG will be awarding an Apple M1 MacBook to each member of the team (max. 4 people) that contributes the most back to the Arm HPC community.\nRead the full post at the AWS HPC Blog.\n","date":"May 12, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/arm-aws-hackathon-1260x630_hu6d44aabb85e747b20df152cdde661f68_473920_460x200_fill_box_smart1_3.png","permalink":"/post/aws-joins-arm-to-support-arm-hpc-hackathon-this-summer.html","title":"AWS joins Arm to support Arm-HPC hackathon this summer"},{"categories":["AWS ParallelCluster","Climate/Environment/Weather"],"contents":"The Weather Research and Forecasting (WRF) model is a numerical weather prediction (NWP) system designed to serve both atmospheric research and operational forecasting needs. With the release of Arm-based AWS Graviton2 Amazon Elastic Compute Cloud (EC2) instances, a common question has been how these instances perform on large-scale NWP workloads. In this blog, we will present results from a standard WRF benchmark simulation and compare across three different instance types.\nRead the full post at the AWS HPC Blog.\n","date":"May 10, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/wfr-g2-header-1260x630_hu57fdfacf99d7854010ea855c79f296b6_376565_460x200_fill_box_smart1_3.png","permalink":"/post/numerical-weather-prediction-on-aws-graviton2.html","title":"Numerical weather prediction on AWS Graviton2"},{"categories":["AWS ParallelCluster","Life Sciences"],"contents":"Carsten Kutzner is a researcher and scientific software developer at the Max Planck Institute for Biophysical Chemistry in Göttingen in Germany. He\u0026rsquo;s been doing some really diverse benchmarking studies in collaboration with our HPC engineering team, and he joins us today to talk about some of the results of that investigation.\nWhat\u0026rsquo;s most interesting (and soft of unexpected) is the instances he concludes are the best for the job - whether you\u0026rsquo;re optimizing for sheer performance against the clock, or price performance because you need to maximize your budget within a more generous time window. No spoilers here: you need to hear him talk.\nMolecular Dynamicists are used to doing these calculations because they measure their progress in nanoseconds of simulation time per DAY of wall clock time. As Carsten points out: if your job runs for days or weeks, it matters whether you can squeeze 10% more from the compute units it\u0026rsquo;s running on.\nIn the talk we mention his a few resources, and here\u0026rsquo;s the links:\nKarsten\u0026rsquo;s home page with a trove of great data: https://www.mpibpc.mpg.de/grubmueller/bench Christian\u0026rsquo;s workshop to guide you setting up the exact same infrastructure that Carsten uses: https://gromacs-on-pcluster.workshop.aws/ Austin\u0026rsquo;s blog post from a few weeks ago about GROMACS performance comparisons on CPUs. https://aws.amazon.com/blogs/hpc/gromacs-price-performance-optimizations-on-aws/ Austin\u0026rsquo;s talk a couple weeks ago about how to build these clusters to run this kind of benchmark operation: https://www.youtube.com/watch?v=ubAvlgNN9PQ As usual, if you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"May 6, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/Zz91uPbk12Y_hu07319c82781ccdd6c1cb50c2078af95d_14455_460x200_fill_box_smart1_3.png","permalink":"/post/interesting-gpu-cpu-performance-results-from-gromacs.html","title":"Interesting GPU \u0026 CPU performance results from GROMACS"},{"categories":["AWS Batch"],"contents":"A customer asked us what is the difference between the CancelJob and TerminateJob API calls in AWS Batch. This post provides an overview of AWS Batch job states, and how these two API calls effect the job requests that you have submitted.\nRead the full post at the AWS HPC Blog.\n","date":"May 3, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/batch-cancel_term-header-1260x630_hu7ca51a7f9276fdad97b5b9ea885b2ba9_388132_460x200_fill_box_smart1_3.png","permalink":"/post/reader-question-what-is-the-difference-between-canceling-and-terminating-a-job-in-aws-batch.html","title":"Reader Question: What is the difference between canceling and terminating a job in AWS Batch?"},{"categories":["AWS ParallelCluster","Amazon NICE DCV"],"contents":"DCV was originally built for supercomputing centers to push pixels over the internet and enable a scientist or aerospace engineer to feel like they had an HPC cluster under their desk when inspecting detailed imagery or manipulating designs.\nNow it\u0026rsquo;s getting used way beyond HPC in gaming, software development and \u0026hellip; enabling lots of us to be productive working from home.\nAll these little things make the desktop virtualization experience more real, so we decided to catch up with Rey Wang (Sr Product Manager for DCV) and Paolo Maggi (the GM for our DCV organization) to find out what\u0026rsquo;s new.\nYou can find out more about DCV here: aws.amazon.com/hpc/dcv and you can also watch the Tech Short we did last week on DCV\u0026rsquo;s better faster and more amazing streaming capabilities which are driving gaming engines to deliver rich experience in 4K @ 60 FPS (which we all know is essential \u0026hellip; right?).\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"April 29, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/h4Dm5ItY96o_hu56c9c97f2dc5a797b1d6c95cbb7bff4c_20161_460x200_fill_box_smart1_3.png","permalink":"/post/whats-new-in-dcv-20210.html","title":"What's New in DCV 2021.0"},{"categories":["CAE/CFD","NICE DCV"],"contents":"This post was written by Dario La Porta, AWS Professional Services Senior Consultant for HPC. Customers across a wide range of industries such as energy, life sciences, and design and engineering are facing challenges in managing and analyzing their data. Not only is the amount and velocity of data increasing, but so is the complexity and […]\nRead the full post at the AWS HPC Blog.\n","date":"April 23, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/vdi-header-1-1260x630_hu588408fcc948321fff88ba189704fcff_329155_460x200_fill_box_smart1_3.png","permalink":"/post/a-vdi-solution-with-enginframe-and-nice-dcv-session-manager-built-with-aws-cdk.html","title":"A VDI solution with EnginFrame and NICE DCV Session Manager built with AWS CDK"},{"categories":["AWS ParallelCluster","Amazon NICE DCV"],"contents":"DCV was originally created for scientists and engineers who run thing on supercomputers. They needed to visualize massive datasets produced by really large compute workloads crunching petabytes of data. That takes some fast machines and some very fancy GPUs. Since we don’t like putting scientist into freezing data centers our DCV engineers worked out how to push pixels over the internet quickly, without any loss of fidelity.\nFast forward 20 years and a generation of gamers want to do the same thing, only this time there are way more pixels (blame HD and 4k) and it’s 60 frames per second, or bust.\nThat meant we had to so some extra tricks to make video streams not miss a beat when very normal things happen, like the internet drops a packet or there’s some congestion because your mum is on a zoom call with her team at the office.\nOur senior software engineer, Marco Mastropaolo leads us through the things we’ve done with DCV to make it work for both HPC, and the gamers.\nDuring the discussion, we mentioned Marco’s detailed talk at NVIDIA’s GTC 21. That’s here: https://tinyurl.com/dcvgtc21\nMore about DCV is here: https://aws.amazon.com/hpc/dcv/\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"April 22, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/_L-U_ET4qrw_hu62c869ce20fe34333fb1a81fd2cc37f2_18149_460x200_fill_box_smart1_3.png","permalink":"/post/supercomputing-visualization-good-enough-for-the-most-demanding-gamers.html","title":"Supercomputing Visualization good enough for the most demanding gamers."},{"categories":[],"contents":"Our AWS Batch development team have been working on some major improvements to the way Batch assesses the need to scale up or down. We’re doing 5x as many scaling evaluations per hour now which should pick up the pace significantly. With some other improvements which we talk about in the video, we’re hoping customers will see less throttling on the submitjob API call and just see an overall faster submission rate, and faster time to results (since that’s what really matters).\nJo Adegbola is the Sr Manager of all of our AWS Batch software engineering teams and works closely with Steve Kendrex the Principal Product Manager for Batch obsessing on how to “make stuff go fast” for Batch users everywhere.\nDuring the show we referred to a couple of paths to get started on Batch.\nA tutorial to get started with simple jobs on Batch running in EC2 Spot: https://aws.amazon.com/getting-started/hands-on/run-batch-jobs-at-scale-with-ec2-spot/ A workshop for scientists who use NextFlow to run genomics pipelines on Batch: https://genomics-nf.workshop.aws (https://genomics-nf.workshop.aws/) If you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"April 15, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/uQCUpw7uSjY_hu43da614da357a8a7e41945129be1b521_18739_460x200_fill_box_smart1_3.png","permalink":"/post/aws-batchs-new-faster-scaling-features.html","title":"AWS Batch's new Faster Scaling features"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"We just published a blog post last week with a deep dive into what makes GROMACS tick. The blog post talked about software stacks and EC2 instances that will deliver the best possible performance for people trying to do some molecular dynamics quickly. This turns out to be pretty much EVERYONE in the MD community, because these are people who measure their progress in nanoseconds of simulation PER DAY of wall clock time.\nAustin Cherian is one of our senior engineers in the Developer Relations team, and the one who is the most obsessed with performance and getting the performance testing methodology right. We asked him to join tech shorts this week to show us how he uses and abuses AWS ParallelCluster in his quest of “making stuff go fast”, because his use cases are pretty common, and might help you imagine some better ways of working.\nDuring the discussion, he talks about his blog post which you can find here: https://aws.amazon.com/blogs/hpc/gromacs-price-performance-optimizations-on-aws/\nWe also mention the step-by-step workshop for running this environment on AWS ParallelCluster, and that’s here: https://gromacs-on-pcluster.workshop.aws/setup.html\nLast but not least, check out the new HPC blog channel because you’ll find loads of articles from HOWTOs through to useful stories from customers and other builders who’ve helped make the cloud a more awesome experience for HPC. https://aws.amazon.com/blogs/hpc\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"April 8, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/ubAvlgNN9PQ_huc2a93721da6647657ef12feae953459c_12604_460x200_fill_box_smart1_3.png","permalink":"/post/making-stuff-run-fast-starting-with-gromacs.html","title":"'Making stuff run fast', starting with GROMACS."},{"categories":["AWS Batch"],"contents":"Large-scale data analysis usually involves some multi-step process where the output of one job acts as the input of subsequent jobs. Customers using AWS Batch for data analysis want a simple and performant storage solution to share with and between jobs. We are excited to announce that customers can now use Amazon Elastic File System (Amazon […]\nRead the full post at the AWS HPC Blog.\n","date":"April 5, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/batch-efs-header-1260x630_hu7ca51a7f9276fdad97b5b9ea885b2ba9_360115_460x200_fill_box_smart1_3.png","permalink":"/post/introducing-support-for-per-job-amazon-efs-volumes-in-aws-batch.html","title":"Introducing support for per-job Amazon EFS volumes in AWS Batch"},{"categories":["AWS ParallelCluster"],"contents":"Christian Kniep (our senior developer relations engineer) from HPC Engineering is back to finish the conversation we started about containers in HPC. Christian is leading the cause for containerization in HPC, and helping our engineering and product teams focus on enabling that path on AWS.\nToday we\u0026rsquo;re talking about how containers work in a shared infrastructure environment, with shared filesystems (like Lustre) and we\u0026rsquo;ll also cover multi-node parallelism, too, since MPI is where it\u0026rsquo;s at for many (most?) people in the HPC community.\n(And we completely missed thew opportunity to make any April fools jokes, so everything we say is totally believable) :-)\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"April 1, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/sN-SZmCza3k_hua0c6b1390056d384371bcfb0915d6c92_11889_460x200_fill_box_smart1_3.png","permalink":"/post/containers-episode-ii-the-runtimes-strike-back.html","title":"Containers, Episode II - the Runtimes Strike Back"},{"categories":["Life Sciences","AWS ParallelCluster"],"contents":"Molecular dynamics (MD) is a simulation method for analyzing the movement and tracing trajectories of atoms and molecules where the dynamics of a system evolve over time. MD simulations are used across various domains such as material sciences, biochemistry, biophysics and are typically used in two broad ways to study a system. The importance of […]\nRead the full post at the AWS HPC Blog.\n","date":"March 31, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/gromacs-header-image-1-1260x630_hu13f2d7acd3e9290b863e31c8cdddaff1_417625_460x200_fill_box_smart1_3.png","permalink":"/post/gromacs-price-performance-optimizations-on-aws.html","title":"GROMACS price-performance optimizations on AWS"},{"categories":["AWS ParallelCluster"],"contents":"This post was written by Dnyanesh Digraskar, Sr. Partner Solutions Architect for HPC at AWS and co-authored by Wei Zhang and Ravi Gupta, Sr Software Engineers for Simcenter Nastran at Siemens. Introduction In this blog, we demonstrate the deployment, performance, and price comparisons of Simcenter Nastran for three finite element analysis (FEA) based use cases […]\nRead the full post at the AWS HPC Blog.\n","date":"March 31, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/nastran-header-1260x630_hu13f2d7acd3e9290b863e31c8cdddaff1_411708_460x200_fill_box_smart1_3.png","permalink":"/post/running-finite-element-analysis-using-simcenter-nastran-on-aws.html","title":"Running finite element analysis using Simcenter Nastran on AWS"},{"categories":["AWS ParallelCluster"],"contents":"This post was written by Benjamin Meyer, AWS Solutions Architect When companies and labs start their high performance computing (HPC) journey in the AWS Cloud, it’s not only because they’re in search of elasticity and scale – they’re also in search of new tools and environments. Initially this can appear challenging as there are many […]\nRead the full post at the AWS HPC Blog.\n","date":"March 31, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/cloud9-pcl-header-1260x630_hu7ca51a7f9276fdad97b5b9ea885b2ba9_367919_460x200_fill_box_smart1_3.png","permalink":"/post/simplify-hpc-cluster-usage-with-aws-cloud9-and-aws-parallelcluster.html","title":"Simplify HPC cluster usage with AWS Cloud9 and AWS ParallelCluster"},{"categories":["AWS ParallelCluster","AWS Batch"],"contents":"This post is written by Deepak Singh, Vice President of Compute Services. At AWS, we love working with customers to solve their toughest challenges. High performance computing (HPC) is one of those challenges that pushes against the boundaries of AWS performance at scale. HPC is also a personal interest of mine, as I came to […]\nRead the full post at the AWS HPC Blog.\n","date":"March 31, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/intro-header-1-1260x630_hu13f2d7acd3e9290b863e31c8cdddaff1_394394_460x200_fill_box_smart1_3.png","permalink":"/post/welcome-to-the-aws-hpc-blog.html","title":"Welcome to the AWS HPC Blog"},{"categories":["AWS ParallelCluster","Amazon NICE DCV","Life Sciences"],"contents":"When COVID-19 struck in 2020, lots of scientists all over the world had to drop EVERYTHING and get busy solving some very big problems, very quickly. AWS became a founding member of the COVID-19 HPC Consortium, which is a White House-led initiative to provide COVID-19 researchers worldwide with free compute time and huge resources on leading industry, government, and academic HPC systems. In short, we all threw the kitchen sink at the problem of speeding up the pace of scientific discovery in the fight to stop the virus.\nFrom AWS’s point of view, global reach and capacity are solved problems. Our challenge was to onboard researchers quickly, with no specialized knowledge necessary \u0026hellip; so they could get computing on AWS as fast as humanly possible. To that end we developed the NoTearsHPC cluster solution for 1-click launches.\nEvan Bollig and Sean Smith from the team who built No Tears join us to talk about how it work, what it provides and show us how easy it is to do really complicated things really fast. They used AWSome things like AWS ParallelCluster and awesome things like Spack to make all that possible. And with the help of our friends at Intel, we all helped the Spack team make package installation even faster still.\nif you have an AWS account, you can launch a No Tears cluster, as per the talk by going to it’s GitHub page here:\nhttps://github.com/aws-samples/no-tears-cluster\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"March 25, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/vmALEY6U33w_hud1570b290efaefb1f2172426b53f07eb_10116_460x200_fill_box_smart1_3.png","permalink":"/post/no-tears-hpc-honest-to-goodness-research-ready-hpc-clusters-in-under-10-minutes.html","title":"'NO TEARS HPC - honest-to-goodness research-ready HPC clusters in under 10 minutes."},{"categories":["AWS ParallelCluster"],"contents":"Today we\u0026rsquo;re joined by Christian Kniep who is one of our senior developer advocates in HPC Engineering. Christian is leading the cause for containerization in HPC, and helping our engineering and product teams focus on enabling that path on AWS.\nWe know containers are a \u0026lsquo;mixed\u0026rsquo; topic for HPC folks, and so we\u0026rsquo;re starting a series of Tech Shorts to talk about what they are, what problems they solve, what new problems they create, and what solutions exist to address those.\nToday is part 1, where we talk about how containers work generally, how they work on AWS, with things like AWS Nitro and GPUs in play.\nNext week, we\u0026rsquo;ll talk about containers in shared infrastructure and multi-node parallelism, too.\nThe FOSDEM devroom we spoke about is here: https://tinyurl.com/fosdemhpc21 and you can find Christian on twitter here https://twitter.com/CQnib - pick up the conversation with him if you have some thoughts about containers and how they work for you (or not).\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"March 18, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/WQTrA4-9ZXk_hu19a37b85e563d7e62e269681c0043a0e_13155_460x200_fill_box_smart1_3.png","permalink":"/post/containers-in-hpc-what-they-fix-and-what-they-break-and-how-to-fix-that-too.html","title":"Containers in HPC - what they fix and what they break (and how to fix that, too)"},{"categories":["AWS ParallelCluster","Life Sciences"],"contents":"In today’s show, we’re talking about how to choose the right compute and storage elements to get great performance for CryoSPARC, which is one of the popular codes used in cryogenic electron microscopy (CryoEM) analysis.\nThis is important because, as you’ll hear in the discussion, breaking the pipeline down into different stages and optimizing the infrastructure for each one can speed up the entire workflow, and save a whole lot of money on that compute, too. All these workloads ran on AWS ParallelCluster, which lets you have multiple queues with different instance types and orchestration options (check out last week’s show: https://www.youtube.com/watch?v=C4iSNjcW5O4). ParallelCluster also makes it easy to construct an FSx for Lustre filesystem on the fly using data from an Amazon S3 bucket.\nSteve Litster is our Principal HPC business leader for Healthcare and Lifesciences, based out of Boston, and is a recovering x-ray crystallographer (and still has occasional flashbacks of being in a lab). Brian Skjerven is a Sr. HPC Specialist Solutions Architect based in London.\nThe folks at Structura BioTechnology (https://structura.bio/) who make CryoSPARC are going to publish a deployment guide in the next couple weeks that Brian has helped them create using the performance data he describes in the show.\nIt's here: https://guide.cryosparc.com/deploy/cryosparc-on-aws If you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"March 11, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/24QVburwONo_hu2b883333f2640d72be5a3bf1aa09c2a0_9624_460x200_fill_box_smart1_3.png","permalink":"/post/how-to-accelerate-cryoem-analysis-with-aws-parallelcluster-and-fsx-for-lustre.html","title":"How to accelerate CryoEM Analysis with AWS ParallelCluster and FSx for Lustre"},{"categories":[],"contents":"Each week we’re going to take you to the water cooler here in AWS to talk to engineers and architects about the technology, tools and techniques that make AWS the best place to run your HPC workloads in the cloud.\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"March 4, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/xCFgdLB07PE_hue13b37a4e5ebc064989ba3efa7ddb15c_12913_460x200_fill_box_smart1_3.png","permalink":"/post/welcome-to-hpc-tech-shorts.html","title":"Welcome to HPC Tech Shorts!"},{"categories":["AWS ParallelCluster"],"contents":"In today’s show, we talk about AWS ParallelCluster 2.9 and its new features built on Slurm’s power management module. This lets you build clusters with multiple queues and instances types within those queues. This allows the cluster to scale nodes that fit a given workload, making scaling decisions much more job-driven, and means each queue can be quite specifically optimized for a code .\nWe’ll show a real live cluster upgrade happening, doing in 10 minutes what it generally takes 18 months to pull off if you still live with on-premises facilities.\nRex Chen is one of our amazing Software Engineers who works every day on AWS ParallelCluster and he’s joined by Angel Pizarro, our Principal Developer Advocate for HPC \u0026amp; Batch who has personal experience building those clusters and upgrading them, and knows better now.\nHere’s the blog we mentioned in the show:\nhttps://aws.amazon.com/blogs/opensource/using-multiple-queues-and-instance-types-in-aws-parallelcluster-2-9/\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"March 4, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/C4iSNjcW5O4_hu1cf5db3bfb13fb382884d737a3ce2b24_11858_460x200_fill_box_smart1_3.png","permalink":"/post/howto-configure-multiple-queues-and-instance-types-in-aws-parallelcluster.html","title":"HOWTO configure multiple queues and instance types in AWS ParallelCluster"},{"categories":[],"contents":"Our CFD expert, Dr Neil Ashton put AWS\u0026rsquo;s Arm-based CPU, the AWS Graviton, to the test with some CFD workloads and shares the results with us. If you\u0026rsquo;re interested in CFD, or just wanting to find out if newer CPU architectures like the Graviton can deliver the performance you expect for HPC codes, you\u0026rsquo;ll get some data to start you off.\nMentioned in the show: Neil\u0026rsquo;s CFD workshops https://cfd-on-pcluster.workshop.aws\nNot mentioned but also important: CFD Direct (two of the original authors of OpenFOAM) also have an offering in AWS Marketplace that runs on AWS Graviton and is really easy to launch. You can check it out here: https://aws.amazon.com/marketplace/pp/B08CHKT98H\nIf you have ideas for technical topics you\u0026rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM\u0026rsquo;ing us with your idea.\n","date":"February 25, 2021","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/4WgE472wIqU_hu277681c2f062a425d45acfd4c059738a_14448_460x200_fill_box_smart1_3.png","permalink":"/post/cfd-on-awss-graviton-arm-based-cpu-early-results.html","title":"CFD on AWS's Graviton (Arm-based) CPU - early results."},{"categories":null,"contents":"Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren.\n","date":"April 14, 2020","image":"https://d175uvn6dnkepf.cloudfront.net/images/shop/product-2.jpg","permalink":"/shop/product-3.html","title":"School Bag"},{"categories":[],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026amp;rsquo;m an inline-style link\nI\u0026amp;rsquo;m an inline-style link with title\nI\u0026amp;rsquo;m a reference-style link\nI\u0026amp;rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab this is second tab this is third tab Collapse collapse 1 This is a simple collapse collapse 2 This is a simple collapse collapse 3 This is a simple collapse Code and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list Is something people use sometimes. Markdown in HTML Does *not* work **very** well. Use HTML tags. Tables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Gallery Youtube video ","date":"March 15, 2020","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/post-6_hue652d174a668504e08bd47e88a7e152b_295919_460x200_fill_q90_box_smart1.jpg","permalink":"/post/elements.html","title":"Elements That You Can Use To Create A New Post On This Template."},{"categories":null,"contents":"Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren.\n","date":"March 14, 2020","image":"https://d175uvn6dnkepf.cloudfront.net/images/shop/product-3.jpg","permalink":"/shop/product-7.html","title":"Blue Jacket"},{"categories":null,"contents":"Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren.\n","date":"March 14, 2020","image":"https://d175uvn6dnkepf.cloudfront.net/images/shop/product-4.jpg","permalink":"/shop/product-6.html","title":"Cotton T-Shirt"},{"categories":null,"contents":"Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren.\n","date":"March 14, 2020","image":"https://d175uvn6dnkepf.cloudfront.net/images/shop/product-1.jpg","permalink":"/shop/product-1.html","title":"Linen Bag"},{"categories":null,"contents":"Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren.\n","date":"March 14, 2020","image":"https://d175uvn6dnkepf.cloudfront.net/images/shop/product-4.jpg","permalink":"/shop/product-4.html","title":"Pink T-Shirt"},{"categories":null,"contents":"Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren.\n","date":"March 14, 2020","image":"https://d175uvn6dnkepf.cloudfront.net/images/shop/product-2.jpg","permalink":"/shop/product-2.html","title":"Side Bag"},{"categories":null,"contents":"Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren.\n","date":"March 14, 2020","image":"https://d175uvn6dnkepf.cloudfront.net/images/shop/product-5.jpg","permalink":"/shop/product-8.html","title":"Travel Carrier"},{"categories":null,"contents":"Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren.\n","date":"February 14, 2020","image":"https://d175uvn6dnkepf.cloudfront.net/images/shop/product-5.jpg","permalink":"/shop/product-5.html","title":"Travel Bag"},{"categories":["HPC Tech Shorts"],"contents":"HPC Tech Shorts HPC Tech Shorts is the HPC engineering water-cooler at AWS. New shows on YouTube weekly - sometimes more often.\nIf the videos on HPC Tech Shorts help you understand AWS a little better and get you closer to your goal, consider subscribing to the channel, so you can get new videos as they\u0026rsquo;re released.\n","date":"October 17, 2019","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/ts-ident-large_hu71761b5706cb4e145b71c3723898e1f7_338810_460x200_fill_box_smart1_3.png","permalink":"/post/techshorts.html","title":"HPC Tech Shorts"},{"categories":["HPC Workshops"],"contents":"HPC Workshops We build workshops that can help you, step-by-step expand your skills and solve problems more efficiently.\n","date":"October 17, 2019","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/workshops-ident_hu8470dfd933aa5f2b2821f10560f59f4b_493363_460x200_fill_box_smart1_3.png","permalink":"/post/workshops.html","title":"HPC Workshops"},{"categories":["HPC Blog Channel"],"contents":" .right { float: right; padding: 0 0 20px 20px; }\nAWS ParallelCluster is an open source tool that makes it easy to deploy and manage HPC clusters on AWS. It is available at no additional charge, and you pay only for the AWS resources needed to run your applications.\n","date":"September 26, 2019","image":"https://d175uvn6dnkepf.cloudfront.net/images/post/hpcblog-ident_hu6676d3abf49cfd58ca77c8fdb9b00c3d_499498_460x200_fill_box_smart1_3.png","permalink":"/post/hpcblog.html","title":"HPC Blog Channel"},{"categories":null,"contents":"Welcome to Day 1 Welcome to day1hpc.com - a community site built and curated by the Developer Relations team in HPC Engineering at AWS. Our job is to be the interface between our HPC engineering teams and the people in the HPC community who want to use AWS to create powerful tools for solving hard problems.\nTo do that, we\u0026rsquo;ll spend a lot of our time explaining how these services work, and getting your feedback so we can improve them, continuously. You’ll find a lot of things on this site to help you understand AWS a little better, all with the aim of getting you where to want to go, faster.\nIf you see us at a conference, please say ‘hi’ and let us know how we can help.\nYou can find us on Twitter at @TechHpc, on YouTube in the HPC Tech Shorts Channel, or blogging to the world from the AWS HPC Blog Channel. Please reach out and let us know what you’re finding hard, or easy.\n","date":"January 1, 1","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/welcomeday1-ident-large_hu55207aec9ed68bb5ddf84e22b0bccde4_538643_460x200_fill_box_smart1_3.png","permalink":"/about.html","title":"About Us"},{"categories":null,"contents":"Angel is a Principal Developer Advocate for HPC and scientific computing. His background is in bioinformatics application development and building system architectures for scalable computing in genomics and other high throughput life science domains.\n","date":"January 1, 1","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/welcomeday1-ident-large_hu55207aec9ed68bb5ddf84e22b0bccde4_538643_460x200_fill_box_smart1_3.png","permalink":"/author/angel-pizarro.html","title":"Angel Pizarro"},{"categories":null,"contents":" Boof (Brendan Bouffler) is the head of our Developer Relations group in HPC Engineering at AWS.\nHe’s been guilty of responsible for designing and building hundreds of HPC systems in all kind of environments, all over the world. However, he joined AWS when it became clear to him that cloud would become the exceptional tool the global research \u0026amp; engineering community needed to bring on the discoveries that would change the world for us all.\nHe holds a degree in Physics and an interest in testing several of its laws as they apply to bicycles.\nThis has, unfortunately, frequently resulted in hospitalization.\nYou can find him on twitter at @boofla, and on linkedIn, or hanging out somewhere with Elmo, who often appears in HPC Tech Shorts\n","date":"January 1, 1","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/welcomeday1-ident-large_hu55207aec9ed68bb5ddf84e22b0bccde4_538643_460x200_fill_box_smart1_3.png","permalink":"/author/brendan-bouffler.html","title":"Brendan Bouffler"},{"categories":null,"contents":"H1 Heading H2 Heading H3 Heading H4 Heading H5 Heading H6 Heading Paragraph Did you come here for something in particular or just general Riker-bashing? And blowing into maximum warp speed, you appeared for an instant to be in two places at once. We have a saboteur aboard. We know you’re dealing in stolen ore. But I wanna talk about the assassination attempt on Lieutenant Worf. Could someone survive inside a transporter buffer for 75 years? Fate. It protects fools, little children, and ships.\nEmphasis : Did you come here for something in particular or just general Did you come here for something in particular Did you come here Did you come here for something in particular Did you come here for something in particular Did you come here for something in particular URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example). Ordered list you appeared for an instant to be in two places at once. We have a saboteur aboard. you appeared for an instant to be in two places at once. Unordered list Quisque sem ipsum, placerat nec tortor vel, blandit vestibulum libero. Morbi sollicitudin viverra justo Blandit vestibulum libero. Morbi sollicitudin viverra justo Placerat nec tortor vel, blandit vestibulum libero. Morbi sollicitudin viverra justo Code and Syntax Highlighting : var s = \u0026#34;JavaScript syntax highlighting\u0026#34;; const plukDeop = key =\u0026gt; obj =\u0026gt; key.split const compose = key =\u0026gt; obj =\u0026gt; key.split alert(s); var s = \u0026#34;JavaScript syntax highlighting\u0026#34;; const plukDeop = key =\u0026gt; obj =\u0026gt; key.split const compose = key =\u0026gt; obj =\u0026gt; key.split alert(s); Buttons Button Quote “Did you come here for something in particular or just general Riker-bashing? And blowing into maximum warp speed, you appeared for an instant to be in two places at once.”\nNotice : This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nThis is a simple warning.\nTab : Title goes here Did you come here for something in particular or just general Riker-bashing? And blowing into maximum warp speed, you appeared for an instant to be in two places at once. We have a saboteur aboard. We know you’re dealing in stolen ore. But I wanna talk about the assassination attempt on Lieutenant Worf. Title goes here Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Title goes here Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.\nLorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo Table : # First Last Handle 1 Row:1 Cell:1 Row:1 Cell:2 Row:1 Cell:3 2 Row:2 Cell:1 Row:2 Cell:2 Row:2 Cell:3 3 Row:3 Cell:1 Row:3 Cell:2 Row:3 Cell:3 Collapse : collapse 1 Lorem ipsum dolor sit amet consectetur adipisicing elit. Lorem ipsum dolor sit amet consectetur adipisicing elit. Lorem ipsum dolor sit amet consectetur collapse 2 Lorem ipsum dolor sit amet consectetur adipisicing elit. Lorem ipsum dolor sit amet consectetur adipisicing elit. Lorem ipsum dolor sit amet consectetur collapse 3 Lorem ipsum dolor sit amet consectetur adipisicing elit. Lorem ipsum dolor sit amet consectetur adipisicing elit. Lorem ipsum dolor sit amet consectetur Image Gallery Youtube : ","date":"January 1, 1","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/welcomeday1-ident-large_hu55207aec9ed68bb5ddf84e22b0bccde4_538643_460x200_fill_box_smart1_3.png","permalink":"/elements.html","title":"Elements"},{"categories":null,"contents":"","date":"January 1, 1","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/welcomeday1-ident-large_hu55207aec9ed68bb5ddf84e22b0bccde4_538643_460x200_fill_box_smart1_3.png","permalink":"/homepage/full.html","title":"Homepage Full"},{"categories":null,"contents":"","date":"January 1, 1","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/welcomeday1-ident-large_hu55207aec9ed68bb5ddf84e22b0bccde4_538643_460x200_fill_box_smart1_3.png","permalink":"/homepage/full-left.html","title":"Homepage Full Left"},{"categories":null,"contents":"","date":"January 1, 1","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/welcomeday1-ident-large_hu55207aec9ed68bb5ddf84e22b0bccde4_538643_460x200_fill_box_smart1_3.png","permalink":"/homepage/full-right.html","title":"Homepage Full Right"},{"categories":null,"contents":"","date":"January 1, 1","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/welcomeday1-ident-large_hu55207aec9ed68bb5ddf84e22b0bccde4_538643_460x200_fill_box_smart1_3.png","permalink":"/homepage/grid.html","title":"Homepage Grid"},{"categories":null,"contents":"","date":"January 1, 1","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/welcomeday1-ident-large_hu55207aec9ed68bb5ddf84e22b0bccde4_538643_460x200_fill_box_smart1_3.png","permalink":"/homepage/grid-left.html","title":"Homepage Grid Left"},{"categories":null,"contents":"","date":"January 1, 1","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/welcomeday1-ident-large_hu55207aec9ed68bb5ddf84e22b0bccde4_538643_460x200_fill_box_smart1_3.png","permalink":"/homepage/grid-right.html","title":"Homepage Grid Right"},{"categories":null,"contents":"","date":"January 1, 1","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/welcomeday1-ident-large_hu55207aec9ed68bb5ddf84e22b0bccde4_538643_460x200_fill_box_smart1_3.png","permalink":"/homepage/list.html","title":"Homepage List"},{"categories":null,"contents":"","date":"January 1, 1","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/welcomeday1-ident-large_hu55207aec9ed68bb5ddf84e22b0bccde4_538643_460x200_fill_box_smart1_3.png","permalink":"/homepage/list-left.html","title":"Homepage List Left"},{"categories":null,"contents":"","date":"January 1, 1","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/welcomeday1-ident-large_hu55207aec9ed68bb5ddf84e22b0bccde4_538643_460x200_fill_box_smart1_3.png","permalink":"/homepage/list-right.html","title":"Homepage List Right"},{"categories":null,"contents":"Matt is a Principal Developer Advocate for HPC and scientific computing. He has a background in life sciences and building user-friendly HPC and cloud systems for long-tail users. When not in front of his laptop, he’s drawing, reading, travelling the world, or playing with the nearest dog.\n","date":"January 1, 1","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/welcomeday1-ident-large_hu55207aec9ed68bb5ddf84e22b0bccde4_538643_460x200_fill_box_smart1_3.png","permalink":"/author/matt-vaughn.html","title":"Matt Vaughn"},{"categories":null,"contents":"","date":"January 1, 1","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/welcomeday1-ident-large_hu55207aec9ed68bb5ddf84e22b0bccde4_538643_460x200_fill_box_smart1_3.png","permalink":"/404.html","title":"No Search Found"},{"categories":null,"contents":"Responsibility of Contributors Lorem ipsum dolor sit amet, consectetur adipiscing elit. Purus, donec nunc eros, ullamcorper id feugiat quisque aliquam sagittis. Sem turpis sed viverra massa gravida pharetra. Non dui dolor potenti eu dignissim fusce. Ultrices amet, in curabitur a arcu a lectus morbi id. Iaculis erat sagittis in tortor cursus. Molestie urna eu tortor, erat scelerisque eget. Nunc hendrerit sed interdum lacus. Lorem quis viverra sed\npretium, aliquam sit. Praesent elementum magna amet, tincidunt eros, nibh in leo. Malesuada purus, lacus, at aliquam suspendisse tempus. Quis tempus amet, velit nascetur sollicitudin. At sollicitudin eget amet in. Eu velit nascetur sollicitudin erhdfvssfvrgss eget viverra nec elementum. Lacus, facilisis tristique lectus in.\nGathering of Personal Information Lorem ipsum dolor sit amet, consectetur adipiscing elit. Purus, donec nunc eros, ullamcorper id feugiat quisque aliquam sagittis. Sem turpis sed viverra massa gravida pharetra. Non dui dolor potenti eu dignissim fusce. Ultrices amet, in curabitur a arcu a lectus morbi id. Iaculis erat sagittis in tortor cursus. Molestie urna eu tortor, erat scelerisque eget. Nunc hendrerit sed interdum lacus. Lorem quis viverra sed\nProtection of Personal- Information Lorem ipsum dolor sit amet, consectetur adipiscing elit. Purus, donec nunc eros, ullamcorper id feugiat quisque aliquam sagittis. Sem turpis sed viverra massa gravida pharetra. Non dui dolor potenti eu dignissim fusce. Ultrices amet, in curabitur a arcu a lectus morbi id. Iaculis erat sagittis in tortor cursus.\nMolestie urna eu tortor, erat scelerisque eget. Nunc hendrerit sed interdum lacus. Lorem quis viverra sed Lorem ipsum dolor sit amet, consectetur adipiscing elit. Purus, donec nunc eros, ullamcorper id feugiat\nPrivacy Policy Changes Sll the Themefisher items are designed to be with the latest , We check all comments that threaten or harm the reputation of any person or organization personal information including, but limited to, email addresses, telephone numbers Any Update come in The technology Customer will get automatic Notification. ","date":"January 1, 1","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/welcomeday1-ident-large_hu55207aec9ed68bb5ddf84e22b0bccde4_538643_460x200_fill_box_smart1_3.png","permalink":"/privacy-policy.html","title":"Our Privacy Policy"},{"categories":null,"contents":"Responsibility of Contributors Lorem ipsum dolor sit amet, consectetur adipiscing elit. Purus, donec nunc eros, ullamcorper id feugiat quisque aliquam sagittis. Sem turpis sed viverra massa gravida pharetra. Non dui dolor potenti eu dignissim fusce. Ultrices amet, in curabitur a arcu a lectus morbi id. Iaculis erat sagittis in tortor cursus. Molestie urna eu tortor, erat scelerisque eget. Nunc hendrerit sed interdum lacus. Lorem quis viverra sed\npretium, aliquam sit. Praesent elementum magna amet, tincidunt eros, nibh in leo. Malesuada purus, lacus, at aliquam suspendisse tempus. Quis tempus amet, velit nascetur sollicitudin. At sollicitudin eget amet in. Eu velit nascetur sollicitudin erhdfvssfvrgss eget viverra nec elementum. Lacus, facilisis tristique lectus in.\nGathering of Personal Information Lorem ipsum dolor sit amet, consectetur adipiscing elit. Purus, donec nunc eros, ullamcorper id feugiat quisque aliquam sagittis. Sem turpis sed viverra massa gravida pharetra. Non dui dolor potenti eu dignissim fusce. Ultrices amet, in curabitur a arcu a lectus morbi id. Iaculis erat sagittis in tortor cursus. Molestie urna eu tortor, erat scelerisque eget. Nunc hendrerit sed interdum lacus. Lorem quis viverra sed\nProtection of Personal- Information Lorem ipsum dolor sit amet, consectetur adipiscing elit. Purus, donec nunc eros, ullamcorper id feugiat quisque aliquam sagittis. Sem turpis sed viverra massa gravida pharetra. Non dui dolor potenti eu dignissim fusce. Ultrices amet, in curabitur a arcu a lectus morbi id. Iaculis erat sagittis in tortor cursus.\nMolestie urna eu tortor, erat scelerisque eget. Nunc hendrerit sed interdum lacus. Lorem quis viverra sed Lorem ipsum dolor sit amet, consectetur adipiscing elit. Purus, donec nunc eros, ullamcorper id feugiat\nPrivacy Policy Changes Sll the Themefisher items are designed to be with the latest , We check all comments that threaten or harm the reputation of any person or organization personal information including, but limited to, email addresses, telephone numbers Any Update come in The technology Customer will get automatic Notification. ","date":"January 1, 1","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/welcomeday1-ident-large_hu55207aec9ed68bb5ddf84e22b0bccde4_538643_460x200_fill_box_smart1_3.png","permalink":"/terms-conditions.html","title":"Our Terms And Conditions"},{"categories":null,"contents":"","date":"January 1, 1","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/welcomeday1-ident-large_hu55207aec9ed68bb5ddf84e22b0bccde4_538643_460x200_fill_box_smart1_3.png","permalink":"/search.html","title":"Search Results"},{"categories":null,"contents":"Ask Us Anything Or just Say Hi, Rather than just filling out a form, Sleeknote also offers help to the user with links directing them to find additional information or take popular actions.\n","date":"January 1, 1","image":"https://d175uvn6dnkepf.cloudfront.net/images/hpc/welcomeday1-ident-large_hu55207aec9ed68bb5ddf84e22b0bccde4_538643_460x200_fill_box_smart1_3.png","permalink":"/contact.html","title":"Talk To Me Anytime :)"}]