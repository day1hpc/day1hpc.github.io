<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs on Day 1 HPC (Staging)</title>
    <link>https://d175uvn6dnkepf.cloudfront.net/post.html</link>
    <description>Recent content in Blogs on Day 1 HPC (Staging)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Oct 2022 17:38:29 +0000</lastBuildDate><atom:link href="https://d175uvn6dnkepf.cloudfront.net/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How KEK changed how everyone in Japan does CryoEM (Part 3 of 4)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/how-kek-changed-how-everyone-in-japan-does-cryoem-part-3-of-4.html</link>
      <pubDate>Thu, 20 Oct 2022 17:38:29 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/how-kek-changed-how-everyone-in-japan-does-cryoem-part-3-of-4.html</guid>
      <description>CryoEM is an awesome research tool, but it also comes with some challenges. The software stack is complex, and the hardware it needs is sometimes exotic and hard to get. Afterwards, there&amp;rsquo;s a massive dataset and a serious set of challenges to moving it around, and managing it. (No one wants to get stuck baby sitting someone else&amp;rsquo;s petabyte dataset unless they have to).
The team at KEK in Japan took all these challenges head on and solved them in a very different way (spoiler: it involved the cloud). But as is often the case, when teams in Japan solve a big HPC problem, it&amp;rsquo;s usually something that&amp;rsquo;s going to get picked up around the world. And KEK are very much those kinds of people.
Today is just one part in a series of Four HPC Tech Shorts dedicated to the work KEK did (yes, it&amp;rsquo;s that awesome).</description>
    </item>
    
    <item>
      <title>KEK&#39;s arsenal of CryoEM benchmark data - a detailed walk through (Part 4 of 4)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/keks-arsenal-of-cryoem-benchmark-data-a-detailed-walk-through-part-4-of-4.html</link>
      <pubDate>Thu, 20 Oct 2022 17:38:23 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/keks-arsenal-of-cryoem-benchmark-data-a-detailed-walk-through-part-4-of-4.html</guid>
      <description>CryoEM is an awesome research tool, but it also comes with some challenges. The software stack is complex, and the hardware it needs is sometimes exotic and hard to get. Afterwards, there&amp;rsquo;s a massive dataset and a serious set of challenges to moving it around, and managing it. (No one wants to get stuck baby sitting someone else&amp;rsquo;s petabyte dataset unless they have to).
The team at KEK in Japan took all these challenges head on and solved them in a very different way (spoiler: it involved the cloud). But as is often the case, when teams in Japan solve a big HPC problem, it&amp;rsquo;s usually something that&amp;rsquo;s going to get picked up around the world. And KEK are very much those kinds of people.
Today is just one part in a series of Four HPC Tech Shorts dedicated to the work KEK did (yes, it&amp;rsquo;s that awesome).</description>
    </item>
    
    <item>
      <title>About Us</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/about.html</link>
      <pubDate>Mon, 17 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/about.html</guid>
      <description>Welcome to Day 1 Welcome to day1hpc.com - a community site built and curated by the Developer Relations team in HPC Engineering at AWS. Our job is to be the interface between our HPC engineering teams and the people in the HPC community who want to use AWS to create powerful tools for solving hard problems.
To do that, we&amp;rsquo;ll spend a lot of our time explaining how these services work, and getting your feedback so we can improve them, continuously. You’ll find a lot of things on this site to help you understand AWS a little better, all with the aim of getting you where to want to go, faster.
We called the site Day1Hpc for a couple of reasons. One is to pay homage to our employer (Amazon Web Services) where Day 1 means &amp;ldquo;focusing on customers, creating long term value over short-term corporate profit, and making many bold bets&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>KEK&#39;s novel solution for CryoEM&#39;s software and infra (Part 2 of 4)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/keks-novel-solution-for-cryoems-software-and-infra-part-2-of-4.html</link>
      <pubDate>Tue, 11 Oct 2022 16:53:39 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/keks-novel-solution-for-cryoems-software-and-infra-part-2-of-4.html</guid>
      <description>CryoEM is an awesome research tool, but it also comes with some challenges. The software stack is complex, and the hardware it needs is sometimes exotic and hard to get. Afterwards, there&amp;rsquo;s a massive dataset and a serious set of challenges to moving it around, and managing it. (No one wants to get stuck baby sitting someone else&amp;rsquo;s petabyte dataset unless they have to).
The team at KEK in Japan took all these challenges head on and solved them in a very different way (spoiler: it involved the cloud). But as is often the case, when teams in Japan solve a big HPC problem, it&amp;rsquo;s usually something that&amp;rsquo;s going to get picked up around the world. And KEK are very much those kinds of people.
Today is just one part in a series of Four HPC Tech Shorts dedicated to the work KEK did (yes, it&amp;rsquo;s that awesome).</description>
    </item>
    
    <item>
      <title>The Challenges of CryoEM with our friends from KEK in Japan (Part 1 of 4)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/the-challenges-of-cryoem-with-our-friends-from-kek-in-japan-part-1-of-4.html</link>
      <pubDate>Thu, 06 Oct 2022 15:04:35 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/the-challenges-of-cryoem-with-our-friends-from-kek-in-japan-part-1-of-4.html</guid>
      <description>CryoEM is an awesome research tool, but it also comes with some challenges. The software stack is complex, and the hardware it needs is sometimes exotic and hard to get. Afterwards, there&amp;rsquo;s a massive dataset and a serious set of challenges to moving it around, and managing it. (No one wants to get stuck baby sitting someone else&amp;rsquo;s petabyte dataset unless they have to).
The team at KEK in Japan took all these challenges head on and solved them in a very different way (spoiler: it involved the cloud). But as is often the case, when teams in Japan solve a big HPC problem, it&amp;rsquo;s usually something that&amp;rsquo;s going to get picked up around the world. And KEK are very much those kinds of people.
Today is one part in a series of Four HPC Tech Shorts dedicated to the work KEK did (yes, it&amp;rsquo;s that awesome).</description>
    </item>
    
    <item>
      <title>Understanding EC2 for HPC users</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/understanding-ec2-for-hpc-users.html</link>
      <pubDate>Thu, 29 Sep 2022 14:59:01 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/understanding-ec2-for-hpc-users.html</guid>
      <description>Amazon EC2 is the engine that powers HPC in the cloud. There&amp;rsquo;s quite a lot that&amp;rsquo;s new if you&amp;rsquo;ve never seen it before. in this Level 1 module in the Tech Shorts Foundations Series, we aim to demystify EC2 for the HPC community.
We&amp;rsquo;ll expand the number of videos in this series over time - but if there&amp;rsquo;s a topic that&amp;rsquo;s really interesting to you that we&amp;rsquo;re overlooking, don&amp;rsquo;t hesitate to contact us.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Getting the Best Price Performance for Numerical Weather Prediction Workloads on AWS</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/getting-the-best-price-performance-for-numerical-weather-prediction-workloads-on-aws.html</link>
      <pubDate>Tue, 27 Sep 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/getting-the-best-price-performance-for-numerical-weather-prediction-workloads-on-aws.html</guid>
      <description>In this post, we will provide an overview of Numerical Weather Prediction (NWP) workloads, and the AWS HPC-optimized services for it. We’ll test three popular NWP codes: WRF, MPAS, and FV3GFS.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Rearchitecting AWS Batch managed services to leverage AWS Fargate</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/rearchitecting-aws-batch-managed-services-to-leverage-aws-fargate.html</link>
      <pubDate>Wed, 21 Sep 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/rearchitecting-aws-batch-managed-services-to-leverage-aws-fargate.html</guid>
      <description>AWS service teams continuously improve the underlying infrastructure and operations of managed services, and AWS Batch is no exception. The AWS Batch team recently moved most of their job scheduler fleet to a serverless infrastructure model leveraging AWS Fargate. I had a chance to sit with Devendra Chavan, Senior Software Development Engineer on the AWS Batch team, to discuss the move to AWS Fargate and its impact on the Batch managed scheduler service component.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Welcome to tech Shorts Foundations</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/welcome-to-tech-shorts-foundations.html</link>
      <pubDate>Thu, 15 Sep 2022 15:45:34 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/welcome-to-tech-shorts-foundations.html</guid>
      <description>We get it: AWS has too much stuff going on for any reasonably busy human to catch up with. So we&amp;rsquo;re going back to the foundations of AWS to help the HPC community come up to speed on the stuff that matters to them most.
Today we&amp;rsquo;re starting a whole new series in Tech Shorts called Tech Shorts Foundations.
We&amp;rsquo;ll start high level and gradually peel back the layers so you can get as deep as you like in the areas you&amp;rsquo;re interested in the most.
Think of this as your map to navigate AWS and get to the HPC places quickly, without having to get lost on the way there. Your journey starts here.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Intro to AWS for HPC People - Tech Short Foundations Level 1</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/intro-to-aws-for-hpc-people-tech-short-foundations-level-1.html</link>
      <pubDate>Thu, 15 Sep 2022 15:35:56 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/intro-to-aws-for-hpc-people-tech-short-foundations-level-1.html</guid>
      <description>We get it: AWS has too much stuff going on for any reasonably busy human to catch up with. So we&amp;rsquo;re going back to the foundations of AWS to help the HPC community come up to speed on the stuff that matters to them most.
Today we&amp;rsquo;re starting a whole new series in Tech Shorts called Tech Shorts Foundations. We&amp;rsquo;ll still have shows about advanced topics, new features and performance analysis. But there&amp;rsquo;s now two tracks in Tech Shorts, which will develop over time.
We&amp;rsquo;ll start high level and gradually peel back the layers so you can get as deep as you like in the areas you&amp;rsquo;re interested in the most.
Think of this as your map to navigate AWS and get to the HPC places quickly, without having to get lost on the way there. Your journey starts here.</description>
    </item>
    
    <item>
      <title>Easing your migration from SGE to Slurm in AWS ParallelCluster 3</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/easing-your-migration-from-sge-to-slurm-in-aws-parallelcluster-3.html</link>
      <pubDate>Wed, 14 Sep 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/easing-your-migration-from-sge-to-slurm-in-aws-parallelcluster-3.html</guid>
      <description>This post will help you understand the tools available to ease the stress of migrating your cluster (and your users) from SGE to Slurm, which is necessary since the HPC community is no longer supporting SGE’s open-source codebase.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>A serverless architecture for high performance financial modelling</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/a-serverless-architecture-for-high-performance-financial-modelling.html</link>
      <pubDate>Tue, 06 Sep 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/a-serverless-architecture-for-high-performance-financial-modelling.html</guid>
      <description>Understanding deal and portfolio risk and capital requirements is a computationally expensive process that requires the execution of multiple financial forecasting models every day and in often in real time. This post describes how it works at RenaissanceRe, one of the world’s leading reinsurance companies.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Simulating 44-Qubit quantum circuits using AWS ParallelCluster</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/simulating-44-qubit-quantum-circuits-using-aws-parallelcluster.html</link>
      <pubDate>Tue, 30 Aug 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/simulating-44-qubit-quantum-circuits-using-aws-parallelcluster.html</guid>
      <description>A key part of the development of quantum hardware and quantum algorithms is simulation using existing classical architectures and HPC techniques. In this blog post, we describe how to perform large-scale quantum circuits simulations using AWS ParallelCluster with QuEST, the Quantum Exact Simulation Toolkit. We demonstrate a simple and rapid deployment of computational resources up to 4,096 compute instances to simulate random quantum circuits with up to 44 qubits. We were able to allocate as many as 4096 EC2 instances of c5.18xlarge to simulate a non-trivial 44 qubit quantum circuit in fewer than 3.5 hours.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Accelerating Genomics Pipelines Using Intel’s Open Omics Acceleration Framework on AWS</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/accelerating-genomics-pipelines-using-intels-open-omics-acceleration-framework-on-aws.html</link>
      <pubDate>Tue, 23 Aug 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/accelerating-genomics-pipelines-using-intels-open-omics-acceleration-framework-on-aws.html</guid>
      <description>In this blog, we showcase the first version of Open Omics and benchmark three applications that are used in processing NGS data – sequence alignment tools BWA-MEM, minimap2, and single cell ATAC-Seq on Xeon-based Amazon Elastic Compute Cloud (Amazon EC2) Instances.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Building a Scalable Predictive Modeling Framework in AWS – Part 3</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/building-a-scalable-predictive-modeling-framework-in-aws-part-3.html</link>
      <pubDate>Thu, 11 Aug 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/building-a-scalable-predictive-modeling-framework-in-aws-part-3.html</guid>
      <description>In this final part of this three-part blog series on building predictive models at scale in AWS, we will use the synthetic dataset and the models generated in the previous post to showcase the model updating and sensitivity analysis capabilities of the aws-do-pm framework.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Building a Scalable Predictive Modeling Framework in AWS – Part 2</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/building-a-scalable-predictive-modeling-framework-in-aws-part-2.html</link>
      <pubDate>Wed, 10 Aug 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/building-a-scalable-predictive-modeling-framework-in-aws-part-2.html</guid>
      <description>In the first part of this three-part blog series, we introduced the aws-do-pm framework for building predictive models at scale in AWS. In this blog, we showcase a sample application for predicting the life of batteries in a fleet of electric vehicles, using the aws-do-pm framework.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Building a Scalable Predictive Modeling Framework in AWS – Part 1</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/building-a-scalable-predictive-modeling-framework-in-aws-part-1.html</link>
      <pubDate>Tue, 09 Aug 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/building-a-scalable-predictive-modeling-framework-in-aws-part-1.html</guid>
      <description>Predictive models have powered the design and analysis of real-world systems such as jet engines, automobiles, and powerplants for decades. These models are used to provide insights on system performance and to run simulations, at a fraction of the cost compared to experiments with physical hardware. In this first post of three, we described the motivation and general architecture of the open-source aws-do-pm framework project for building predictive models at scale in AWS.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Get your HPC codes installed and running in minutes using Spack&#39;s Binary Cache</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/get-your-hpc-codes-installed-and-running-in-minutes-using-spacks-binary-cache.html</link>
      <pubDate>Mon, 08 Aug 2022 18:28:02 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/get-your-hpc-codes-installed-and-running-in-minutes-using-spacks-binary-cache.html</guid>
      <description>Spack has already been removing the ugly work from building HPC codes, but with the announcement of the Spack Binary Cache at ISC&#39;22, build and deploy times for these complicated applications will drop by 95% or more in most cases.
Greg Becker from Livermore came along to show us how it works, and discuss what&amp;rsquo;s behind it.
You can find a blog post about the announcement here: hpc.news/binaryCache
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Running large-scale CFD fire simulations on AWS for Amazon.com</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/running-large-scale-cfd-fire-simulations-on-aws-for-amazoncom.html</link>
      <pubDate>Wed, 03 Aug 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/running-large-scale-cfd-fire-simulations-on-aws-for-amazoncom.html</guid>
      <description>In this blog post, we discuss the AWS solution that Amazon’s construction division used to conduct large-scale CFD fire simulations as part of their Fire Strategy solutions to demonstrate safety and fire mitigation strategies. We outline the five key steps taken that resulted in simulation times that were 15-20x faster than previous on-premises architectures, reducing the time to complete from up to twenty-one days to less than one day.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>New file systems support in ParallelCluster 3.2 (Part 1 of 2)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/new-file-systems-support-in-parallelcluster-32-part-1-of-2.html</link>
      <pubDate>Thu, 28 Jul 2022 14:07:47 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/new-file-systems-support-in-parallelcluster-32-part-1-of-2.html</guid>
      <description>ParallelCluster can now mount lots and lots of file systems that you&amp;rsquo;ve previously created in your AWS account, in addition to the scratch filesystem you can ask it to create for you when you launch your cluster. And as of today, ParallelCluster supports OpenZFS as one of those filesystems, along with Netapp ONTAP - which will help you get access to data on your enterprise filesystems, too.
Olly Perks and Austin Cherian describe all this in detail, as part 1 of a 2-part series covering the new features of ParallelCluster 3.2.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Memory aware scheduling with Slurm in ParallelCluster 3.2 (Part 2 of 2)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/memory-aware-scheduling-with-slurm-in-parallelcluster-32-part-2-of-2.html</link>
      <pubDate>Thu, 28 Jul 2022 14:07:45 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/memory-aware-scheduling-with-slurm-in-parallelcluster-32-part-2-of-2.html</guid>
      <description>If you&amp;rsquo;ve ever had to iteratively guess how much memory is left in a bunch of compute nodes in order to get your memory-hungry jobs running, then this feature will save your sanity.
It&amp;rsquo;s a new integration between ParallelCluster and Slurm that lets you specify how much RAM your jobs need, and gives Slurm the ability to figure out how to place your jobs in order to achieve that - not just counting cores, which is the default behavior for most schedulers.
Olly Perks and Austin Cherian describe this in detail, as part of a 2-part series covering the new features of ParallelCluster 3.2 (part 1 covered new file systems support and you can find it here: https://youtu.be/2JOoMv-K1FY).
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Expanded filesystems support in AWS ParallelCluster 3.2</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/expanded-filesystems-support-in-aws-parallelcluster-32.html</link>
      <pubDate>Thu, 28 Jul 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/expanded-filesystems-support-in-aws-parallelcluster-32.html</guid>
      <description>AWS ParallelCluster version 3.2 introduces support for two new Amazon FSx filesystem types (NetApp ONTAP and OpenZFS). It also lifts the limit on the number of filesystem mounts you can have on your cluster. We’ll show you how, and help you with the details for getting this going right away.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Slurm-based memory-aware scheduling in AWS ParallelCluster 3.2</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/slurm-based-memory-aware-scheduling-in-aws-parallelcluster-32.html</link>
      <pubDate>Thu, 28 Jul 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/slurm-based-memory-aware-scheduling-in-aws-parallelcluster-32.html</guid>
      <description>AWS ParallelCluster version 3.2 now supports memory-aware scheduling in Slurm to give you control over the placement of jobs with specific memory requirements. In this blog post, we’ll show you how it works, and explain why this will be really useful to people with memory-hungry workloads.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Call for participation: RADIUSS Tutorial Series</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/call-for-participation-radiuss-tutorial-series.html</link>
      <pubDate>Fri, 22 Jul 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/call-for-participation-radiuss-tutorial-series.html</guid>
      <description>Lawrence Livermore National Laboratory (LLNL) and AWS are joining forces to provide a training opportunity for emerging HPC tools and application. RADIUSS (Rapid Application Development via an Institutional Universal Software Stack) is a broad suite of open-source software projects originating from LLNL. Together we are hosting a tutorial series to give attendees hands-on experience with these cutting-edge technologies. Find out how to participate in these events in this blog post.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Clusters in the Cloud made easier with PCluster Manager (Part 2)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/clusters-in-the-cloud-made-easier-with-pcluster-manager-part-2.html</link>
      <pubDate>Tue, 19 Jul 2022 17:09:59 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/clusters-in-the-cloud-made-easier-with-pcluster-manager-part-2.html</guid>
      <description>In part 1, we covered all the aspects of designing and creating ParallelClusters in PCluster Manager. Today we delve into some more advanced topics like debugging your stack when something goes wrong, managing access to the cluster via the UI and CLI, and visualization, which is something that&amp;rsquo;s just super hard to do anywhere else.
Sean Smith shows us how this is all a lot easier with the right tools. if you want to install PCluster Manager in your account, head over to hpc.news/pclustermanager and deploy one of the one-click lauchable stacks to get going. There&amp;rsquo;s even an episode with Charlie (from our engineering team) showing how to install it and get it set up (it&amp;rsquo;s here: https://youtu.be/Z1vlpJYb1KQ).
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Analyzing Genomic Data using Amazon Genomics CLI and Amazon SageMaker</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/analyzing-genomic-data-using-amazon-genomics-cli-and-amazon-sagemaker.html</link>
      <pubDate>Tue, 19 Jul 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/analyzing-genomic-data-using-amazon-genomics-cli-and-amazon-sagemaker.html</guid>
      <description>In this blog post, we demonstrate how to leverage the AWS Genomics Command line and Amazon SageMaker to analyze large-scale exome sequences and derive meaningful insights. We use the bioinformatics workflow manager Nextflow, it’s open source library of pipelines, NF-Core, and AWS Batch.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Clusters in the Cloud made easier with PCluster Manager (Part 1 of 2)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/clusters-in-the-cloud-made-easier-with-pcluster-manager-part-1-of-2.html</link>
      <pubDate>Thu, 14 Jul 2022 16:59:11 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/clusters-in-the-cloud-made-easier-with-pcluster-manager-part-1-of-2.html</guid>
      <description>Today is the first part of a 2-part series and we&amp;rsquo;re covering all the aspects of designing and creating ParallelClusters using PCluster Manager, which is a graphical user interface tool for making all the hard bits of cluster management way less messy and so much easier. All the documentation in the world isn&amp;rsquo;t as useful as this one tool.
In part 2, we&amp;rsquo;ll dive into some advanced topics like debugging your cluster build when something goes wrong, and we&amp;rsquo;ll show you some fancy access methods, and tools for managing jobs - and visualizing the results.
Sean Smith shows us how this is all a lot easier with the right tools. if you want to install PCluster Manager in your account, head over to hpc.news/pclustermanager and deploy one of the one-click lauchable stacks to get going. There&amp;rsquo;s even an episode with Charlie (from our engineering team) showing how to install it and get it set up (it&amp;rsquo;s here: https://youtu.</description>
    </item>
    
    <item>
      <title>How Thermo Fisher Scientific Accelerated Cryo-EM using AWS ParallelCluster</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/how-thermo-fisher-scientific-accelerated-cryo-em-using-aws-parallelcluster.html</link>
      <pubDate>Tue, 12 Jul 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/how-thermo-fisher-scientific-accelerated-cryo-em-using-aws-parallelcluster.html</guid>
      <description>In this blog post, we’ll walk you through the process of building a successful Cryo-EM benchmarking pilot using AWS ParallelCluster, Amazon FSx for Lustre, and cryoSPARC (from Structura Biotechnology) and explain some of our design decisions along the way.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Efficient and cost-effective rendering pipelines with Blender and AWS Batch</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/efficient-and-cost-effective-rendering-pipelines-with-blender-and-aws-batch.html</link>
      <pubDate>Tue, 05 Jul 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/efficient-and-cost-effective-rendering-pipelines-with-blender-and-aws-batch.html</guid>
      <description>This blog post explains how to run parallel rendering workloads and produce an animation in a cost and time effective way using AWS Batch and AWS Step Functions. AWS Batch manages the rendering jobs on Amazon Elastic Compute Cloud (Amazon EC2), and AWS Step Functions coordinates the dependencies across the individual steps of the rendering workflow. Additionally, Amazon EC2 Spot instances can be used to reduce compute costs by up to 90% compared to On-Demand prices.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>NCCL on EFA makes the ML world go around in the cloud</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/nccl-on-efa-makes-the-ml-world-go-around-in-the-cloud.html</link>
      <pubDate>Thu, 30 Jun 2022 17:05:07 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/nccl-on-efa-makes-the-ml-world-go-around-in-the-cloud.html</guid>
      <description>Machine Learning is a huge workload, and one of the most demanding when it comes to scaling to thousands (and thousands) of CPUs. Some of the largest workloads customers run in the cloud are deep learning models, which require huge numbers of GPUs and saturate the networks connecting them.
To make all that work on AWS, NVIDIA&amp;rsquo;s collectives communications library (NCCL) relies on libfabrics to speak to the EFA hardware that makes up EC2&amp;rsquo;s high performance interconnect.
Rashika Kheria leads the team in Annapurna that handles this interface, ensuring your models, using all your favorite frameworks, scale really nicely to as far as your imagination allows (well, maybe a little further). She came to Tech Shorts to tell us how that works.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Getting Started with NVIDIA Clara Parabricks on AWS Batch using AWS CloudFormation</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/getting-started-with-nvidia-clara-parabricks-on-aws-batch-using-aws-cloudformation.html</link>
      <pubDate>Tue, 28 Jun 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/getting-started-with-nvidia-clara-parabricks-on-aws-batch-using-aws-cloudformation.html</guid>
      <description>In this blog post, we’ll show how you can run NVIDIA Parabricks on AWS Batch leveraging AWS CloudFormation templates. Parabricks is a GPU-accelerated tool for secondary genomic analysis. It reduces the runtime of variant calling on a 30x human genome from 30 hours to just 30 minutes, and leverages AWS Batch to provide an interface that scales compute jobs across multiple instances in the cloud.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Bayesian models and half a million cores - what&#39;re you waiting for?</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/bayesian-models-and-half-a-million-cores-whatre-you-waiting-for.html</link>
      <pubDate>Fri, 24 Jun 2022 14:59:17 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/bayesian-models-and-half-a-million-cores-whatre-you-waiting-for.html</guid>
      <description>The Ampersand Data Science team had a challenge: their Bayesian statistical models needed more than half a million core-hours of runtime, regularly, if they were to get an answer fast enough for it to be useful to their customers.
Scaling to a million core or more isn&amp;rsquo;t really a challenge now (thanks to Amazon EC2). The hard part is all the code pipelines and plumbing - and the management of the entire thing when it&amp;rsquo;s in flight.
Daniel Gerlanc (Senior Director for Data Science) and Jeffrey Enos (Senior Machine Learning Engineer) swung by the Tech Shorts virtual watercooler to tell us how it worked, what was most surprising, and which bits made all the difference.
There&amp;rsquo;s also a blog that was posted last week which talks to some of this too. Worth a read: https://aws.amazon.com/blogs/hpc/bayesian-ml-models-at-scale-with-aws-batch/
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Understanding the AWS Batch termination process</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/understanding-the-aws-batch-termination-process.html</link>
      <pubDate>Tue, 21 Jun 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/understanding-the-aws-batch-termination-process.html</guid>
      <description>In this blog post, we help you understand the AWS Batch job termination process and how you may take actions to gracefully terminate a job by capturing SIGTERM signal inside the application. It provides you with an efficient way to exit your Batch jobs. You also get to know about how job timeouts occur, and how the retry operation works with both traditional AWS Batch jobs and array jobs.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>New console features including container insights in AWS Batch</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/new-console-features-including-container-insights-in-aws-batch.html</link>
      <pubDate>Thu, 16 Jun 2022 17:12:05 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/new-console-features-including-container-insights-in-aws-batch.html</guid>
      <description>The AWS Batch team recently added container insights and advanced logging features to the Batch console, which is making a LOT of people very happy.
We decided to catch up with the dev team in Seattle who worked on this, and one of my favotire Amazonians - David Chambers - joined me to help us all understand what this means to Batch users and to taker us for a drive to sere how easy it is.
This is our first ever in-person Tech Short, which is something we&amp;rsquo;re really keen to build on, now that travel is starting to happen more. Hope you like it.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Bayesian ML Models at Scale with AWS Batch</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/bayesian-ml-models-at-scale-with-aws-batch.html</link>
      <pubDate>Tue, 14 Jun 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/bayesian-ml-models-at-scale-with-aws-batch.html</guid>
      <description>Ampersand is a data-driven TV advertising technology company that provides aggregated TV audience impression insights and planning on 42 million households, in every media market, across more than 165 networks and apps and in all dayparts (broadcast day segments). The Ampersand Data Science team estimated that building their statistical models would require up to 600,000 physical CPU hours to run, which would not be feasible without using a massively parallel and large-scale architecture in the cloud. AWS Batch enabled Ampersand to compress their time of computation over 500x through massive scaling while optimizing their costs using Amazon EC2 Spot. In this blog post, we will provide an overview of how Ampersand built their TV audience impressions (“impressions”) models at scale on AWS, review the architecture they have been using, and discuss optimizations they conducted to run their workload efficiently on AWS Batch.</description>
    </item>
    
    <item>
      <title>Running cost-effective GROMACS simulations using Amazon EC2 Spot Instances with AWS ParallelCluster</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/running-cost-effective-gromacs-simulations-using-amazon-ec2-spot-instances-with-aws-parallelcluster.html</link>
      <pubDate>Thu, 09 Jun 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/running-cost-effective-gromacs-simulations-using-amazon-ec2-spot-instances-with-aws-parallelcluster.html</guid>
      <description>In this blog post, we cover how to run GROMACS – a popular open source designed for simulations of proteins, lipids, and nucleic acids – cost effectively by leveraging EC2 Spot Instances within AWS ParallelCluster. We also show how to checkpoint GROMACS to recover gracefully from possible Spot Instance interruptions.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>The Arm64 developer environments - Part 2 of 2</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/the-arm64-developer-environments-part-2-of-2.html</link>
      <pubDate>Wed, 08 Jun 2022 15:15:36 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/the-arm64-developer-environments-part-2-of-2.html</guid>
      <description>Olly Perks - our resident Arm64 architecture expert and all round HPC software guy - walks us through the quite extensive support for HPC developers who are planning to work on AWS Graviton processors.
There&amp;rsquo;s a lot of software and it comes from a number of sources, including open source and commercial groups.
This is part 2 of a two-part series - the first part was published last week, abd you can find it here: https://youtu.be/qFrpmgvN9Xs
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>The Arm64 developer environments - Part 1 (of 2)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/the-arm64-developer-environments-part-1-of-2.html</link>
      <pubDate>Thu, 02 Jun 2022 07:48:02 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/the-arm64-developer-environments-part-1-of-2.html</guid>
      <description>Olly Perks - our resident Arm64 architecture expert and all round HPC software guy - walks us through the quite extensive support for HPC developers who are planning to work on AWS Graviton processors.
There&amp;rsquo;s a lot of software and it comes from a number of sources, including open source and commercial groups.
This is part 1 of a two-part series. The other part will be out early next week.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Introducing the Spack Rolling Binary Cache hosted on AWS</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/introducing-the-spack-rolling-binary-cache-hosted-on-aws.html</link>
      <pubDate>Tue, 31 May 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/introducing-the-spack-rolling-binary-cache-hosted-on-aws.html</guid>
      <description>Today we’re excited to announce the availability of a new public Spack Binary Cache. In a collaboration, between AWS, E4S, Kitware, and the Lawrence Livermore National Laboratory (LLNL), Spack users now have access to a public build cache hosted on Amazon S3. The use of this Binary Cache will result in up to 20x faster install times for common Spack packages.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Winter Invitational Episode II - The Algorithms Strike Back</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/winter-invitational-episode-ii-the-algorithms-strike-back.html</link>
      <pubDate>Thu, 19 May 2022 15:42:17 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/winter-invitational-episode-ii-the-algorithms-strike-back.html</guid>
      <description>This is the second part of a two-part set about the Winter Invitational Student Cluster Competition, which the AWS HPC team supported as one of four mentor teams.
This isn&amp;rsquo;t just about the competition or the results. It&amp;rsquo;s about the skills that are important to getting a job, learning new things and solving tough problems. And it&amp;rsquo;s about working together with other people.
If you&amp;rsquo;re able to support these kinds of events, you should give it a try. it&amp;rsquo;s super rewarding.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Winter Invitational Episode I - New Hopefuls</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/winter-invitational-episode-i-new-hopefuls.html</link>
      <pubDate>Tue, 17 May 2022 13:09:07 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/winter-invitational-episode-i-new-hopefuls.html</guid>
      <description>This is the first part of a two-part set about the Winter Invitational Student Cluster Competition (which the AWS HPC team supported as one of four mentor teams). The competition targeted academic institutions that we need to succeed to bring some balance to the HPC force.
Dan Olds, the Chief Research Officer of Intersect360, and the unofficial historian (and historical figure) of Student Cluster Comps, joined us to walk us through the structure of the comp, and feed us some of the drama leading up to an extraordinary result in 2022.
This isn&amp;rsquo;t just about the competition or the results. It&amp;rsquo;s about the skills that are important to getting a job, learning new things and solving tough problems while working together with other people.
If you&amp;rsquo;re able to support these kinds of events, you should give it a try. it&amp;rsquo;s super rewarding.</description>
    </item>
    
    <item>
      <title>Encoding workflow dependencies in AWS Batch</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/encoding-workflow-dependencies-in-aws-batch.html</link>
      <pubDate>Wed, 11 May 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/encoding-workflow-dependencies-in-aws-batch.html</guid>
      <description>This post covers the different ways you can encode a dependency between basic and array jobs in AWS Batch. We also cover why you may want to encode dependencies outside of Batch altogether using a workflow system like AWS Step Functions or Apache Airflow.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>What makes the AWS Graviton 3 so interesting to HPC and AI/ML customers?</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/what-makes-the-aws-graviton-3-so-interesting-to-hpc-and-aiml-customers.html</link>
      <pubDate>Thu, 05 May 2022 16:35:59 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/what-makes-the-aws-graviton-3-so-interesting-to-hpc-and-aiml-customers.html</guid>
      <description>The AWS Graviton 3 will be launched this year as part of the new C7g instance. Inside this chip are some interesting innovations that already have a lot of HPC and AI/ML customers interested.
We sat down with Olly Perks, who recently joined AWS HPC Engineering from Arm, to discuss what&amp;rsquo;s most interesting. This is the first of a series of Tech Shorts covering the Graviton 3 architecture. We&amp;rsquo;ll deep dive on techniques and tools for getting the most out of these CPUs.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>YellowDog make scaling to crazy large workloads easy</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/yellowdog-make-scaling-to-crazy-large-workloads-easy.html</link>
      <pubDate>Fri, 29 Apr 2022 13:00:02 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/yellowdog-make-scaling-to-crazy-large-workloads-easy.html</guid>
      <description>YellowDog (yellowdog.co) cut their teeth in the media and entertainment industry working on orchestrating really large workloads for CGI and FX companies. They&amp;rsquo;ve generalized the platform and now they&amp;rsquo;re able to do the same thing for nearly any kind of workload in the cloud (or hybrid on-prem/cloud scenarios).
They understand the complexities of doing this, and have features and tooling for dealing with the nitty gritty (rationing licenses for ISV applications, data movement and staging etc etc).
Alan Parry, their Director of Engineering showed us how easy it is.
In the show, we mentioned a blog post, which you can find here: https://hpc.news/hawking901
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>AWS Batch updates: higher compute utilization, AWS PrivateLink support, and updatable compute environments</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/aws-batch-updates-higher-compute-utilization-aws-privatelink-support-and-updatable-compute-environments.html</link>
      <pubDate>Mon, 25 Apr 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/aws-batch-updates-higher-compute-utilization-aws-privatelink-support-and-updatable-compute-environments.html</guid>
      <description>In this post, I cover some of the recent updates to AWS Batch, including improvements to job placement, addition of AWS PrivateLink support, and the new capabilities to update your AWS Batch compute environments.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Protein folding (AlphaFold) at SCALE using a notebook and the cloud</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/protein-folding-alphafold-at-scale-using-a-notebook-and-the-cloud.html</link>
      <pubDate>Wed, 20 Apr 2022 13:33:52 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/protein-folding-alphafold-at-scale-using-a-notebook-and-the-cloud.html</guid>
      <description>Protein folding simulations can consume huge CPU time on supercomputers. AlphaFold changed that by applying ML techniques to the task. Then the problem became &amp;lsquo;how do I run AlphaFold easily?&amp;rsquo;.
The Healthcare AI/ML team at AWS figured that out, built a really accessible solution and open-sourced it to GitHub, which is where you can grab it today.
Brian Loyal and Ujjwal Ratan from the AI/ML specialist team in our global HCLS org joined us to help understand how this fits, why it works, and described some of the awesome science it supports.
They blogged about it here: https://hpc.news/curie264 The CloudFormation stack is here: https://github.com/aws-samples/aws-batch-architecture-for-alphafold
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Benchmarking NVIDIA Clara Parabricks Somatic Variant Calling Pipeline on AWS</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/benchmarking-nvidia-clara-parabricks-somatic-variant-calling-pipeline-on-aws.html</link>
      <pubDate>Wed, 20 Apr 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/benchmarking-nvidia-clara-parabricks-somatic-variant-calling-pipeline-on-aws.html</guid>
      <description>Somatic variants are genetic alterations which are not inherited but acquired during one’s lifespan, for example those that are present in cancer tumors. In this post, we will demonstrate how to perform somatic variant calling from matched tumor and normal genome sequence data, as well as tumor-only whole genome and whole exome datasets using an NVIDIA GPU-accelerated Parabricks pipeline, and compare the results with baseline CPU-based workflows.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>AI-based drug discovery with Atomwise and WEKA Data Platform</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/ai-based-drug-discovery-with-atomwise-and-weka-data-platform.html</link>
      <pubDate>Tue, 12 Apr 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/ai-based-drug-discovery-with-atomwise-and-weka-data-platform.html</guid>
      <description>Drug discovery is an expensive proposition, with a $2.6 billion cost over 10 years and just a 12% success rate. AI promises to significantly improve the success rate by finding small molecule hits for undruggable targets. On the forefront of using AI in drug discovery is Atomwise, with its AtomNet® platform. In this blog, we will lay out the challenges of the drug discovery process, and show how AI/ML startups are solving these challenges using solutions from Atomwise, AWS, and WEKA.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Spooky-action-at-a-distance As a Service (Quantum Computing)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/spooky-action-at-a-distance-as-a-service-quantum-computing.html</link>
      <pubDate>Fri, 08 Apr 2022 16:16:10 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/spooky-action-at-a-distance-as-a-service-quantum-computing.html</guid>
      <description>Amazon Braket is AWS&amp;rsquo;s Quantum Computing service which gives you access to several Quantum Computers using a serverless programming model and &amp;hellip; YOU ACTUALLY GET TO USE REAL QUANTUM COMPUTERS without needing a $50M budget and a cryo facility.
This is therefore one of the weirdest Tech Shorts we&amp;rsquo;ve done. We even run a Quantum version of &amp;lsquo;Hello, world&amp;rsquo;. No kidding.
You can get more information about the service at aws.amazon.com/braket
But : why not try it yourself?
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Simcenter STAR-CCM&#43; price-performance on AWS</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/simcenter-star-ccm-price-performance-on-aws.html</link>
      <pubDate>Tue, 05 Apr 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/simcenter-star-ccm-price-performance-on-aws.html</guid>
      <description>Organizations such as Amazon Prime Air and Joby Aviation use Simcenter STAR-CCM+ for running CFD simulations on AWS so they can reduce product manufacturing cycles and achieve faster times to market. In this post today, we describe the performance and price analysis of running Computational Fluid Dynamics (CFD) simulations using Siemens SimcenterTM STAR-CCM+TM software on AWS HPC clusters.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Controlling hybrid workflows between on-premises and cloud with EnginFrame</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/controlling-hybrid-workflows-between-on-premises-and-cloud-with-enginframe.html</link>
      <pubDate>Thu, 31 Mar 2022 20:54:58 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/controlling-hybrid-workflows-between-on-premises-and-cloud-with-enginframe.html</guid>
      <description>EnginFrame makes life easy for scientists and engineers so they can use HPC resources without having to understand the complexity.
BUT: EnginFrame also makes the local sysadmin a hero by giving them the ability to embed into simple scripts the decisions that lead to determining how and where and when a job gets run.
The use cases are almost endless:
decide whether your job bursts to the cloud based on congestion conditions in your on&amp;ndash;prem queues figure out if data needs to be moved first before a job can be run decide based on job parameters whether a job is fit for execution in the AWS spot market, or should be run on-prem, or run in on-demand queues. The workshop we discussed is here: https://hpc.news/efConnectorWorkshop
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Data Science workflows at insitro: how redun uses the advanced service features from AWS Batch and AWS Glue</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/data-science-workflows-at-insitro-how-redun-uses-the-advanced-service-features-from-aws-batch-and-aws-glue.html</link>
      <pubDate>Thu, 31 Mar 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/data-science-workflows-at-insitro-how-redun-uses-the-advanced-service-features-from-aws-batch-and-aws-glue.html</guid>
      <description>Matt Rasmussen, VP of Software Engineering at insitro, expands on his first post on redun, insitro’s data science tool for bioinformatics, to describe how redun makes use of advanced AWS features. Specifically, Matt describes how AWS Batch’s Array Jobs is used to support workflows with large fan-out, and how AWS Glue’s DynamicFrame is used to run computationally heterogenous workflows with different back-end needs such as Spark, all in the same workflow definition.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Data Science workflows at insitro: using redun on AWS Batch</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/data-science-workflows-at-insitro-using-redun-on-aws-batch.html</link>
      <pubDate>Thu, 31 Mar 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/data-science-workflows-at-insitro-using-redun-on-aws-batch.html</guid>
      <description>Matt Rasmussen, VP of Software Engineering at insitro describes their recently released, open-source data science framework, redun, which allows data scientists to define complex scientific workflows that scale from their laptop to large-scale distributed runs on serverless platforms like AWS Batch and AWS Glue. I this post, Matt shows how redun lends itself to Bioinformatics workflows which typically involve wrapping Unix-based programs that require file staging to and from object storage. In the next blog post, Matt describes how redun scales to large and heterogenous workflows by leveraging AWS Batch features such as Array Jobs and AWS Glue features such as Glue DynamicFrame.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Genomics workflow set made easy with AWS Genomics CLI</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/genomics-workflow-set-made-easy-with-aws-genomics-cli.html</link>
      <pubDate>Fri, 25 Mar 2022 11:29:03 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/genomics-workflow-set-made-easy-with-aws-genomics-cli.html</guid>
      <description>The AWS Genomics CLI (or AGC) seriously removes all the grunt work involved in setting up bioinformatics pipelines to run in the cloud. We know that Snakemake, Cromwell, NextFlow and miniWDL all work happily in the cloud on AWS Batch, but AGC means you don&amp;rsquo;t have to know to set all that stuff up - it does it for you.
You can have completely separate tool chains using completely different workflow languages all running at the same time, on the same infrastructure (if you like), sharing data and tooling.
Lee Pang from the dev team that built this came along to show us how it works, and - most importantly - how easy it is to get productive. Zero to hero in less than 30 mins - it&amp;rsquo;s really impressive.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Creating a digital map of COVID-19 virus for discovery of new treatment compounds</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/creating-a-digital-map-of-covid-19-virus-for-discovery-of-new-treatment-compounds.html</link>
      <pubDate>Tue, 22 Mar 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/creating-a-digital-map-of-covid-19-virus-for-discovery-of-new-treatment-compounds.html</guid>
      <description>Quantum physics and high-performance computing have slashed research times for a consortium of researchers led by Qubit Pharmaceuticals. This post describes the discovery of chemical substances that may lead to new COVID-19 treatments in only six months using cloud technology.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Multi-user in ParallelCluster with LDAP and MS Active Directory</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/multi-user-in-parallelcluster-with-ldap-and-ms-active-directory.html</link>
      <pubDate>Thu, 17 Mar 2022 15:34:44 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/multi-user-in-parallelcluster-with-ldap-and-ms-active-directory.html</guid>
      <description>Customers have been asking us to make it easy to configure ParallelCluster to use a corporate directory server, so adding a user to the cloud HPC stack becomes as easy as adding them to (say) the Engineering Group in Active Directory or LDAP.
We&amp;rsquo;re happy to say we&amp;rsquo;ve done that, and released a blog about it a couple weeks ago. In today&amp;rsquo;s show, Giacomo Marciani (one of the developers who built this feature) came along to explain how it works and give us a demo, so you can get to the important steps fast.
During the show we spoke about a blog and a tutorial. They&amp;rsquo; right here:
Blog: https://hpc.news/turing313 Tutorial: https://hpc.news/hawking513 If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Migrating to AWS ParallelCluster v3 – Updated CLI interactions</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/migrating-to-aws-parallelcluster-v3-updated-cli-interactions.html</link>
      <pubDate>Thu, 17 Mar 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/migrating-to-aws-parallelcluster-v3-updated-cli-interactions.html</guid>
      <description>The AWS ParallelCluster version 3 CLI differs significantly from ParallelCluster version 2. This post provides some guidance on mapping between versions to help you with migrating to ParallelCluster 3. We also summarize new CLI features in ParallelCluster 3 to expose the things you just couldn’t do previously.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>OpenZFS created quickly with checkpoints, backups and big performance</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/openzfs-created-quickly-with-checkpoints-backups-and-big-performance.html</link>
      <pubDate>Thu, 10 Mar 2022 15:47:17 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/openzfs-created-quickly-with-checkpoints-backups-and-big-performance.html</guid>
      <description>We launched Amazon FSx for OpenZFS at re:invent in December, and well let&amp;rsquo;s just say it&amp;rsquo;s been more than a bit popular :-)
ZFS and it&amp;rsquo;s friends have been a long time favorite of the HPC community, so we asked Delwin Olivan, the Snr Product Manager for the service to come and show us how easy it is to get big scale, and big performance with very little effort.
Auto-backups and one-click snapshots are exactly as they&amp;rsquo;re described on the lid. Which is awesome.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Choosing between AWS Batch or AWS ParallelCluster for your HPC Workloads</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/choosing-between-aws-batch-or-aws-parallelcluster-for-your-hpc-workloads.html</link>
      <pubDate>Thu, 10 Mar 2022 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/choosing-between-aws-batch-or-aws-parallelcluster-for-your-hpc-workloads.html</guid>
      <description>It’s an understatement that AWS has a lot of services (more than 200 at the time of this post!). We’re usually the first to point out that there’s more than one way to solve a problem. HPC is no different in this regard, because we offer a choice: customers can run their HPC workloads using AWS […]
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Scaling CFD a lot by breaking down a workflow, to speed things up</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/scaling-cfd-a-lot-by-breaking-down-a-workflow-to-speed-things-up.html</link>
      <pubDate>Thu, 03 Mar 2022 17:26:49 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/scaling-cfd-a-lot-by-breaking-down-a-workflow-to-speed-things-up.html</guid>
      <description>Sometimes when we&amp;rsquo;re looking for performance we lose the forrest for all those trees - we miss the huge improvements we can do to unglamorous parts of the overall workflow, while we obsess on the pieces that look hard. It&amp;rsquo;s an engineer thing, I think.
In today&amp;rsquo;s Tech Short, Neil Ashton shows us exactly this kind of example from the world of CFD (using OpenFOAM, but this lesson applies generally) - and shows us how to break the problem down in order to speed it all up - and pretty easily, too.
The workshops we reference in the discussion are all listed here: https://aws.amazon.com/hpc/cfd/
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Getting the best OpenFOAM Performance on AWS</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/getting-the-best-openfoam-performance-on-aws.html</link>
      <pubDate>Wed, 02 Mar 2022 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/getting-the-best-openfoam-performance-on-aws.html</guid>
      <description>OpenFOAM is one the most widely used Computational Fluid Dynamics (CFD) packages and helps companies in a broad range of sectors (automotive, aerospace, energy, and life-sciences) to conduct research and design new products. In this post, we’ll discuss six practical things you can do as an OpenFOAM user to run your simulations faster and more cost effectively.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Cloud-native, high throughput grid computing using the AWS HTC-Grid solution</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/cloud-native-high-throughput-grid-computing-using-the-aws-htc-grid-solution.html</link>
      <pubDate>Mon, 28 Feb 2022 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/cloud-native-high-throughput-grid-computing-using-the-aws-htc-grid-solution.html</guid>
      <description>We worked with our financial services customers to develop an open-source, scalable, cloud-native, high throughput computing solution on AWS — AWS HTC-Grid. HTC-Grid allows you to submit large volumes of short and long running tasks and scale environments dynamically. In this first blog of a two-part series, we describe the structure of HTC-Grid and its objective to provide a configurable blueprint for HPC grid scheduling on the cloud.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Using Lustre to build very fast file systems with Amazon Fsx for Lustre</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/using-lustre-to-build-very-fast-file-systems-with-amazon-fsx-for-lustre.html</link>
      <pubDate>Thu, 24 Feb 2022 15:01:26 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/using-lustre-to-build-very-fast-file-systems-with-amazon-fsx-for-lustre.html</guid>
      <description>The Amazon Fsx for Lustre team have managed to turn what used to be a multi-year process to buy and build a Lustre filesystem, into a simple process with a launch button on the front.
It&amp;rsquo;s hard to overstate the pain that this saves, but the most interesting things to ponder are the consequences of it. Jordan Dolman from the FSx Lustre team shows us some of the more unusual things you can do with Fsx Lustre that you just couldn&amp;rsquo;t think about doing any other way.
Storage is one of the foundational layers we covered in the AWS HPC Speeds&amp;rsquo;n&amp;rsquo;Feeds event in early Feb 2022. if you missed it, head over to https://hpc.news/SpeedsFeeds to watch the replay.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Optimize your Monte Carlo simulations using AWS Batch</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/optimize-your-monte-carlo-simulations-using-aws-batch.html</link>
      <pubDate>Wed, 23 Feb 2022 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/optimize-your-monte-carlo-simulations-using-aws-batch.html</guid>
      <description>Introduction Monte Carlo methods are a class of methods based on the idea of sampling to study mathematical problems for which analytical solutions may be unavailable. The basic idea is to create samples through repeated simulations that can be used to derive approximations about a quantity we’re interested in, and its probability distribution. In this […]
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Integrating OKTA identity service provider with NICE EnginFrame</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/integrating-okta-identity-service-provider-with-nice-enginframe.html</link>
      <pubDate>Thu, 17 Feb 2022 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/integrating-okta-identity-service-provider-with-nice-enginframe.html</guid>
      <description>This post by Roberto Meda and Salvo Maccarone covers how you can configure NICE EnginFrame to leverage OKTA as an identity service provider to support SAML 2.0 single sign on authentication and several other features like multi-factor verification, API access management and multi-device support.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>A new GUI for building and managing clusters - PCluster Manager</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/a-new-gui-for-building-and-managing-clusters-pcluster-manager.html</link>
      <pubDate>Wed, 16 Feb 2022 15:00:39 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/a-new-gui-for-building-and-managing-clusters-pcluster-manager.html</guid>
      <description>Charlie Gruenweld, our newest Principal Engineer in the HPC org, took advantage of the new API in ParallelCluster and a few dozen years of web development revolutions to build a snappy UI for creating and managing clusters that makes it nearly impossible to go wrong - and gives you immediate access to some of ParallelCluster&amp;rsquo;s most powerful features.
In less than 20 minutes, you&amp;rsquo;ll grok how to create a cluster, run jobs, spawn elastic (disposable) nodes, and jump into a full graphical desktop to run visual applications on the cluster itself.
This is the project that brings it all together. I hope you enjoy this as much as I did.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>GROMACS performance on Amazon EC2 with Intel Ice Lake processors</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/gromacs-performance-on-amazon-ec2-with-intel-ice-lake-processors.html</link>
      <pubDate>Wed, 16 Feb 2022 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/gromacs-performance-on-amazon-ec2-with-intel-ice-lake-processors.html</guid>
      <description>We recently launched two new Amazon EC2 instance families based on Intel’s Ice Lake – the C6i and M6i. These instances provide higher core counts and take advantage of generational performance improvements on Intel’s Xeon scalable processor family architectures. In this post we show how GROMACS performs on these new instance families. We use similar methodologies as for previous posts where we characterized price-performance for CPU-only and GPU instances (Part 1, Part 2, Part 3), providing instance recommendations for different workload sizes.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Introducing AWS ParallelCluster multiuser support via Active Directory</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/introducing-aws-parallelcluster-multiuser-support-via-active-directory.html</link>
      <pubDate>Thu, 10 Feb 2022 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/introducing-aws-parallelcluster-multiuser-support-via-active-directory.html</guid>
      <description>Today we’re announcing the release of AWS ParallelCluster 3.1 which now supports multiuser authentication based on Active Directory (AD). Starting with v3.1.1 clusters can be configured to use an AD domain managed via one of the AWS Directory Service options like Simple AD or AWS Managed Microsoft AD (MSAD). This blog post describes the new feature, and gives an example of a configuration block for ParallelCluster 3 configuration files.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>How to Arm a world-leading forecast model with AWS Graviton and Lambda</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/how-to-arm-a-world-leading-forecast-model-with-aws-graviton-and-lambda.html</link>
      <pubDate>Wed, 02 Feb 2022 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/how-to-arm-a-world-leading-forecast-model-with-aws-graviton-and-lambda.html</guid>
      <description>The Met Office is the UK’s National Meteorological Service, providing 24×7 world-renowned scientific excellence in weather, climate and environmental forecasts and severe weather warnings for the protection of life and property. They provide forecasts and guidance for the public, to our government and defence colleagues as well as the private sector. As an example, if you’ve been on a plane over Europe, Middle East, or Africa; that plane took off because the Met Office (as one of two World Aviation Forecast Centres) provided a forecast. This article explains one of the ways they use AWS to collect these observations, which has freed them to focus more on top quality delivery for their customers.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Join us for our HPC “Speeds n’ Feeds” event on Feb. 9</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/join-us-for-our-hpc-speeds-n-feeds-event-on-feb-9.html</link>
      <pubDate>Fri, 28 Jan 2022 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/join-us-for-our-hpc-speeds-n-feeds-event-on-feb-9.html</guid>
      <description>It’s often difficult to keep track of all the announcements AWS is making around HPC. Come and join us on Feb. 9th for a quick overview of the latest and greatest AWS HPC products and services launched over the past year. You will hear directly from the AWS HPC engineers and product managers who have built these exciting new offerings.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>How to explore Slurm&#39;s job management API from a Python notebook - (Part 5)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/how-to-explore-slurms-job-management-api-from-a-python-notebook-part-5.html</link>
      <pubDate>Thu, 27 Jan 2022 16:48:25 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/how-to-explore-slurms-job-management-api-from-a-python-notebook-part-5.html</guid>
      <description>Slurm&amp;rsquo;s job management API, coupled with ParallelCluster&amp;rsquo;s own API for managing cluster infrastructure opens up a lot of doors to create new ways for your users to interact with HPC resources - in this case without having to leave their familiar Jupyter notebook environment.
We made this video as part of the launch of ParallelCluster 3 - we want to make it easy to migrate all your workflows from to Slurm, and we figured it would be easier if you heard abotu it from the source.
So we enlisted the help of SchedMD&amp;rsquo;s Director of Cloud Engineering, Nick Ihli to help us show you how it&amp;rsquo;s a lot easier than it might look.
Joining Nick today is Josiah Bjorgaard from the AWS APN Solution Architecture team, who regularly works with Nick and the team from SchedMD to solve lots of customer problems.</description>
    </item>
    
    <item>
      <title>ParallelCluster 3 - Launch and Cluster Operations</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/parallelcluster-3-launch-and-cluster-operations.html</link>
      <pubDate>Thu, 20 Jan 2022 15:11:08 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/parallelcluster-3-launch-and-cluster-operations.html</guid>
      <description>In a previous show (https://youtu.be/6gAwAK5IJ2w), we talked about &amp;lsquo;Infrastructure as Code&amp;rsquo;, or how the ParallelCluster 3 YAML config translated to an actual cluster running, ready to take jobs.
Today, Austin Cherian joins us again to walk through standing up the cluster using that config, and how we can adjust the cluster on the fly if, for example, we needed to add some new node types into the compute fleets, like if we need some fat memory nodes, or a new GPU type.
The reference guide for doing dynamic updates is here: https://docs.aws.amazon.com/parallelcluster/latest/ug/using-pcluster-update-cluster-v3.html
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Using the ParallelCluster 3 Configuration Converter</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/using-the-parallelcluster-3-configuration-converter.html</link>
      <pubDate>Thu, 20 Jan 2022 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/using-the-parallelcluster-3-configuration-converter.html</guid>
      <description>ParallelCluster 3 was a major release with several changes and a lot of new features. To help get you started migrating your clusters, we describe the config file converter tool which is part of the ParallelCluster (&amp;gt;= v3.0.1) command line interface (CLI).
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Using Spot Instances with AWS ParallelCluster and Amazon FSx for Lustre</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/using-spot-instances-with-aws-parallelcluster-and-amazon-fsx-for-lustre.html</link>
      <pubDate>Wed, 19 Jan 2022 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/using-spot-instances-with-aws-parallelcluster-and-amazon-fsx-for-lustre.html</guid>
      <description>Processing large amounts of complex data often requires leveraging a mix of different Amazon EC2 instance types. These types of computations also benefit from shared, high performance, scalable storage like Amazon FSx for Lustre. A way to save costs on your analysis is to use Amazon EC2 Spot Instances, which can help to reduce EC2 costs up to 90% compared to On-Demand Instance pricing. This post will guide you in the creation of a fault-tolerant cluster using AWS ParallelCluster. We will explain how to configure ParallelCluster to automatically unmount the Amazon FSx for Lustre filesystem and resubmit the interrupted jobs back into the queue in the case of Spot interruption events.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>HPC on AWS just got faster and lower-cost with the launch of Hpc6a</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/hpc-on-aws-just-got-faster-and-lower-cost-with-the-launch-of-hpc6a.html</link>
      <pubDate>Thu, 13 Jan 2022 16:00:31 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/hpc-on-aws-just-got-faster-and-lower-cost-with-the-launch-of-hpc6a.html</guid>
      <description>On Jan 10, the new Hpc6a instance became generally available in two regions. This instance is built from the ground up for HPC and comes with a number of interesting innovations. Neil and Heidi from our application performance team came along to show results from real analyses of real codes running at tens of thousands of cores, with some of the hardest applications on Earth.
The instance details are here: https://aws.amazon.com/ec2/instance-types/hpc6/
&amp;hellip; and the blog post talking about them is here: https://hpc.news/Hpc6aLaunch
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Custom AMIs with ParallelCluster 3</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/custom-amis-with-parallelcluster-3.html</link>
      <pubDate>Thu, 13 Jan 2022 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/custom-amis-with-parallelcluster-3.html</guid>
      <description>This blog post shows how you can create and manage custom AMI images for AWS ParallelCluster 3 using the new AMI creation and management process, which is built using EC2 Image Builder.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Make your HPC users highly productive using EnginFrame with AWS HPC Connector</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/make-your-hpc-users-highly-productive-using-enginframe-with-aws-hpc-connector.html</link>
      <pubDate>Tue, 11 Jan 2022 15:53:44 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/make-your-hpc-users-highly-productive-using-enginframe-with-aws-hpc-connector.html</guid>
      <description>In December, we introduced EnginFrame with AWS HPC Connector, which is a new feature in EnginFrame to help customers leverage managed HPC resources in AWS as well as their on-prem systems - that means a single interface so administrators can make easy to use workflows available no matter where they live.
It means highly specialized users like scientists and engineers can use EnginFrame’s portal interface to run their important workflows without having to understand the detailed operation of infrastructure underneath. HPC is, after all, a tool used by humans. Their productivity is the real measure of success, and we think AWS HPC Connector will make a big difference to them.
In this talk, Fabrizio Chignoli (who leads the EnginFrame dev team) shows us how it all works and what the end user experience looks like.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Running Windows HPC Workloads using HPC Pack in AWS</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/running-windows-hpc-workloads-using-hpc-pack-in-aws.html</link>
      <pubDate>Tue, 11 Jan 2022 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/running-windows-hpc-workloads-using-hpc-pack-in-aws.html</guid>
      <description>This blog post shows you how to deploy an HPC cluster for Windows workloads. We have provided an AWS CloudFormation template that automates the creation process to deploy an HPC Pack 2019 Windows cluster. This will help you get started quickly to run Windows-based HPC workloads, while leveraging highly scalable, resilient, and secure AWS infrastructure. As an example, we show how to run a sample parametric sweep for EnergyPlus, an open source energy simulation tool maintained by the U.S. Department of Energy’s Building Technology Office.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Accelerating drug discovery with Amazon EC2 Spot Instances</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/accelerating-drug-discovery-with-amazon-ec2-spot-instances.html</link>
      <pubDate>Wed, 05 Jan 2022 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/accelerating-drug-discovery-with-amazon-ec2-spot-instances.html</guid>
      <description>We have been working with a team of researchers at the Max Planck Institute, helping them adopt the AWS cloud for drug research applications in the pharmaceutical industry. In this post, we’ll focus on how the team at Max Planck obtained thousands of EC2 Spot Instances spread across multiple AWS Regions for running their compute intensive simulations in a cost-effective manner, and how their solution will be enhanced further using the new Spot Placement Score API.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Recap on all the HPC developments from re:invent 21</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/recap-on-all-the-hpc-developments-from-reinvent-21.html</link>
      <pubDate>Tue, 14 Dec 2021 16:00:02 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/recap-on-all-the-hpc-developments-from-reinvent-21.html</guid>
      <description>reInvent &amp;lsquo;21 is over, but it was a busy week with new tech developments we&amp;rsquo;ve built for customers in pretty much every area of HPC: storage, networking and compute.
We talk with our HPC General Manager, Ian Colle, about ZFS, Lustre, Graviton3, DDR5, EFA at 400 Gb/s on dozens of instances &amp;hellip; and a lot more.
If you didn&amp;rsquo;t get to reinvent, or even if you did and couldn&amp;rsquo;t be in 10 places at once, this is a compact recap for you.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>What supercomputers, scientists and TV stations have in common</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/what-supercomputers-scientists-and-tv-stations-have-in-common.html</link>
      <pubDate>Thu, 09 Dec 2021 16:19:43 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/what-supercomputers-scientists-and-tv-stations-have-in-common.html</guid>
      <description>CDI (Cloud Digital Interface) uses EFA to blast broadcast quality video, uncompressed, from instance to instance in the cloud as live video feeds its way through complex stacks of software and makes its way to your TV set.
It&amp;rsquo;s one of the more unusual use cases we&amp;rsquo;ve heard for a technology that we invented for keeping highly parallel scientific applications in sync when running in HPC environments in the cloud. Yes, that&amp;rsquo;s weird.
Evan Statton from our media and entertainment tech team talks about how this happened and why it&amp;rsquo;s so useful.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Introducing AWS HPC Connector for NICE EnginFrame</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/introducing-aws-hpc-connector-for-nice-enginframe.html</link>
      <pubDate>Mon, 06 Dec 2021 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/introducing-aws-hpc-connector-for-nice-enginframe.html</guid>
      <description>Today we’re introducing AWS HPC Connector, a new feature in NICE EnginFrame that allows customers to leverage managed HPC resources on AWS. With this release, EnginFrame provides a unified interface for administrators to make hybrid HPC resources available to their users both on-premises and within AWS. In this post, we’ll provide some context around EnginFrame’s typical use cases, and show how you can use AWS HPC Connector to stand up HPC compute resources on AWS.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>miniWDL workflows with 100% cloud elasticity, and no DevOps geekery</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/miniwdl-workflows-with-100-cloud-elasticity-and-no-devops-geekery.html</link>
      <pubDate>Thu, 02 Dec 2021 16:42:11 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/miniwdl-workflows-with-100-cloud-elasticity-and-no-devops-geekery.html</guid>
      <description>Mike Lin is the creator and maintainer of miniWDL and has been working with our Genomics and ML teams to integrate it with Sagemaker Studio and AWS Batch. The result is a working environment that&amp;rsquo;s completely familiar and easy to use (it&amp;rsquo;s Jupyter, after all) and yet has access to all the elasticity of the cloud, but without ever needing to leave the familiar notebook interface to make that magic happen.
It&amp;rsquo;s a really cool example of bringing the tools to science, rather than forcing the scientists to understand yet another new tool.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Fair share scheduling to maximize user happiness in AWS Batch</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/fair-share-scheduling-to-maximize-user-happiness-in-aws-batch.html</link>
      <pubDate>Wed, 24 Nov 2021 17:55:32 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/fair-share-scheduling-to-maximize-user-happiness-in-aws-batch.html</guid>
      <description>If you’re a frequent user of Batch queues, you have opinions about whether it’s scheduling is “fair” on your users.
But &amp;lsquo;fair&amp;rsquo; means different things to everyone - what’s needed are some levers and dials that allow you to figure out what ‘fair’ means for your organization.
AWS Batch now has Fair Share Scheduling, and it comes with a whole lot of controls.
We’re joined this week on Tech Shorts by Aswin Damodar from the Batch engineering team, and Christian Kniep, our Snr Dev Advocate for HPC &amp;amp; Batch, to run through all these controls and what they mean.
I’m betting this video is going to get watched a LOT. Fair-share scheduling is a complex topic, but the discussion makes it a lot easier to understand that reading the documentation probably will the first time.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>How we enabled uncompressed live video with CDI over EFA</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/how-we-enabled-uncompressed-live-video-with-cdi-over-efa.html</link>
      <pubDate>Wed, 24 Nov 2021 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/how-we-enabled-uncompressed-live-video-with-cdi-over-efa.html</guid>
      <description>We’re going to take you into the world of broadcast video, and explain how it led to us announcing today the general availability of EFA on smaller instance sizes. For a range of applications, this is going to save customers a lot of money because they no longer need to use the biggest instances in each instance family to get HPC-style network performance. But the story of how we got there involves our Elastic Fabric Adapter (EFA), some difficult problems presented to us by customers in the entertainment industry, and an invention called the Cloud Digital Interface (CDI). And it started not very far from Hollywood.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Benchmarking the NVIDIA Clara Parabricks germline pipeline on AWS</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/benchmarking-the-nvidia-clara-parabricks-germline-pipeline-on-aws.html</link>
      <pubDate>Tue, 23 Nov 2021 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/benchmarking-the-nvidia-clara-parabricks-germline-pipeline-on-aws.html</guid>
      <description>This blog provides an overview of NVIDIA’s Clara Parabricks along with a guide on how to use Parabricks within the AWS Marketplace. It focuses on germline analysis for whole genome and whole exome applications using GPU accelerated bwa-mem and GATK’s HaplotypeCaller.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Running a 3.2M vCPU HPC Workload on AWS with YellowDog</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/running-a-32m-vcpu-hpc-workload-on-aws-with-yellowdog.html</link>
      <pubDate>Tue, 23 Nov 2021 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/running-a-32m-vcpu-hpc-workload-on-aws-with-yellowdog.html</guid>
      <description>OMass Therapeutics, a biotechnology company identifying medicines against highly validated target ecosystems, used Yellowdog on AWS to analyze and screen 337 million compounds in 7 hours, a task which would have taken two months using an on-premises HPC cluster. YellowDog, based in Bristol in the UK, ran the drug discovery application on an extremely large, multi-region cluster in AWS with the AWS ‘pay-as-you-go’ pricing model. It provided a central, unified interface to monitor and manage AWS Region selection, compute provisioning, job allocation and execution. The entire workload completed in 65 minutes, enabling scientists to start work on analysis the same day, significantly accelerating the drug discovery process. In this post, we’ll discuss the AWS and YellowDog services we deployed, and the mechanisms used to scale to 3.2m vCPUs using multiple EC2 instance types across multiple regions in 33 minutes, running at a 95% utilization rate.</description>
    </item>
    
    <item>
      <title>Easy migration from SGE to Slurm - Part 4 Slurm Accounting</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/easy-migration-from-sge-to-slurm-part-4-slurm-accounting.html</link>
      <pubDate>Thu, 18 Nov 2021 11:31:59 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/easy-migration-from-sge-to-slurm-part-4-slurm-accounting.html</guid>
      <description>As part of the launch of ParallelCluster 3, we want to make it easy to migrate all your workflows from to Slurm, but we know that it can seem daunting. So we enlisted the help of SchedMD&amp;rsquo;s Director of Cloud Engineering, Nick Ihli to help us show you how it&amp;rsquo;s a lot easier than it might look.
(Seriously, you may need to change very little).
Over the next 5 x Tech Shorts we&amp;rsquo;ll show:
Part 1) Command line syntax and which Slurm commands map to familiar SGE ones. Part 2) Job Scripts - what&amp;rsquo;s the stuff you need to care about to adjust ytour scripts? What&amp;rsquo;s actually easier with Slurm? Part 3) Array Jobs - Slurm has a really elegant way to handle array jobs, and also has some really nice SGE-like commands that will fool you into thinking nothing actually changed.</description>
    </item>
    
    <item>
      <title>Coming soon: dedicated HPC instances and hybrid functionality</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/coming-soon-dedicated-hpc-instances-and-hybrid-functionality.html</link>
      <pubDate>Thu, 18 Nov 2021 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/coming-soon-dedicated-hpc-instances-and-hybrid-functionality.html</guid>
      <description>This year, we’ve launched a lot of new capabilities for HPC customers, making AWS the best place for the length and breadth of their workflows. EFA went mainstream and is now available in sixteen instance families for fast fabric capabilities for scaling MPI and NCCL codes. We’ve written deep-dive studies to explore and explain the optimizations that will drive your workloads faster in the cloud than elsewhere. We released a major new version of AWS ParallelCluster with its own API for controlling the cluster lifecycle. AWS Batch became deeply integrated into AWS Step Functions and now supports fair-share scheduling, with multiple levers to control the experience. Today we’re signaling the arrival of a new HPC-dedicated instance family – the Hpc6a – and an enhanced EnginFrame that will bring the best of the cloud and on-premises together in a single interface.</description>
    </item>
    
    <item>
      <title>How to manage HPC jobs using a serverless API</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/how-to-manage-hpc-jobs-using-a-serverless-api.html</link>
      <pubDate>Wed, 17 Nov 2021 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/how-to-manage-hpc-jobs-using-a-serverless-api.html</guid>
      <description>HPC systems are traditionally access through a Command Line Interface (CLI) where the users submit and manage their computational jobs. Depending on their experience and sophistication, the CLI can be a daunting experience for users not accustomed in using it. Fortunately, the cloud offers many other options for users to submit and manage their computational jobs. In this blog post we will cover how to create a serverless API to interact with an HPC system in the the cloud built with AWS ParallelCluster.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Using the Slurm REST API to integrate with distributed architectures on AWS</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/using-the-slurm-rest-api-to-integrate-with-distributed-architectures-on-aws.html</link>
      <pubDate>Wed, 17 Nov 2021 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/using-the-slurm-rest-api-to-integrate-with-distributed-architectures-on-aws.html</guid>
      <description>The Slurm Workload Manager by SchedMD is a popular HPC scheduler and is supported by AWS ParallelCluster, an elastic HPC cluster management service offered by AWS. Traditional HPC workflows involve logging into a head node and running shell commands to submit jobs to a scheduler and check job status. Modern distributed systems often use representational […]
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Deep dive into the AWS ParallelCluster 3 configuration file</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/deep-dive-into-the-aws-parallelcluster-3-configuration-file.html</link>
      <pubDate>Mon, 15 Nov 2021 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/deep-dive-into-the-aws-parallelcluster-3-configuration-file.html</guid>
      <description>In September, we announced the release of AWS ParallelCluster 3, a major release with lots of changes and new features. To help get you started migrating your clusters, we provided the Moving from AWS ParallelCluster 2.x to 3.x guide. We know moving versions can be a quite an undertaking, so we’re augmenting that official documentation with additional color and context on a few key areas. With this blog post, we’ll focus on the configuration file format changes for ParallelCluster 3, and how they map back to the same configuration sections for ParallelCluster 2.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Easy migration from SGE to Slurm - Part 3 - Array Jobs</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/easy-migration-from-sge-to-slurm-part-3-array-jobs.html</link>
      <pubDate>Thu, 11 Nov 2021 17:54:45 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/easy-migration-from-sge-to-slurm-part-3-array-jobs.html</guid>
      <description>As part of the launch of ParallelCluster 3, we want to make it easy to migrate all your workflows from to Slurm, but we know that it can seem daunting. So we enlisted the help of SchedMD&amp;rsquo;s Director of Cloud Engineering, Nick Ihli to help us show you how it&amp;rsquo;s a lot easier than it might look.
(Seriously, you may need to change very little).
Over the next 5 x Tech Shorts we&amp;rsquo;ll show:
Part 1) Command line syntax and which Slurm commands map to familiar SGE ones. Part 2) Job Scripts - what&amp;rsquo;s the stuff you need to care about to adjust ytour scripts? What&amp;rsquo;s actually easier with Slurm? Part 3) Array Jobs - Slurm has a really elegant way to handle array jobs, and also has some really nice SGE-like commands that will fool you into thinking nothing actually changed.</description>
    </item>
    
    <item>
      <title>EFA is now mainstream, and that’s a Good Thing</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/efa-is-now-mainstream-and-thats-a-good-thing.html</link>
      <pubDate>Thu, 11 Nov 2021 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/efa-is-now-mainstream-and-thats-a-good-thing.html</guid>
      <description>We have recently launched three new Amazon EC2 instances types enabled with Elastic Fabric Adapter (EFA), our network interface for Amazon EC2 instances that enables customers to run applications requiring high levels of inter-node communications at scale on AWS. These bring our EFA-enabled count to sixteen different instance families covering a wide range of use cases. EFA is going mainstream and we are just getting started.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Easy migration from SGE to Slurm - Part 2 - Job Scripts</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/easy-migration-from-sge-to-slurm-part-2-job-scripts.html</link>
      <pubDate>Tue, 09 Nov 2021 16:34:10 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/easy-migration-from-sge-to-slurm-part-2-job-scripts.html</guid>
      <description>As part of the launch of ParallelCluster 3, we want to make it easy to migrate all your workflows from to Slurm, but we know that it can seem daunting. So we enlisted the help of SchedMD&amp;rsquo;s Director of Cloud Engineering, Nick Ihli to help us show you how it&amp;rsquo;s a lot easier than it might look.
(Seriously, you may need to change very little).
Over the next 5 x Tech Shorts we&amp;rsquo;ll show:
Part 1) Command line syntax and which Slurm commands map to familiar SGE ones. Part 2) Job Scripts - what&amp;rsquo;s the stuff you need to care about to adjust ytour scripts? What&amp;rsquo;s actually easier with Slurm? Part 3) Array Jobs - Slurm has a really elegant way to handle array jobs, and also has some really nice SGE-like commands that will fool you into thinking nothing actually changed.</description>
    </item>
    
    <item>
      <title>Introducing fair-share scheduling for AWS Batch</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/introducing-fair-share-scheduling-for-aws-batch.html</link>
      <pubDate>Tue, 09 Nov 2021 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/introducing-fair-share-scheduling-for-aws-batch.html</guid>
      <description>Today we are announcing fair-share scheduling (FSS) for AWS Batch, which provides fine-grain control of the scheduling behavior by using a scheduling policy. With FSS, customers can prevent “unfair” situations caused by strict first-in, first-out scheduling where high priority jobs can’t “jump the queue” without draining other jobs first. You can now balance resource consumption between groups of workloads and have confidence that the shared compute environment is not dominated by a single workload. In this post, we’ll explain how fair-share scheduling works in more detail. You’ll also find a link to a step-by-step workshop at the end of this post, so you can try it out yourself.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Easy migrating from SGE to Slurm - Part 1 - Command Line Tools</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/easy-migrating-from-sge-to-slurm-part-1-command-line-tools.html</link>
      <pubDate>Thu, 04 Nov 2021 15:36:14 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/easy-migrating-from-sge-to-slurm-part-1-command-line-tools.html</guid>
      <description>As part of the launch of ParallelCluster 3, we want to make it easy to migrate all your workflows from to Slurm, but we know that it can seem daunting. So we enlisted the help of SchedMD&amp;rsquo;s Director of Cloud Engineering, Nick Ihli to help us show you how it&amp;rsquo;s a lot easier than it might look.
(Seriously, you may need to change very little).
Over the next 5 x Tech Shorts we&amp;rsquo;ll show:
Part 1) Command line syntax and which Slurm commands map to familiar SGE ones. Part 2) Job Scripts - what&amp;rsquo;s the stuff you need to care about to adjust ytour scripts? What&amp;rsquo;s actually easier with Slurm? Part 3) Array Jobs - Slurm has a really elegant way to handle array jobs, and also has some really nice SGE-like commands that will fool you into thinking nothing actually changed.</description>
    </item>
    
    <item>
      <title>Scaling a read-intensive, low-latency file system to 10M&#43; IOPs</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/scaling-a-read-intensive-low-latency-file-system-to-10m-iops.html</link>
      <pubDate>Thu, 04 Nov 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/scaling-a-read-intensive-low-latency-file-system-to-10m-iops.html</guid>
      <description>Many shared file systems are used in supporting read-intensive applications, like financial backtesting. These applications typically exploit copies of datasets whose authoritative copy resides somewhere else. For small datasets, in-memory databases and caching techniques can yield impressive results. However, low latency flash-based scalable shared file systems can provide both massive IOPs and bandwidth. They’re also easy to adopt because of their use of a file-level abstraction. In this post, I’ll share how to easily create and scale a shared, distributed POSIX compatible file system that performs at local NVMe speeds for files opened read-only.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>CFD performance on Ice Lake CPU with the Amazon EC2 C6i (Part 2)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/cfd-performance-on-ice-lake-cpu-with-the-amazon-ec2-c6i-part-2.html</link>
      <pubDate>Tue, 02 Nov 2021 14:00:09 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/cfd-performance-on-ice-lake-cpu-with-the-amazon-ec2-c6i-part-2.html</guid>
      <description>Today we launched the new Amazon EC2 C6i instance family which is powered by the Intel Xeon Ice Lake processor and comes equipped with our Elastic Fabric Adapter.
Neil Ashton and Nicola Venuti joined us to talk about CFD performance on this new instance family, and spent some time comparing network connectivity options, too.
This is a two part series of Tech Shorts:
Part 1) Discuss C6i and how it&amp;rsquo;s put together. Look at OpenFOAM and Siemens Simcenter StarCCM+ Part 2) Ansys Fluent, under populated cores, and some on-premises comparisons for calibration.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Running 20k simulations in 3 days to accelerate early stage drug discovery with AWS Batch</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/running-20k-simulations-in-3-days-to-accelerate-early-stage-drug-discovery-with-aws-batch.html</link>
      <pubDate>Tue, 02 Nov 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/running-20k-simulations-in-3-days-to-accelerate-early-stage-drug-discovery-with-aws-batch.html</guid>
      <description>In this blog post, we’ll describe an ensemble run of 20K simulations to accelerate the drug discovery process, while also optimizing for run time and cost. We used two popular open-source packages — GROMACS, which does a molecular dynamics simulations, and pmx, a free-energy calculation package from the Computational Biomolecular Dynamics Group at Max Planck Institute in Germany.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>CFD performance on Ice Lake CPU with the Amazon EC2 C6i (Part 1)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/cfd-performance-on-ice-lake-cpu-with-the-amazon-ec2-c6i-part-1.html</link>
      <pubDate>Fri, 29 Oct 2021 10:48:01 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/cfd-performance-on-ice-lake-cpu-with-the-amazon-ec2-c6i-part-1.html</guid>
      <description>Today we launched the new Amazon EC2 C6i instance family which is powered by the Intel Xeon Ice Lake processor and comes equipped with our Elastic Fabric Adapter.
Neil Ashton and Nicola Venuti joined us to talk about CFD performance on this new instance family, and spent some time comparing network connectivity options, too.
This is a two part series of Tech Shorts:
Part 1) Discuss C6i and how it&amp;rsquo;s put together. Look at OpenFOAM and Siemens Simcenter StarCCM+ Part 2) Ansys Fluent, under populated cores, and some on-premises comparisons for calibration.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Using AWS Batch Console Support for Step Functions Workflows</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/using-aws-batch-console-support-for-step-functions-workflows.html</link>
      <pubDate>Wed, 27 Oct 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/using-aws-batch-console-support-for-step-functions-workflows.html</guid>
      <description>Last year, we published the Genomics Secondary Analysis Using AWS Step Functions and AWS Batch solution as a companion solution to the Genomics Data Transfer, Analytics, and Machine Learning Using AWS Services whitepaper. Since then, many customers have used the secondary analysis solution to automate their bioinformatics pipelines in AWS. A common pain point expressed […]
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>How to deconstruct FSI Grid scheduling and mapping it onto AWS services</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/how-to-deconstruct-fsi-grid-scheduling-and-mapping-it-onto-aws-services.html</link>
      <pubDate>Thu, 21 Oct 2021 16:05:26 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/how-to-deconstruct-fsi-grid-scheduling-and-mapping-it-onto-aws-services.html</guid>
      <description>Last month we talked about the evolution of HPC workloads in the financial world. We agreed with Alex Kimber, our global FSI HPC expert, to come back and talk about how to deconstruct these very complex, high-speed, task scheduling environments and map them onto AWS services. This is cool because it lets us scale them even further in a bunch of new ways. it also means we can solve some tricky organizational problems (like sharing infra and cross-charging between depts) without any fuss at all.
Alex dives into this area in today&amp;rsquo;s discussion.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>The Convergent Evolution of Grid Computing in Financial Services</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/the-convergent-evolution-of-grid-computing-in-financial-services.html</link>
      <pubDate>Thu, 21 Oct 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/the-convergent-evolution-of-grid-computing-in-financial-services.html</guid>
      <description>The Financial Services industry makes significant use of high performance computing (HPC) but it tends to be in the form of loosely coupled, embarrassingly parallel workloads to support risk modelling. The infrastructure tends to scale out to meet ever increasing demand as the analyses look at more and finer grained data. At AWS we’ve helped many customers tackle scaling challenges are noticing some common themes. In this post we describe how HPC teams are thinking about how they deliver compute capacity today, and highlight how we see the solutions converging for the future.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Putting bitrates into perspective</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/putting-bitrates-into-perspective.html</link>
      <pubDate>Tue, 19 Oct 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/putting-bitrates-into-perspective.html</guid>
      <description>Recently, we talked about the advances NICE DCV has made to push pixels from cloud-hosted desktops or applications over the internet even more efficiently than before. Since we published that post on this blog channel, we’ve been asked by several customers whether all this efficient pixel-pushing could lead to outbound data charges moving up on their AWS bill. We decided to try it on your behalf, and share the details with you in this post. The bottom line? The charges are unlikely to be significant unless you’re doing intensive streaming (such as gaming) and other cost optimizations (like AWS Instance Savings Plans) that will have more impact on your bill.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Getting started with bioinformatics on AWS with Swaine Chen from GIS</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/getting-started-with-bioinformatics-on-aws-with-swaine-chen-from-gis.html</link>
      <pubDate>Fri, 15 Oct 2021 13:01:30 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/getting-started-with-bioinformatics-on-aws-with-swaine-chen-from-gis.html</guid>
      <description>Swaine Chen had a problem: his new group at the Genomics Institute of Singapore needed common tooling involving zillions of utilities, scripts carrying algorithms and techniques, so they could migrate their workloads from their on-premises infrastructure and over to the cloud.
His solution was to build a golden AMI. So much better is that he automated the production of the AMI, and documented it in GitHub, for the world to see, and for the world to use.
Today&amp;rsquo;s discussion shows just a glimpse of what his team has built, and you can get all the goodness in the hands-on training workshops he&amp;rsquo;s offering (for free) to anyone in the world who wants to sign up.
If you want to know more about the workshops, or sign up, go here: https://hpc.news/giswsregister
If you want to plunder all this goodness and use it for the forces of good (not evil!</description>
    </item>
    
    <item>
      <title>Running GROMACS on GPU instances: multi-node price-performance</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/running-gromacs-on-gpu-instances-multi-node-price-performance.html</link>
      <pubDate>Thu, 14 Oct 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/running-gromacs-on-gpu-instances-multi-node-price-performance.html</guid>
      <description>This three-part series of posts cover the price performance characteristics of running GROMACS on Amazon Elastic Compute Cloud (Amazon EC2) GPU instances. Part 1 covered some background no GROMACS and how it utilizes GPUs for acceleration. Part 2 covered the price performance of GROMACS on a particular GPU instance family running on a single instance. […]
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Running GROMACS on GPU instances: single-node price-performance</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/running-gromacs-on-gpu-instances-single-node-price-performance.html</link>
      <pubDate>Wed, 13 Oct 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/running-gromacs-on-gpu-instances-single-node-price-performance.html</guid>
      <description>This three-part series of posts cover the price performance characteristics of running GROMACS on Amazon Elastic Compute Cloud (Amazon EC2) GPU instances. Part 1 covered some background no GROMACS and how it utilizes GPUs for acceleration. This post (Part 2) covers the price performance of GROMACS on a particular GPU instance family running on a […]
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Running GROMACS on GPU instances</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/running-gromacs-on-gpu-instances.html</link>
      <pubDate>Tue, 12 Oct 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/running-gromacs-on-gpu-instances.html</guid>
      <description>Comparing the performance of real applications across different Amazon Elastic Compute Cloud (Amazon EC2) instance types is the best way we’ve found for finding optimal configurations for HPC applications here at AWS. Previously, we wrote about price-performance optimizations for GROMACS that showed how the GROMACS molecular dynamics simulation runs on single instances, and how it […]
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Infrastructure as Code - ParallelCluster 3&#39;s config</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/infrastructure-as-code-parallelcluster-3s-config.html</link>
      <pubDate>Thu, 07 Oct 2021 13:56:05 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/infrastructure-as-code-parallelcluster-3s-config.html</guid>
      <description>&amp;lsquo;Infrastructure as Code&amp;rsquo; has a weird meaning in HPC, because it says we can write a script which stands up the entire incredibly complex software stack that is an HPC cluster, complete with MPIs, math libraries, schedulers - and even a parallel file system and visualization server.
Austin Cherian takes us for a walk through the new ParallelCluster 3 config file and shows how the syntax aligns with the significant elements of the cluster architecture. He also talks about a few quirks we found along the way.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>AWS Batch Dos and Don’ts: Best Practices in a Nutshell</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/aws-batch-dos-and-donts-best-practices-in-a-nutshell.html</link>
      <pubDate>Mon, 04 Oct 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/aws-batch-dos-and-donts-best-practices-in-a-nutshell.html</guid>
      <description>AWS Batch is a service that enables scientists and engineers to run computational workloads at virtually any scale without requiring them to manage a complex architecture. In this blog post, we share a set of best practices and practical guidance devised from our experience working with customers in running and optimizing their computational workloads. The readers will learn how to optimize their costs with Amazon EC2 Spot on AWS Batch, how to troubleshoot their architecture should an issue arise and how to tune their architecture and containers layout to run at scale.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Launch a machine in RONIN</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/launch-a-machine-in-ronin.html</link>
      <pubDate>Thu, 30 Sep 2021 15:40:16 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/launch-a-machine-in-ronin.html</guid>
      <description>RONIN is probably the easiest way for a research scientist or engineer to be able to quickly jump into the cloud on AWS and get stuff done. And they&amp;rsquo;ve made a regular habit of delighting their customers by making hard things easy.
In this Tech Short, Tara from RONIN shows us how to spin up a Linux machine and connect to it securely. All of this happens inside the safety and governance of RONIN&amp;rsquo;s security posture and it&amp;rsquo;s budget management guardrails.
More on all of this is available at www.ronin.cloud.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Running the Harmonie numerical weather prediction model on AWS</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/running-the-harmonie-numerical-weather-prediction-model-on-aws.html</link>
      <pubDate>Thu, 30 Sep 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/running-the-harmonie-numerical-weather-prediction-model-on-aws.html</guid>
      <description>The Danish Meteorological Institute (DMI) is responsible for running atmospheric, climate and ocean models covering the kingdom of Denmark. We worked together with the DMI to port and run a full numerical weather prediction (NWP) cycling dataflow with the Harmonie Numerical Weather Prediction (NWP) model to AWS. You can find a report of the porting and operational experience in the ACCORD community newsletter. In this blog post, we expand on that report to present the initial timing results from running the forecast component of Harmonie model on AWS. We also present these as-is timing results together with as-is timings attained on the supercomputing systems based on Cray XC40 and Intel Xeon based Cray XC50.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Cost-optimization on Spot Instances using checkpoint for Ansys LS-DYNA</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/cost-optimization-on-spot-instances-using-checkpoint-for-ansys-ls-dyna.html</link>
      <pubDate>Mon, 27 Sep 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/cost-optimization-on-spot-instances-using-checkpoint-for-ansys-ls-dyna.html</guid>
      <description>A major portion of the costs incurred for running Finite Element Analyses (FEA) workloads on AWS comes from the usage of Amazon EC2 instances. Amazon EC2 Spot Instances offer a cost-effective architectural choice, allowing you to take advantage of unused EC2 capacity for up to a 90% discount compared to On-Demand Instance prices. In this post, we describe how you 0can run fault-tolerant FEA workloads on Spot Instances using Ansys LS-DYNA’s checkpointing and auto-restart utility.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Quantum Chemistry Calculation with FHI-aims code on AWS</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/quantum-chemistry-calculation-with-fhi-aims-code-on-aws.html</link>
      <pubDate>Fri, 24 Sep 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/quantum-chemistry-calculation-with-fhi-aims-code-on-aws.html</guid>
      <description>This article was contributed by Dr. Fabio Baruffa, Sr. HPC and QC Solutions Architect at AWS, and Dr. Jesús Pérez Ríos, Group Leader at the Fritz Haber Institute, Max-Planck Society. Introduction Quantum chemistry – the study of the inherently quantum interactions between atoms forming part of molecules – is a cornerstone of modern chemistry. […]
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>HPC in Financial Services is not boring, and has some interesting problems to solve.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/hpc-in-financial-services-is-not-boring-and-has-some-interesting-problems-to-solve.html</link>
      <pubDate>Thu, 23 Sep 2021 15:53:01 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/hpc-in-financial-services-is-not-boring-and-has-some-interesting-problems-to-solve.html</guid>
      <description>HPC in Financial Services is interesting for a lot of reasons that have nothing to do with banking. It&amp;rsquo;s an embarrassingly parallel workload, typically made up from zillions of short&amp;rsquo;ish jobs (seconds to minutes). And the decisions it supports are pretty big ones: whole banks and entire economies might be impacted by the outcomes. It&amp;rsquo;s interesting because there are so many fun ways you can solve the problem of &amp;lsquo;get it done fast, cheaply, and soon enough to help the people who are making really big decisions&amp;rsquo;. And whilst you expect us to say this: the cloud truly is a real game changer for this workload.
Alex Kimber wrote the HPC book at AWS for Financial Services, but what he talks about in this short discussion puts context around the whole thing &amp;hellip; He&amp;rsquo;ll be back to expand on several of the key points in a few weeks.</description>
    </item>
    
    <item>
      <title>Virtual Screening of Novel Active Drug Compounds on AWS with Orion®</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/virtual-screening-of-novel-active-drug-compounds-on-aws-with-orion.html</link>
      <pubDate>Thu, 23 Sep 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/virtual-screening-of-novel-active-drug-compounds-on-aws-with-orion.html</guid>
      <description>Computer-aided drug discovery (CADD) has been a key player in lowering the cost and speeding up the timeline for drug development. CADD uses high performance computing (HPC) resources to virtually screen databases with billions of molecules. It can speed up the searching of potential drug molecules, and filter out molecules and compounds that are unsuitable. OpenEye Scientific developed Orion®, a cloud-based molecular design platform for CADD. Orion provides computational chemists with virtually unlimited HPC resources. These include data visualization, collaboration, and workflow management tools that help them perform calculations more efficiently. In this post, we describe the Orion architecture on AWS, and it’s capabilities to address the challenges in drug development.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Call for participation: PRACE Winter School</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/call-for-participation-prace-winter-school.html</link>
      <pubDate>Thu, 16 Sep 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/call-for-participation-prace-winter-school.html</guid>
      <description>The Inter University Computing Centre (IUCC) in Israel and AWS have joined forces to train Researchers and Research Software Engineers (RSEs) in the use of AWS for High Performance Computing (HPC) at the PRACE Winter School, 7-9 December 2021, and we’re calling for interested groups to sign up and join us.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Governance and guardrails for researchers with RONIN ISOLATE</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/governance-and-guardrails-for-researchers-with-ronin-isolate.html</link>
      <pubDate>Wed, 15 Sep 2021 15:50:26 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/governance-and-guardrails-for-researchers-with-ronin-isolate.html</guid>
      <description>RONIN (www.ronin.cloud) is one of our favorite HPC partners, because they&amp;rsquo;ve done a really comprehensive job providing research customers the governance and guardrails they need around budget management and data security to make insanely easy to explore and experiment with workloads in the cloud. It helps that they pack hundreds of DevOps tasks into every click so you can launch HPC clusters in less time than it takes to buy your groceries online.
Tara Madhyastha is one of RONIN&amp;rsquo;s Principal Research Scientists based out of Seattle, and she gives us a peek at a new product built on RONIN Core called &amp;lsquo;RONIN Isolate&amp;rsquo;, which takes secure enclaves and project isolation to an obsessive new level to help customers with really extreme (and particular) security and compliance needs.
Isolate launched this week, and you can find more about it at the RONIN Blo at https://blog.</description>
    </item>
    
    <item>
      <title>Introducing ParallelCluster 3 - HPC in the Cloud built by customers</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/introducing-parallelcluster-3-hpc-in-the-cloud-built-by-customers.html</link>
      <pubDate>Fri, 10 Sep 2021 16:19:34 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/introducing-parallelcluster-3-hpc-in-the-cloud-built-by-customers.html</guid>
      <description>Today we’re announcing AWS ParallelCluster 3.
Customers, integrators, and other builders have told us they want to build end-to-end “recipes” for HPC, spanning the whole gamut from infrastructure to middleware, libraries, and runtime codes. They also asked to interact with ParallelCluster programmatically to create interfaces and services for their users. We worked backwards from this feedback, using thousands of conversations with customers to create a shiny new version of ParallelCluster rebuilt from the ground up.
In 15 minutes, we walk through the main features with Nathan Stornetta our Snr Product Manager for ParallelCluster, who also explains what customer feedback lead to each idea.
This is a big release with loads of new features. But the most exciting part is what it sets us up for next.
You can find out more about this in our launch blog at: http://hpc.news/pc3day1 and also by checking out the workshop and documentation that are linked to in the bottom of that blog post.</description>
    </item>
    
    <item>
      <title>New: Introducing AWS ParallelCluster 3</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/new-introducing-aws-parallelcluster-3.html</link>
      <pubDate>Fri, 10 Sep 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/new-introducing-aws-parallelcluster-3.html</guid>
      <description>Running HPC workloads, like computational fluid dynamics (CFD), molecular dynamics, or weather forecasting typically involves a lot of moving parts. You need a hundreds or thousands of compute cores, a job scheduler for keeping them fed, a shared file system that’s tuned for throughput or IOPS (or both), loads of libraries, a fast network, and […]
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Supporting climate model simulations to accelerate climate science</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/supporting-climate-model-simulations-to-accelerate-climate-science.html</link>
      <pubDate>Fri, 03 Sep 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/supporting-climate-model-simulations-to-accelerate-climate-science.html</guid>
      <description>The Amazon Sustainability Data Initiative (ASDI), AWS is donating cloud resources, technical support, and access to scalable infrastructure and fast networking providing high performance computing solutions to support simulations of near-term climate using the National Center for Atmospheric Research (NCAR) Community Earth System Model Version 2 (CESM2) and its Whole Atmosphere Community Climate Model (WACCM). In collaboration with ASDI, AWS, and SilverLining, a nonprofit dedicated to ensuring a safe climate, the National Center for Atmospheric Research (NCAR) will run an ensemble of 30 climate-model simulations on AWS. The climate runs will simulate the Earth system over the period of years 2022-2070 under a median scenario for warming and make them available through the AWS Open Data Program. The simulation work will demonstrate the ability to use cloud infrastructure to advance climate models in support of robust scientific studies by researchers around the world and aims to accelerate and democratize climate science.</description>
    </item>
    
    <item>
      <title>What&#39;s the impact of DCV&#39;s pixel streaming on my AWS Bill?</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/whats-the-impact-of-dcvs-pixel-streaming-on-my-aws-bill.html</link>
      <pubDate>Thu, 02 Sep 2021 15:28:09 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/whats-the-impact-of-dcvs-pixel-streaming-on-my-aws-bill.html</guid>
      <description>Since our NICE DCV high performance desktop and application streaming product does such a great job of making it feel like you have a cloud dat center sitting behind your laptop screen somewhere, lots of customers have asked &amp;lsquo;Won&amp;rsquo;t that impact my bill? You guys charge for data, right?&amp;rsquo;.
There&amp;rsquo;s no way to give a precise answer, but what we did do was to put together a range of usage scenarios from fairly pedestrian usage (doing work processing on a remote windows machine) through to high bit-rate gaming and video streaming. We tested them all and measured their consumption. And we show you the results. Bottom line: data charges are&amp;rsquo;t really going to nudge your billing by much unless you&amp;rsquo;re getting close to 4K video streaming.
Jyothi Venkatesh and Boof put DCV to the test, and we walk you through the scenarios and our analysis.</description>
    </item>
    
    <item>
      <title>High Burst CPU Compute for Monte Carlo Simulations on AWS</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/high-burst-cpu-compute-for-monte-carlo-simulations-on-aws.html</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/high-burst-cpu-compute-for-monte-carlo-simulations-on-aws.html</guid>
      <description>Playtech mathematicians and game designers need accurate, detailed game play simulation results to create fun experiences for players. While software developers have been able to iterate on code in an agile manner for many years, for non-analytical solutions, mathematicians have had to rely on slow CPU-bound Monte-Carlo simulations, waiting, as software engineers once did, many hours or overnight to get the results of their latest changes. These statistics are also required as evidence of game fairness in the highly regulated online gaming business. Playtech has developed an AWS Lambda Serverless based solution that provides massive burst compute performance that allows game simulations in minutes rather than hours. This post goes into the details of the architecture, as well as some examples of using the system in our development and operations.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>How to Spack a software package</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/how-to-spack-a-software-package.html</link>
      <pubDate>Thu, 26 Aug 2021 17:26:42 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/how-to-spack-a-software-package.html</guid>
      <description>This is a talk that was delivered at the AWS/Arm Cloud Hackathon in July 2021.
Todd Gamblin (the creator of Spack) and his colleague Greg Becker talk us through the essential skills and concepts needed to understand how to create and deploy Spack recipes to build scientific codes. Spack massively simplifies the task of building scientific applications, which are almost defined by their insane build methods and dependency hierarchies. We made extensive use of Spack in the hackathon, and were extremely grateful for their help.
The Summer Hackathon ran for a week from July 12-16 in 2021. It&amp;rsquo;s purpose was to assemble the HPC community around a common goal of beginning the porting/testing/tuning process for dozens of codes to use Arm-based processors, in this case, AWS&amp;rsquo;s Graviton2&amp;rsquo;s.
During the week we had a series of talks from global leaders and experts in various fields from compilers to networking, profiling debugging and tooling.</description>
    </item>
    
    <item>
      <title>Stion – a Software as a Service for Cryo-EM data processing on AWS</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/stion-a-software-as-a-service-for-cryo-em-data-processing-on-aws.html</link>
      <pubDate>Tue, 17 Aug 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/stion-a-software-as-a-service-for-cryo-em-data-processing-on-aws.html</guid>
      <description>This post was written by Swapnil Bhatkar, Cloud Engineer, NREL in collaboration with Edward Eng Ph.D. and Micah Rapp Ph.D, both SEMC/NYSBC, and Evan Bollig Ph.D. and Aniket Deshpande, both AWS. Introduction Cryo-electron microscopy (Cryo-EM) technology allows biomedical researchers to image frozen biological molecules, such as proteins, viruses and nucleic acids, and obtain structures of […]
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Arm/AWS Cloud Hackathon - Compilers in HPC, their use and abuse</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/armaws-cloud-hackathon-compilers-in-hpc-their-use-and-abuse.html</link>
      <pubDate>Thu, 12 Aug 2021 15:00:23 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/armaws-cloud-hackathon-compilers-in-hpc-their-use-and-abuse.html</guid>
      <description>This is a talk that was delivered at the AWS/Arm Cloud Hackathon in July 2021.
Will Lovett works for Arm on the Arm compiler, and talk about how compilers work, and how to best leverage them to get the result you&amp;rsquo;re looking for when porting a code or working on performance.
The Summer Hackathon ran for a week from July 12-16 in 2021. It&amp;rsquo;s purpose was to assemble the HPC community around a common goal of beginning the porting/testing/tuning process for dozens of codes to use Arm-based processors, in this case, AWS&amp;rsquo;s Graviton2&amp;rsquo;s.
During the week we had a series of talks from global leaders and experts in various fields from compilers to networking, profiling debugging and tooling. We&amp;rsquo;ve published all the talks here in the HPC Tech Shorts channel, in the hope that everyone gets some value from them.</description>
    </item>
    
    <item>
      <title>Price-Performance Analysis of Amazon EC2 GPU Instance Types using NVIDIA’s GPU optimized seismic code</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/price-performance-analysis-of-amazon-ec2-gpu-instance-types-using-nvidias-gpu-optimized-seismic-code.html</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/price-performance-analysis-of-amazon-ec2-gpu-instance-types-using-nvidias-gpu-optimized-seismic-code.html</guid>
      <description>Seismic imaging is the process of positioning the Earth’s subsurface reflectors. It transforms the seismic data recorded in time at the Earth’s surface to an image of the Earth’s subsurface. This is done by back-propagating data from time to space in a given velocity model. Kirchhoff depth migration is a well-known technique used in geophysics for seismic imaging. Kirchhoff time and depth migration produce an image with higher resolution and generate an image of the subsurface for a subset class of the data, providing valuable information about the petrophysical properties of the rocks and helps to determine how accurate the velocity model is. This blog post looks at the price-performance characteristics computing Kirchhoff migration methods on GPUs using Nvidia’s GPU-optimized code.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Arm/AWS Cloud Hackathon - Performance Portability with Tom Deakin</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/armaws-cloud-hackathon-performance-portability-with-tom-deakin.html</link>
      <pubDate>Tue, 10 Aug 2021 15:00:22 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/armaws-cloud-hackathon-performance-portability-with-tom-deakin.html</guid>
      <description>This is a talk that was delivered at the AWS/Arm Cloud Hackathon in July 2021.
Tom Deakin is a computer scientist at the University of Bristol, and speaks to us about how to build for portability, and what that really means in an era of so many hardware technology choices.
The Summer Hackathon ran for a week from July 12-16 in 2021. It&amp;rsquo;s purpose was to assemble the HPC community around a common goal of beginning the porting/testing/tuning process for dozens of codes to use Arm-based processors, in this case, AWS&amp;rsquo;s Graviton2&amp;rsquo;s.
During the week we had a series of talks from global leaders and experts in various fields from compilers to networking, profiling debugging and tooling. We&amp;rsquo;ve published all the talks here in the HPC Tech Shorts channel, in the hope that everyone gets some value from them.
If you want to join in our collective effort to boost the performance of HPC workloads on Arm, DM us at Twitter (@TechHpc) or head to a-hug.</description>
    </item>
    
    <item>
      <title>Arm/AWS Cloud Hackathon - Profiling without printf()</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/armaws-cloud-hackathon-profiling-without-printf.html</link>
      <pubDate>Thu, 05 Aug 2021 15:00:23 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/armaws-cloud-hackathon-profiling-without-printf.html</guid>
      <description>This is a talk that was delivered at the AWS/Arm Cloud Hackathon in July 2021.
John Linford is Arm&amp;rsquo;s director of HPC and in this talk he covers why you should evolve your profiling beyond using print(), and how much time and effort this can save you. He also dives into some reasons to avoid premature optimization.
The Summer Hackathon ran for a week from July 12-16 in 2021. It&amp;rsquo;s purpose was to assemble the HPC community around a common goal of beginning the porting/testing/tuning process for dozens of codes to use Arm-based processors, in this case, AWS&amp;rsquo;s Graviton2&amp;rsquo;s.
During the week we had a series of talks from global leaders and experts in various fields from compilers to networking, profiling debugging and tooling. We&amp;rsquo;ve published all the talks here in the HPC Tech Shorts channel, in the hope that everyone gets some value from them.</description>
    </item>
    
    <item>
      <title>Bare metal performance with the AWS Nitro System</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/bare-metal-performance-with-the-aws-nitro-system.html</link>
      <pubDate>Thu, 05 Aug 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/bare-metal-performance-with-the-aws-nitro-system.html</guid>
      <description>High Performance Computing (HPC) is known as a domain where applications are well-optimized to get the highest performance possible on a platform. Unsurprisingly, a common question when moving a workload to AWS is what performance difference there may be from an existing on-premises “bare metal” platform. This blog will show the performance differential between “bare metal” instances and instances that use the AWS Nitro hypervisor is negligible for the evaluated HPC workloads.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Arm/AWS Cloud Hackathon Talk - Application Scaling with Jeff Hammond</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/armaws-cloud-hackathon-talk-application-scaling-with-jeff-hammond.html</link>
      <pubDate>Tue, 03 Aug 2021 15:00:07 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/armaws-cloud-hackathon-talk-application-scaling-with-jeff-hammond.html</guid>
      <description>This is a talk that was delivered at the AWS/Arm Cloud Hackathon in July 2021.
Jeff Hammond is a computational scientist at NVIDIA and walks through techniques for getting applications to scale in HPC.
The Summer Hackathon ran for a week from July 12-16 in 2021. It&amp;rsquo;s purpose was to assemble the HPC community around a common goal of beginning the porting/testing/tuning process for dozens of codes to use Arm-based processors, in this case, AWS&amp;rsquo;s Graviton2&amp;rsquo;s.
During the week we had a series of talks from global leaders and experts in various fields from compilers to networking, profiling debugging and tooling. We&amp;rsquo;ve published all the talks here in the HPC Tech Shorts channel, in the hope that everyone gets some value from them.
If you want to join in our collective effort to boost the performance of HPC workloads on Arm, DM us at Twitter (@TechHpc) or head to a-hug.</description>
    </item>
    
    <item>
      <title>Pushing pixels with NICE DCV</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/pushing-pixels-with-nice-dcv.html</link>
      <pubDate>Fri, 30 Jul 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/pushing-pixels-with-nice-dcv.html</guid>
      <description>NICE DCV, our high-performance, low-latency remote-display protocol, was originally created for scientists and engineers who ran large workloads on far-away supercomputers, but needed to visualize data without moving it. Pushing pixels over limited bandwidth across the globe has been the goal of the DCV team since 2007. DCV was able to make very frugal use of very scarce bandwidth, because it was super lean, used data-compression techniques and quickly adopted cutting-edge technologies of the time from GPUs (this is HPC, after all, we left nothing on the table when it came to exploiting new gadgets). This allowed the team to create a super light-weight visualization package that could stream pixels over almost any network. Fast forward to the 2020s, and a generation of gamers, artists, and film-makers all want to do the same thing as HPC researchers- only this time there are way more pixels, because we now have HD and 4k (and some people have multiple), and for most of them, it’s 60 frames per second, or it’s not worth having.</description>
    </item>
    
    <item>
      <title>How Netflix used DCV (and AWS) to distribute their creative workforce (and saved our sanity)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/how-netflix-used-dcv-and-aws-to-distribute-their-creative-workforce-and-saved-our-sanity.html</link>
      <pubDate>Thu, 29 Jul 2021 15:00:22 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/how-netflix-used-dcv-and-aws-to-distribute-their-creative-workforce-and-saved-our-sanity.html</guid>
      <description>Netflix has a really ambitious goal to create more content than practically anyone, but ran into headwinds when Covid-19 forced us all to lock down: it became hard to recruit and support artists, editors and other creatives without finding some really clever solutions to putting powerful compute at their fingertips anywhere in the world : a perfect job for AWS&amp;rsquo;s NICE DCV and globally-distributed infrastructure.
But there&amp;rsquo;s another lesson here - when you listen to how Michelle describes the way Netflix works to support their most valuable assets (clever, talented people), it should strike you as a really good lesson for how we should prosecute our mission to make scientists and engineers more powerful and productive with all these same tools.
You can find Michelle on linkedIn (linkedin.com/in/michellebrenner/) if you want to know more, or you can join her working at Netflix by heading to job.</description>
    </item>
    
    <item>
      <title>Scalable and Cost-Effective Batch Processing for ML workloads with AWS Batch and Amazon FSx</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/scalable-and-cost-effective-batch-processing-for-ml-workloads-with-aws-batch-and-amazon-fsx.html</link>
      <pubDate>Fri, 23 Jul 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/scalable-and-cost-effective-batch-processing-for-ml-workloads-with-aws-batch-and-amazon-fsx.html</guid>
      <description>Batch processing is a common need across varied machine learning use cases such as video production, financial modeling, drug discovery, or genomic research. The elasticity of the cloud provides efficient ways to scale and simplify batch processing workloads while cutting costs. In this post, you’ll learn a scalable and cost-effective approach to configure AWS Batch Array jobs to process datasets that are stored on Amazon S3 and presented to compute instances with Amazon FSx for Lustre.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Nextflow Tower  and how it makes it easy to manage a lot of infrastructure quickly.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/nextflow-tower-and-how-it-makes-it-easy-to-manage-a-lot-of-infrastructure-quickly.html</link>
      <pubDate>Thu, 22 Jul 2021 15:00:09 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/nextflow-tower-and-how-it-makes-it-easy-to-manage-a-lot-of-infrastructure-quickly.html</guid>
      <description>Nextflow Tower became necessary when lots of Nextflow users started to leave the confines of their existing environments in search of more compute, more storage, more capabilities. Nextflow made their workflows portable, but there was still a lot of relatively complex work to be done standing up cloud infrastructure, setting limits, choosing instances, pondering VPCs, Batch compute environments &amp;hellip; You get the idea.
Nextflow Tower massively simplifies that. It integrates deeply with all these environments (lots of clouds, lots of cluster types, and even in hybrid mode) such that a researcher with a good idea can go from laptop to server to cluster to massive compute farm, step-by-step as their idea and ambition grows.
Evan Floden, the CEO of Seqera Labs and a core developer of Nextflow (along with his friend and business partner Paolo Di Tommaso) steps in to take us for a drive of Tower and explains some of the tech underpinning it.</description>
    </item>
    
    <item>
      <title>NF Core is an example to all compute-intensive scientific fields. They should all watch this.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/nf-core-is-an-example-to-all-compute-intensive-scientific-fields-they-should-all-watch-this.html</link>
      <pubDate>Thu, 15 Jul 2021 20:12:22 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/nf-core-is-an-example-to-all-compute-intensive-scientific-fields-they-should-all-watch-this.html</guid>
      <description>NF Core is an open source repository of Nextflow workflows that can be downloaded, shared, forked, updated, tested and validated. If you&amp;rsquo;ve ever heard that quote from Isaac Newtown about seeing further by standing on the shoulders of giants: well this is the ladder modern day scientists are using to climb up those very big shoulders. Phil Ewels from the SciLife lab at Stockholm University walks us through some of the history of NF Core and gives us a demo of how easy it is to use, and the cool tools he and his project team have built around to just make it &amp;hellip; a no brainer. Every domain of science should be looking at how this works, because every field of science performs a workflow on data - and those workflows are getting increasingly complicated. If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>AWS&#39;s HPC Storage storage options - choose, use and abuse. Here&#39;s how.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/awss-hpc-storage-storage-options-choose-use-and-abuse-heres-how.html</link>
      <pubDate>Fri, 09 Jul 2021 12:43:09 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/awss-hpc-storage-storage-options-choose-use-and-abuse-heres-how.html</guid>
      <description>AWS has a LOT of storage options (block, PIOPS, object, volume, loads of file sytems), pretty much all of which can be used for HPC. That&amp;rsquo;s because we&amp;rsquo;re not forced to come up with one single massively fast storage solution that can cope with a hurricane of worst-case usage - you&amp;rsquo;re not making one-way door decisions on infrastructure. Cloud infrastructure is a two-way door and you can change your mind as many times as you like.
So, yes, you can have 200 GByte/s Lustre, but if you&amp;rsquo;re doing embarrassingly-parallel workloads that just need to stream data through a CPU, you&amp;rsquo;re missing an optimization right there. You can have very specifically tuned storage for very specific workloads (and specific clusters or compute environments, too, if you like). And you can have very specifically lower costs associated with it if you do.</description>
    </item>
    
    <item>
      <title>Batch can now use Fargate for a truly serverless experience.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/batch-can-now-use-fargate-for-a-truly-serverless-experience.html</link>
      <pubDate>Thu, 01 Jul 2021 14:31:04 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/batch-can-now-use-fargate-for-a-truly-serverless-experience.html</guid>
      <description>Now that AWS Batch supports AWS Fargate, you can submit jobs any time you like, and AWS just spins up the resources you need when you need them, REALLY quickly (because that&amp;rsquo;s what Fargate is awesome at). You don&amp;rsquo;t need to keep any resources running other than precisely the ones you need right now, and you get results right away. No managing servers, or even worrying about them. That&amp;rsquo;s why it&amp;rsquo;s serverless :-)
Our Principal Product Manager for AWS Batch, Steve Kendrex, explains how it all works and walks us through a live example.
During the talk, we mentioned this page for more information on using Batch with Fargate, and that page is here: https://aws.amazon.com/batch/features/?nc=sn&amp;amp;loc=2
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Nextflow has changed the way science does computing and energized a community. We need this.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/nextflow-has-changed-the-way-science-does-computing-and-energized-a-community-we-need-this.html</link>
      <pubDate>Thu, 24 Jun 2021 17:18:49 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/nextflow-has-changed-the-way-science-does-computing-and-energized-a-community-we-need-this.html</guid>
      <description>In recent years, we’ve all been amazed at how large areas of scientific computing have moved to abstract their workflows away from the details of systems and cluster architecture. Bioinformatics has really lead the way in this regard - exploiting the fact that they came to large scale computing without decades of legacy code.
Nextflow is an open source project that has been hugely impactful in that movement, enabling a community leverage each other&amp;rsquo;s scientific work and - honestly - stand on each others&amp;rsquo; shoulders to reach further. We didn&amp;rsquo;t need a pandemic to drive that point home, but they played their part there too (more on that in a later show).
Today we’re joined by Evan Floden, who is a principle author of Nextflow (along with other leaders like Paolo Di Tommaso who was one of Nextflow’s creators). Evan is also the CEO of Seqera Labs, the company he and Paolo formed to provide extra support to Nextflow users, and to take management of the workflows and integration with the underlying infrastructure to a whole new level.</description>
    </item>
    
    <item>
      <title>In the search for performance, there’s more than one way to build a network</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/in-the-search-for-performance-theres-more-than-one-way-to-build-a-network.html</link>
      <pubDate>Tue, 22 Jun 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/in-the-search-for-performance-theres-more-than-one-way-to-build-a-network.html</guid>
      <description>AWS worked backwards from an essential problem in HPC networking (MPI ranks need to exchange lots of data quickly) and found a different solution for our unique circumstances, without trading off the things customers love the most about cloud: that you can run virtually any application, at scale, and right away. Find out more about how Elastic Fabric Adapter (EFA) can help your HPC workloads scale on AWS.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Getting started with containers in HPC at ISC’21</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/getting-started-with-containers-in-hpc-at-isc21.html</link>
      <pubDate>Fri, 18 Jun 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/getting-started-with-containers-in-hpc-at-isc21.html</guid>
      <description>Containers are rapidly maturing within the high performance computing (HPC) community and we’re excited to be part of the movement: listening to what customers have to say and feeding this back to both the community and our own product and service teams. Containerization has the potential to unblock HPC environments, so AWS ParallelCluster and container-native schedulers like AWS Batch are moving quickly to reflect the best practices developed by the community and our customers. This year is the seventh consecutive year we are hosting the ‘High Performance Container Workshop’ at ISC High Performance 2021 conference (ISC’21). The workshop will be taking place on July 2nd at 2PM CEST (7AM CST). The full program for the workshop is available on the High Performance Container Workshop page at https://hpcw.github.io/
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Arm HPC Cloud Hackathon</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/arm-hpc-cloud-hackathon.html</link>
      <pubDate>Thu, 17 Jun 2021 14:45:01 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/arm-hpc-cloud-hackathon.html</guid>
      <description>Arm and AWS are jointly sponsoring a summer hackathon that&amp;rsquo;s being run by the Arm HPC User group (A-HUG - and definitely we send a hug to everyone out there). The goal of the hackathon is to get loads of codes with Spack recipes building, running and validating on Arm based systems and identifying the ones that don&amp;rsquo;t work or don&amp;rsquo;t perform well, so we - as a community - can find them and get to work fixing them.
In today&amp;rsquo;s show we cover the logistics of how the hackathon will work, and especially cover the software and hardware we&amp;rsquo;re going to be using. There are a lot of new tools (especially cool things like Spack and reFrame) that we think are awesome - we&amp;rsquo;re pretty sure you&amp;rsquo;ll agree after you see them in action - Olly gives us a demo with a real test case.</description>
    </item>
    
    <item>
      <title>CloudWatch automation to keep your scratch disks humming, and your clusters running.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/cloudwatch-automation-to-keep-your-scratch-disks-humming-and-your-clusters-running.html</link>
      <pubDate>Thu, 10 Jun 2021 15:19:32 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/cloudwatch-automation-to-keep-your-scratch-disks-humming-and-your-clusters-running.html</guid>
      <description>Some workloads generate a LOT of output files and sometimes quite suddenly. For codes like OpenFOAM, this is data that you may not need until later when you run a post-processing job.
Given the amount of data isn’t always predictable, there are a few ways to prepare for this deluge, but most of them involve pre-provisioning too much storage in advance (and hoping you guessed correctly).
We’ve never been fans of guessing like that - we think infrastructure should just expand when you need it.
Stephen Sachs from our HPC Performance Engineering team came up with a great technique for solving this. He’s built some cloud automation with CloudWatch into AWS ParallelCluster so it triggers a “drain” process (a shell script) that pushes all the output files into Amazon S3 whenever the local filesystem on a compute instance reaches 80%. It’s surprisingly easy to do.</description>
    </item>
    
    <item>
      <title>The impact of network conditions on application performance is complicated.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/the-impact-of-network-conditions-on-application-performance-is-complicated.html</link>
      <pubDate>Thu, 03 Jun 2021 15:16:16 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/the-impact-of-network-conditions-on-application-performance-is-complicated.html</guid>
      <description>In the search for HPC application performance, there’s more than one way to build a network.
HPC applications (or “codes” as we usually call them) are often at the mercy of the network underpinning an HPC cluster. If your CPUs aren’t busy, it’s usually because something that’s meant to be feeding them data isn’t doing so at a fast enough rate. And often the culprit is the network.
Peter Mendygral has a lot of years of experience looking at networks and is one of the co-authors of the GPCnet benchmark, which aims to have a more nuanced look at how networks deliver for (or fail) the clusters built on them. In todays’s discussion, Pete speaks about the real world conditions found on many network fabrics, and shows us that when they depart from the idealized scenarios that common micro benchmarks measure, the results are anything but stellar.</description>
    </item>
    
    <item>
      <title>Building highly-available HPC infrastructure on AWS</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/building-highly-available-hpc-infrastructure-on-aws.html</link>
      <pubDate>Thu, 03 Jun 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/building-highly-available-hpc-infrastructure-on-aws.html</guid>
      <description>In this blog post, we will explain how to launch highly available HPC clusters across an AWS Region. The solution is deployed using the AWS Cloud Developer Kit (AWS CDK), a software development framework for defining cloud infrastructure in code and provisioning it through AWS CloudFormation, hiding the complexity of integration between the components.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>HOWTO make sure EFA is setup correctly (and what to do if it isn&#39;t).</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/howto-make-sure-efa-is-setup-correctly-and-what-to-do-if-it-isnt.html</link>
      <pubDate>Fri, 28 May 2021 14:22:10 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/howto-make-sure-efa-is-setup-correctly-and-what-to-do-if-it-isnt.html</guid>
      <description>By popular request, we’re looking today at the EFA software stack and environment: how to make sure it’s set up correctly (so you get great performance from your codes), how to tell if it’s not, and how to fix that.
Austin Cherian, our performance junkie from the HPC Developer Relations team in AWS Engineering joins us to deep dive into the nitty gritty of the stack and some useful techniques for debugging. We cover both Open MPI and Intel MPI, as well as checking the libfabric providers and the hardware enablement underneath.
In the discussion, we reference a lot of helpful pages, including:
the EFA homepage: https://aws.amazon.com/hpc/efa/ supported instances: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html#efa-instance-types the user guide: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html troubleshooting: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/troubleshooting-ena.html#disable-enhanced-networking-ena-instance-store If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Accelerating research and development of new medical treatments with HPC on AWS</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/accelerating-research-and-development-of-new-medical-treatments-with-hpc-on-aws.html</link>
      <pubDate>Fri, 28 May 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/accelerating-research-and-development-of-new-medical-treatments-with-hpc-on-aws.html</guid>
      <description>Today, more than 290,000 researchers in France are working to provide better support and care for patients through modern medical treatment. To fulfill their mission, these researchers must be equipped with powerful tools. At AWS, we believe that technology has a critical role to play in medical research. Why? Because technology can take advantage of the significant amount of data generated in the healthcare system and in the research community to enable opportunities for more accurate diagnoses, and better treatments for many existing and future diseases. To support elite research in France, we are proud to be a sponsor of two French organizations: Gustave Roussy and Sorbonne University. AWS is providing them with the computing power and machine learning technologies needed to accelerate cancer research and develop a treatment for COVID-19.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>WRF performance teardown on Graviton vs x86</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/wrf-performance-teardown-on-graviton-vs-x86.html</link>
      <pubDate>Thu, 20 May 2021 16:16:52 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/wrf-performance-teardown-on-graviton-vs-x86.html</guid>
      <description>(A complete teardown of WRF performance on x86 and AWS Graviton, from memory subsystems, compilers and MPI stacks).
Weather simulation is a reliably difficult workload for almost any HPC architecture and is often used as a litmus test by many customers before they look too hard at a novel or different systems. Customers have asked us frequently about our performance for codes like WRF, and that’s been even more the case since we launched our Arm-based processor, the AWS Graviton2, in a range of EC2 instances.
So it’s exciting that Karthik Raman and Matt Koop (two leading engineers from our global HPC solution architecture team) dived deep to look at WRF’s performance across a range of instance types (both Intel and Graviton), with EFA (our fast fabric, as you might remember from last week) as well as investigating the impact of different MPIs and compilers.</description>
    </item>
    
    <item>
      <title>Training forecasters to warn severe hazardous weather on AWS</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/training-forecasters-to-warn-severe-hazardous-weather-on-aws.html</link>
      <pubDate>Tue, 18 May 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/training-forecasters-to-warn-severe-hazardous-weather-on-aws.html</guid>
      <description>Training users on how to use high performance computing resources — and the data that comes out as a result of those analyses — is an essential function of most research organizations. Having a robust, scalable, and easy-to-use platform for on-site and remote training is becoming a requirement for creating a community around your research mission. A great example of this comes from the NOAA National Weather Service Warning Decision Training Division (WDTD), which develops and delivers training on the integrated elements of the hazardous weather warning process within a National Weather Service (NWS) forecast office. In collaboration with the University of Oklahoma’s Cooperative Institute for Mesoscale Meteorological Studies (OU/CIMMS), WDTD conducts its flagship course, the Radar and Applications Course (RAC), for forecasters issuing warnings for flash floods, severe thunderstorms, and tornadoes. Trainees learn the warning process, the science and application of conceptual models, and technical aspects of analyzing radar and other weather data in the Advanced Weather Interactive Processing System (AWIPS).</description>
    </item>
    
    <item>
      <title>How EFA works and why we don&#39;t use infiniband in the cloud.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/how-efa-works-and-why-we-dont-use-infiniband-in-the-cloud.html</link>
      <pubDate>Thu, 13 May 2021 15:00:17 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/how-efa-works-and-why-we-dont-use-infiniband-in-the-cloud.html</guid>
      <description>AWS’s compute infrastructure is very much not like a ‘normal’ supercomputer (whatever that is). We don’t start with a blank page every few years and design the next big system. It’s more like a city where we have to build on what’s there already, renovate occasionally, and push for bigger and better and faster whilst keeping the lights on at all times.
That leads to a bunch of design decisions that drive our engineers in a very different direction and our Elastic Fabric Adapter is an example of just that. Brian Barrett (one of our Principal Engineers in the HPC team) joins us this week to talk about the genesis of EFA, how it works, and why it convinced us that we could do without specialist fabrics like Infiniband and still deliver the same (or better) application scaling performance that our HPC customers were pushing us for.</description>
    </item>
    
    <item>
      <title>AWS joins Arm to support Arm-HPC hackathon this summer</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/aws-joins-arm-to-support-arm-hpc-hackathon-this-summer.html</link>
      <pubDate>Wed, 12 May 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/aws-joins-arm-to-support-arm-hpc-hackathon-this-summer.html</guid>
      <description>Arm and AWS are calling all grad students and post-docs who want to gain experience advancing the adoption of the Arm architecture in HPC to join a world-wide community effort lead by the Arm HPC User’s Group (A-HUG). The event will take the form of a hackathon this summer and is aimed at getting open-source HPC codes to build and run fast on Arm-based processors, specifically AWS Graviton2. To make it a bit more exciting, A-HUG will be awarding an Apple M1 MacBook to each member of the team (max. 4 people) that contributes the most back to the Arm HPC community.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Numerical weather prediction on AWS Graviton2</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/numerical-weather-prediction-on-aws-graviton2.html</link>
      <pubDate>Mon, 10 May 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/numerical-weather-prediction-on-aws-graviton2.html</guid>
      <description>The Weather Research and Forecasting (WRF) model is a numerical weather prediction (NWP) system designed to serve both atmospheric research and operational forecasting needs. With the release of Arm-based AWS Graviton2 Amazon Elastic Compute Cloud (EC2) instances, a common question has been how these instances perform on large-scale NWP workloads. In this blog, we will present results from a standard WRF benchmark simulation and compare across three different instance types.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Interesting GPU &amp; CPU performance results from GROMACS</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/interesting-gpu-cpu-performance-results-from-gromacs.html</link>
      <pubDate>Thu, 06 May 2021 16:33:38 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/interesting-gpu-cpu-performance-results-from-gromacs.html</guid>
      <description>Carsten Kutzner is a researcher and scientific software developer at the Max Planck Institute for Biophysical Chemistry in Göttingen in Germany. He&amp;rsquo;s been doing some really diverse benchmarking studies in collaboration with our HPC engineering team, and he joins us today to talk about some of the results of that investigation.
What&amp;rsquo;s most interesting (and soft of unexpected) is the instances he concludes are the best for the job - whether you&amp;rsquo;re optimizing for sheer performance against the clock, or price performance because you need to maximize your budget within a more generous time window. No spoilers here: you need to hear him talk.
Molecular Dynamicists are used to doing these calculations because they measure their progress in nanoseconds of simulation time per DAY of wall clock time. As Carsten points out: if your job runs for days or weeks, it matters whether you can squeeze 10% more from the compute units it&amp;rsquo;s running on.</description>
    </item>
    
    <item>
      <title>Reader Question: What is the difference between canceling and terminating a job in AWS Batch?</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/reader-question-what-is-the-difference-between-canceling-and-terminating-a-job-in-aws-batch.html</link>
      <pubDate>Mon, 03 May 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/reader-question-what-is-the-difference-between-canceling-and-terminating-a-job-in-aws-batch.html</guid>
      <description>A customer asked us what is the difference between the CancelJob and TerminateJob API calls in AWS Batch. This post provides an overview of AWS Batch job states, and how these two API calls effect the job requests that you have submitted.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>What&#39;s New in DCV 2021.0</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/whats-new-in-dcv-20210.html</link>
      <pubDate>Thu, 29 Apr 2021 14:58:00 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/whats-new-in-dcv-20210.html</guid>
      <description>DCV was originally built for supercomputing centers to push pixels over the internet and enable a scientist or aerospace engineer to feel like they had an HPC cluster under their desk when inspecting detailed imagery or manipulating designs.
Now it&amp;rsquo;s getting used way beyond HPC in gaming, software development and &amp;hellip; enabling lots of us to be productive working from home.
All these little things make the desktop virtualization experience more real, so we decided to catch up with Rey Wang (Sr Product Manager for DCV) and Paolo Maggi (the GM for our DCV organization) to find out what&amp;rsquo;s new.
You can find out more about DCV here: aws.amazon.com/hpc/dcv and you can also watch the Tech Short we did last week on DCV&amp;rsquo;s better faster and more amazing streaming capabilities which are driving gaming engines to deliver rich experience in 4K @ 60 FPS (which we all know is essential &amp;hellip; right?</description>
    </item>
    
    <item>
      <title>A VDI solution with EnginFrame and NICE DCV Session Manager built with AWS CDK</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/a-vdi-solution-with-enginframe-and-nice-dcv-session-manager-built-with-aws-cdk.html</link>
      <pubDate>Fri, 23 Apr 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/a-vdi-solution-with-enginframe-and-nice-dcv-session-manager-built-with-aws-cdk.html</guid>
      <description>This post was written by Dario La Porta, AWS Professional Services Senior Consultant for HPC. Customers across a wide range of industries such as energy, life sciences, and design and engineering are facing challenges in managing and analyzing their data. Not only is the amount and velocity of data increasing, but so is the complexity and […]
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Supercomputing Visualization good enough for the most demanding gamers.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/supercomputing-visualization-good-enough-for-the-most-demanding-gamers.html</link>
      <pubDate>Thu, 22 Apr 2021 15:16:21 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/supercomputing-visualization-good-enough-for-the-most-demanding-gamers.html</guid>
      <description>DCV was originally created for scientists and engineers who run thing on supercomputers. They needed to visualize massive datasets produced by really large compute workloads crunching petabytes of data. That takes some fast machines and some very fancy GPUs. Since we don’t like putting scientist into freezing data centers our DCV engineers worked out how to push pixels over the internet quickly, without any loss of fidelity.
Fast forward 20 years and a generation of gamers want to do the same thing, only this time there are way more pixels (blame HD and 4k) and it’s 60 frames per second, or bust.
That meant we had to so some extra tricks to make video streams not miss a beat when very normal things happen, like the internet drops a packet or there’s some congestion because your mum is on a zoom call with her team at the office.</description>
    </item>
    
    <item>
      <title>AWS Batch&#39;s new Faster Scaling features</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/aws-batchs-new-faster-scaling-features.html</link>
      <pubDate>Thu, 15 Apr 2021 16:50:36 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/aws-batchs-new-faster-scaling-features.html</guid>
      <description>Our AWS Batch development team have been working on some major improvements to the way Batch assesses the need to scale up or down. We’re doing 5x as many scaling evaluations per hour now which should pick up the pace significantly. With some other improvements which we talk about in the video, we’re hoping customers will see less throttling on the submitjob API call and just see an overall faster submission rate, and faster time to results (since that’s what really matters).
Jo Adegbola is the Sr Manager of all of our AWS Batch software engineering teams and works closely with Steve Kendrex the Principal Product Manager for Batch obsessing on how to “make stuff go fast” for Batch users everywhere.
During the show we referred to a couple of paths to get started on Batch.
A tutorial to get started with simple jobs on Batch running in EC2 Spot: https://aws.</description>
    </item>
    
    <item>
      <title>&#39;Making stuff run fast&#39;, starting with GROMACS.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/making-stuff-run-fast-starting-with-gromacs.html</link>
      <pubDate>Thu, 08 Apr 2021 15:00:15 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/making-stuff-run-fast-starting-with-gromacs.html</guid>
      <description>We just published a blog post last week with a deep dive into what makes GROMACS tick. The blog post talked about software stacks and EC2 instances that will deliver the best possible performance for people trying to do some molecular dynamics quickly. This turns out to be pretty much EVERYONE in the MD community, because these are people who measure their progress in nanoseconds of simulation PER DAY of wall clock time.
Austin Cherian is one of our senior engineers in the Developer Relations team, and the one who is the most obsessed with performance and getting the performance testing methodology right. We asked him to join tech shorts this week to show us how he uses and abuses AWS ParallelCluster in his quest of “making stuff go fast”, because his use cases are pretty common, and might help you imagine some better ways of working.</description>
    </item>
    
    <item>
      <title>Introducing support for per-job Amazon EFS volumes in AWS Batch</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/introducing-support-for-per-job-amazon-efs-volumes-in-aws-batch.html</link>
      <pubDate>Mon, 05 Apr 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/introducing-support-for-per-job-amazon-efs-volumes-in-aws-batch.html</guid>
      <description>Large-scale data analysis usually involves some multi-step process where the output of one job acts as the input of subsequent jobs. Customers using AWS Batch for data analysis want a simple and performant storage solution to share with and between jobs. We are excited to announce that customers can now use Amazon Elastic File System (Amazon […]
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Containers, Episode II - the Runtimes Strike Back</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/containers-episode-ii-the-runtimes-strike-back.html</link>
      <pubDate>Thu, 01 Apr 2021 16:48:35 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/containers-episode-ii-the-runtimes-strike-back.html</guid>
      <description>Christian Kniep (our senior developer relations engineer) from HPC Engineering is back to finish the conversation we started about containers in HPC. Christian is leading the cause for containerization in HPC, and helping our engineering and product teams focus on enabling that path on AWS.
Today we&amp;rsquo;re talking about how containers work in a shared infrastructure environment, with shared filesystems (like Lustre) and we&amp;rsquo;ll also cover multi-node parallelism, too, since MPI is where it&amp;rsquo;s at for many (most?) people in the HPC community.
(And we completely missed thew opportunity to make any April fools jokes, so everything we say is totally believable) :-)
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>GROMACS price-performance optimizations on AWS</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/gromacs-price-performance-optimizations-on-aws.html</link>
      <pubDate>Wed, 31 Mar 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/gromacs-price-performance-optimizations-on-aws.html</guid>
      <description>Molecular dynamics (MD) is a simulation method for analyzing the movement and tracing trajectories of atoms and molecules where the dynamics of a system evolve over time. MD simulations are used across various domains such as material sciences, biochemistry, biophysics and are typically used in two broad ways to study a system. The importance of […]
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Running finite element analysis using Simcenter Nastran on AWS</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/running-finite-element-analysis-using-simcenter-nastran-on-aws.html</link>
      <pubDate>Wed, 31 Mar 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/running-finite-element-analysis-using-simcenter-nastran-on-aws.html</guid>
      <description>This post was written by Dnyanesh Digraskar, Sr. Partner Solutions Architect for HPC at AWS and co-authored by Wei Zhang and Ravi Gupta, Sr Software Engineers for Simcenter Nastran at Siemens. Introduction In this blog, we demonstrate the deployment, performance, and price comparisons of Simcenter Nastran for three finite element analysis (FEA) based use cases […]
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Simplify HPC cluster usage with AWS Cloud9 and AWS ParallelCluster</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/simplify-hpc-cluster-usage-with-aws-cloud9-and-aws-parallelcluster.html</link>
      <pubDate>Wed, 31 Mar 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/simplify-hpc-cluster-usage-with-aws-cloud9-and-aws-parallelcluster.html</guid>
      <description>This post was written by Benjamin Meyer, AWS Solutions Architect When companies and labs start their high performance computing (HPC) journey in the AWS Cloud, it’s not only because they’re in search of elasticity and scale – they’re also in search of new tools and environments. Initially this can appear challenging as there are many […]
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Welcome to the AWS HPC Blog</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/welcome-to-the-aws-hpc-blog.html</link>
      <pubDate>Wed, 31 Mar 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/welcome-to-the-aws-hpc-blog.html</guid>
      <description>This post is written by Deepak Singh, Vice President of Compute Services. At AWS, we love working with customers to solve their toughest challenges. High performance computing (HPC) is one of those challenges that pushes against the boundaries of AWS performance at scale. HPC is also a personal interest of mine, as I came to […]
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>&#39;NO TEARS HPC - honest-to-goodness research-ready HPC clusters in under 10 minutes.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/no-tears-hpc-honest-to-goodness-research-ready-hpc-clusters-in-under-10-minutes.html</link>
      <pubDate>Thu, 25 Mar 2021 16:30:03 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/no-tears-hpc-honest-to-goodness-research-ready-hpc-clusters-in-under-10-minutes.html</guid>
      <description>When COVID-19 struck in 2020, lots of scientists all over the world had to drop EVERYTHING and get busy solving some very big problems, very quickly. AWS became a founding member of the COVID-19 HPC Consortium, which is a White House-led initiative to provide COVID-19 researchers worldwide with free compute time and huge resources on leading industry, government, and academic HPC systems. In short, we all threw the kitchen sink at the problem of speeding up the pace of scientific discovery in the fight to stop the virus.
From AWS’s point of view, global reach and capacity are solved problems. Our challenge was to onboard researchers quickly, with no specialized knowledge necessary &amp;hellip; so they could get computing on AWS as fast as humanly possible. To that end we developed the NoTearsHPC cluster solution for 1-click launches.
Evan Bollig and Sean Smith from the team who built No Tears join us to talk about how it work, what it provides and show us how easy it is to do really complicated things really fast.</description>
    </item>
    
    <item>
      <title>Containers in HPC - what they fix and what they break (and how to fix that, too)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/containers-in-hpc-what-they-fix-and-what-they-break-and-how-to-fix-that-too.html</link>
      <pubDate>Thu, 18 Mar 2021 16:50:52 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/containers-in-hpc-what-they-fix-and-what-they-break-and-how-to-fix-that-too.html</guid>
      <description>Today we&amp;rsquo;re joined by Christian Kniep who is one of our senior developer advocates in HPC Engineering. Christian is leading the cause for containerization in HPC, and helping our engineering and product teams focus on enabling that path on AWS.
We know containers are a &amp;lsquo;mixed&amp;rsquo; topic for HPC folks, and so we&amp;rsquo;re starting a series of Tech Shorts to talk about what they are, what problems they solve, what new problems they create, and what solutions exist to address those.
Today is part 1, where we talk about how containers work generally, how they work on AWS, with things like AWS Nitro and GPUs in play.
Next week, we&amp;rsquo;ll talk about containers in shared infrastructure and multi-node parallelism, too.
The FOSDEM devroom we spoke about is here: https://tinyurl.com/fosdemhpc21 and you can find Christian on twitter here https://twitter.com/CQnib - pick up the conversation with him if you have some thoughts about containers and how they work for you (or not).</description>
    </item>
    
    <item>
      <title>How to accelerate CryoEM Analysis with AWS ParallelCluster and FSx for Lustre</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/how-to-accelerate-cryoem-analysis-with-aws-parallelcluster-and-fsx-for-lustre.html</link>
      <pubDate>Thu, 11 Mar 2021 10:55:25 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/how-to-accelerate-cryoem-analysis-with-aws-parallelcluster-and-fsx-for-lustre.html</guid>
      <description>In today’s show, we’re talking about how to choose the right compute and storage elements to get great performance for CryoSPARC, which is one of the popular codes used in cryogenic electron microscopy (CryoEM) analysis.
This is important because, as you’ll hear in the discussion, breaking the pipeline down into different stages and optimizing the infrastructure for each one can speed up the entire workflow, and save a whole lot of money on that compute, too. All these workloads ran on AWS ParallelCluster, which lets you have multiple queues with different instance types and orchestration options (check out last week’s show: https://www.youtube.com/watch?v=C4iSNjcW5O4). ParallelCluster also makes it easy to construct an FSx for Lustre filesystem on the fly using data from an Amazon S3 bucket.
Steve Litster is our Principal HPC business leader for Healthcare and Lifesciences, based out of Boston, and is a recovering x-ray crystallographer (and still has occasional flashbacks of being in a lab).</description>
    </item>
    
    <item>
      <title>Welcome to HPC Tech Shorts!</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/welcome-to-hpc-tech-shorts.html</link>
      <pubDate>Thu, 04 Mar 2021 14:53:51 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/welcome-to-hpc-tech-shorts.html</guid>
      <description>Each week we’re going to take you to the water cooler here in AWS to talk to engineers and architects about the technology, tools and techniques that make AWS the best place to run your HPC workloads in the cloud.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>HOWTO configure multiple queues and instance types in AWS ParallelCluster</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/howto-configure-multiple-queues-and-instance-types-in-aws-parallelcluster.html</link>
      <pubDate>Thu, 04 Mar 2021 14:15:29 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/howto-configure-multiple-queues-and-instance-types-in-aws-parallelcluster.html</guid>
      <description>In today’s show, we talk about AWS ParallelCluster 2.9 and its new features built on Slurm’s power management module. This lets you build clusters with multiple queues and instances types within those queues. This allows the cluster to scale nodes that fit a given workload, making scaling decisions much more job-driven, and means each queue can be quite specifically optimized for a code .
We’ll show a real live cluster upgrade happening, doing in 10 minutes what it generally takes 18 months to pull off if you still live with on-premises facilities.
Rex Chen is one of our amazing Software Engineers who works every day on AWS ParallelCluster and he’s joined by Angel Pizarro, our Principal Developer Advocate for HPC &amp;amp; Batch who has personal experience building those clusters and upgrading them, and knows better now.
Here’s the blog we mentioned in the show:</description>
    </item>
    
    <item>
      <title>CFD on AWS&#39;s Graviton (Arm-based) CPU - early results.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/cfd-on-awss-graviton-arm-based-cpu-early-results.html</link>
      <pubDate>Thu, 25 Feb 2021 12:59:34 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/cfd-on-awss-graviton-arm-based-cpu-early-results.html</guid>
      <description>Our CFD expert, Dr Neil Ashton put AWS&amp;rsquo;s Arm-based CPU, the AWS Graviton, to the test with some CFD workloads and shares the results with us. If you&amp;rsquo;re interested in CFD, or just wanting to find out if newer CPU architectures like the Graviton can deliver the performance you expect for HPC codes, you&amp;rsquo;ll get some data to start you off.
Mentioned in the show: Neil&amp;rsquo;s CFD workshops https://cfd-on-pcluster.workshop.aws
Not mentioned but also important: CFD Direct (two of the original authors of OpenFOAM) also have an offering in AWS Marketplace that runs on AWS Graviton and is really easy to launch. You can check it out here: https://aws.amazon.com/marketplace/pp/B08CHKT98H
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Elements That You Can Use To Create A New Post On This Template.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/elements.html</link>
      <pubDate>Sun, 15 Mar 2020 15:40:24 +0600</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/elements.html</guid>
      <description>Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.
Heading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.
Strong emphasis, aka bold, with asterisks or underscores.
Combined emphasis with asterisks and underscores.
Strikethrough uses two tildes. Scratch this.
Link I&amp;amp;rsquo;m an inline-style link
I&amp;amp;rsquo;m an inline-style link with title
I&amp;amp;rsquo;m a reference-style link
I&amp;amp;rsquo;m a relative reference to a repository file
You can use numbers for reference-style link definitions
Or leave it empty and use the link text itself.
URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).
Some text to show that the reference links can follow later.</description>
    </item>
    
    <item>
      <title>HPC Tech Shorts</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/techshorts.html</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/techshorts.html</guid>
      <description>HPC Tech Shorts HPC Tech Shorts is the HPC engineering water-cooler at AWS. New shows on YouTube weekly - sometimes more often.
If the videos on HPC Tech Shorts help you understand AWS a little better and get you closer to your goal, consider subscribing to the channel, so you can get new videos as they&amp;rsquo;re released.</description>
    </item>
    
    <item>
      <title>HPC Workshops</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/workshops.html</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/workshops.html</guid>
      <description>HPC Workshops We build workshops that can help you, step-by-step expand your skills and solve problems more efficiently.</description>
    </item>
    
    <item>
      <title>HPC Blog Channel</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/hpcblog.html</link>
      <pubDate>Thu, 26 Sep 2019 13:29:51 -0500</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/hpcblog.html</guid>
      <description>.right { float: right; padding: 0 0 20px 20px; }
AWS ParallelCluster is an open source tool that makes it easy to deploy and manage HPC clusters on AWS. It is available at no additional charge, and you pay only for the AWS resources needed to run your applications.</description>
    </item>
    
  </channel>
</rss>
