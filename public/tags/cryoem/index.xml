<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>cryoem on Day 1 HPC (Staging)</title>
    <link>https://d175uvn6dnkepf.cloudfront.net/tags/cryoem.html</link>
    <description>Recent content in cryoem on Day 1 HPC (Staging)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Oct 2022 17:38:29 +0000</lastBuildDate><atom:link href="https://d175uvn6dnkepf.cloudfront.net/tags/cryoem/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How KEK changed how everyone in Japan does CryoEM (Part 3 of 4)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/how-kek-changed-how-everyone-in-japan-does-cryoem-part-3-of-4.html</link>
      <pubDate>Thu, 20 Oct 2022 17:38:29 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/how-kek-changed-how-everyone-in-japan-does-cryoem-part-3-of-4.html</guid>
      <description>CryoEM is an awesome research tool, but it also comes with some challenges. The software stack is complex, and the hardware it needs is sometimes exotic and hard to get. Afterwards, there&amp;rsquo;s a massive dataset and a serious set of challenges to moving it around, and managing it. (No one wants to get stuck baby sitting someone else&amp;rsquo;s petabyte dataset unless they have to).
The team at KEK in Japan took all these challenges head on and solved them in a very different way (spoiler: it involved the cloud). But as is often the case, when teams in Japan solve a big HPC problem, it&amp;rsquo;s usually something that&amp;rsquo;s going to get picked up around the world. And KEK are very much those kinds of people.
Today is just one part in a series of Four HPC Tech Shorts dedicated to the work KEK did (yes, it&amp;rsquo;s that awesome).</description>
    </item>
    
    <item>
      <title>KEK&#39;s arsenal of CryoEM benchmark data - a detailed walk through (Part 4 of 4)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/keks-arsenal-of-cryoem-benchmark-data-a-detailed-walk-through-part-4-of-4.html</link>
      <pubDate>Thu, 20 Oct 2022 17:38:23 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/keks-arsenal-of-cryoem-benchmark-data-a-detailed-walk-through-part-4-of-4.html</guid>
      <description>CryoEM is an awesome research tool, but it also comes with some challenges. The software stack is complex, and the hardware it needs is sometimes exotic and hard to get. Afterwards, there&amp;rsquo;s a massive dataset and a serious set of challenges to moving it around, and managing it. (No one wants to get stuck baby sitting someone else&amp;rsquo;s petabyte dataset unless they have to).
The team at KEK in Japan took all these challenges head on and solved them in a very different way (spoiler: it involved the cloud). But as is often the case, when teams in Japan solve a big HPC problem, it&amp;rsquo;s usually something that&amp;rsquo;s going to get picked up around the world. And KEK are very much those kinds of people.
Today is just one part in a series of Four HPC Tech Shorts dedicated to the work KEK did (yes, it&amp;rsquo;s that awesome).</description>
    </item>
    
    <item>
      <title>KEK&#39;s novel solution for CryoEM&#39;s software and infra (Part 2 of 4)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/keks-novel-solution-for-cryoems-software-and-infra-part-2-of-4.html</link>
      <pubDate>Tue, 11 Oct 2022 16:53:39 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/keks-novel-solution-for-cryoems-software-and-infra-part-2-of-4.html</guid>
      <description>CryoEM is an awesome research tool, but it also comes with some challenges. The software stack is complex, and the hardware it needs is sometimes exotic and hard to get. Afterwards, there&amp;rsquo;s a massive dataset and a serious set of challenges to moving it around, and managing it. (No one wants to get stuck baby sitting someone else&amp;rsquo;s petabyte dataset unless they have to).
The team at KEK in Japan took all these challenges head on and solved them in a very different way (spoiler: it involved the cloud). But as is often the case, when teams in Japan solve a big HPC problem, it&amp;rsquo;s usually something that&amp;rsquo;s going to get picked up around the world. And KEK are very much those kinds of people.
Today is just one part in a series of Four HPC Tech Shorts dedicated to the work KEK did (yes, it&amp;rsquo;s that awesome).</description>
    </item>
    
    <item>
      <title>The Challenges of CryoEM with our friends from KEK in Japan (Part 1 of 4)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/the-challenges-of-cryoem-with-our-friends-from-kek-in-japan-part-1-of-4.html</link>
      <pubDate>Thu, 06 Oct 2022 15:04:35 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/the-challenges-of-cryoem-with-our-friends-from-kek-in-japan-part-1-of-4.html</guid>
      <description>CryoEM is an awesome research tool, but it also comes with some challenges. The software stack is complex, and the hardware it needs is sometimes exotic and hard to get. Afterwards, there&amp;rsquo;s a massive dataset and a serious set of challenges to moving it around, and managing it. (No one wants to get stuck baby sitting someone else&amp;rsquo;s petabyte dataset unless they have to).
The team at KEK in Japan took all these challenges head on and solved them in a very different way (spoiler: it involved the cloud). But as is often the case, when teams in Japan solve a big HPC problem, it&amp;rsquo;s usually something that&amp;rsquo;s going to get picked up around the world. And KEK are very much those kinds of people.
Today is one part in a series of Four HPC Tech Shorts dedicated to the work KEK did (yes, it&amp;rsquo;s that awesome).</description>
    </item>
    
    <item>
      <title>How to accelerate CryoEM Analysis with AWS ParallelCluster and FSx for Lustre</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/how-to-accelerate-cryoem-analysis-with-aws-parallelcluster-and-fsx-for-lustre.html</link>
      <pubDate>Thu, 11 Mar 2021 10:55:25 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/how-to-accelerate-cryoem-analysis-with-aws-parallelcluster-and-fsx-for-lustre.html</guid>
      <description>In today’s show, we’re talking about how to choose the right compute and storage elements to get great performance for CryoSPARC, which is one of the popular codes used in cryogenic electron microscopy (CryoEM) analysis.
This is important because, as you’ll hear in the discussion, breaking the pipeline down into different stages and optimizing the infrastructure for each one can speed up the entire workflow, and save a whole lot of money on that compute, too. All these workloads ran on AWS ParallelCluster, which lets you have multiple queues with different instance types and orchestration options (check out last week’s show: https://www.youtube.com/watch?v=C4iSNjcW5O4). ParallelCluster also makes it easy to construct an FSx for Lustre filesystem on the fly using data from an Amazon S3 bucket.
Steve Litster is our Principal HPC business leader for Healthcare and Lifesciences, based out of Boston, and is a recovering x-ray crystallographer (and still has occasional flashbacks of being in a lab).</description>
    </item>
    
  </channel>
</rss>
