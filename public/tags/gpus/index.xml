<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>GPUs on Day 1 HPC (Staging)</title>
    <link>https://d175uvn6dnkepf.cloudfront.net/tags/gpus.html</link>
    <description>Recent content in GPUs on Day 1 HPC (Staging)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Oct 2022 17:38:29 +0000</lastBuildDate><atom:link href="https://d175uvn6dnkepf.cloudfront.net/tags/gpus/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How KEK changed how everyone in Japan does CryoEM (Part 3 of 4)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/how-kek-changed-how-everyone-in-japan-does-cryoem-part-3-of-4.html</link>
      <pubDate>Thu, 20 Oct 2022 17:38:29 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/how-kek-changed-how-everyone-in-japan-does-cryoem-part-3-of-4.html</guid>
      <description>CryoEM is an awesome research tool, but it also comes with some challenges. The software stack is complex, and the hardware it needs is sometimes exotic and hard to get. Afterwards, there&amp;rsquo;s a massive dataset and a serious set of challenges to moving it around, and managing it. (No one wants to get stuck baby sitting someone else&amp;rsquo;s petabyte dataset unless they have to).
The team at KEK in Japan took all these challenges head on and solved them in a very different way (spoiler: it involved the cloud). But as is often the case, when teams in Japan solve a big HPC problem, it&amp;rsquo;s usually something that&amp;rsquo;s going to get picked up around the world. And KEK are very much those kinds of people.
Today is just one part in a series of Four HPC Tech Shorts dedicated to the work KEK did (yes, it&amp;rsquo;s that awesome).</description>
    </item>
    
    <item>
      <title>KEK&#39;s arsenal of CryoEM benchmark data - a detailed walk through (Part 4 of 4)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/keks-arsenal-of-cryoem-benchmark-data-a-detailed-walk-through-part-4-of-4.html</link>
      <pubDate>Thu, 20 Oct 2022 17:38:23 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/keks-arsenal-of-cryoem-benchmark-data-a-detailed-walk-through-part-4-of-4.html</guid>
      <description>CryoEM is an awesome research tool, but it also comes with some challenges. The software stack is complex, and the hardware it needs is sometimes exotic and hard to get. Afterwards, there&amp;rsquo;s a massive dataset and a serious set of challenges to moving it around, and managing it. (No one wants to get stuck baby sitting someone else&amp;rsquo;s petabyte dataset unless they have to).
The team at KEK in Japan took all these challenges head on and solved them in a very different way (spoiler: it involved the cloud). But as is often the case, when teams in Japan solve a big HPC problem, it&amp;rsquo;s usually something that&amp;rsquo;s going to get picked up around the world. And KEK are very much those kinds of people.
Today is just one part in a series of Four HPC Tech Shorts dedicated to the work KEK did (yes, it&amp;rsquo;s that awesome).</description>
    </item>
    
    <item>
      <title>KEK&#39;s novel solution for CryoEM&#39;s software and infra (Part 2 of 4)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/keks-novel-solution-for-cryoems-software-and-infra-part-2-of-4.html</link>
      <pubDate>Tue, 11 Oct 2022 16:53:39 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/keks-novel-solution-for-cryoems-software-and-infra-part-2-of-4.html</guid>
      <description>CryoEM is an awesome research tool, but it also comes with some challenges. The software stack is complex, and the hardware it needs is sometimes exotic and hard to get. Afterwards, there&amp;rsquo;s a massive dataset and a serious set of challenges to moving it around, and managing it. (No one wants to get stuck baby sitting someone else&amp;rsquo;s petabyte dataset unless they have to).
The team at KEK in Japan took all these challenges head on and solved them in a very different way (spoiler: it involved the cloud). But as is often the case, when teams in Japan solve a big HPC problem, it&amp;rsquo;s usually something that&amp;rsquo;s going to get picked up around the world. And KEK are very much those kinds of people.
Today is just one part in a series of Four HPC Tech Shorts dedicated to the work KEK did (yes, it&amp;rsquo;s that awesome).</description>
    </item>
    
    <item>
      <title>The Challenges of CryoEM with our friends from KEK in Japan (Part 1 of 4)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/the-challenges-of-cryoem-with-our-friends-from-kek-in-japan-part-1-of-4.html</link>
      <pubDate>Thu, 06 Oct 2022 15:04:35 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/the-challenges-of-cryoem-with-our-friends-from-kek-in-japan-part-1-of-4.html</guid>
      <description>CryoEM is an awesome research tool, but it also comes with some challenges. The software stack is complex, and the hardware it needs is sometimes exotic and hard to get. Afterwards, there&amp;rsquo;s a massive dataset and a serious set of challenges to moving it around, and managing it. (No one wants to get stuck baby sitting someone else&amp;rsquo;s petabyte dataset unless they have to).
The team at KEK in Japan took all these challenges head on and solved them in a very different way (spoiler: it involved the cloud). But as is often the case, when teams in Japan solve a big HPC problem, it&amp;rsquo;s usually something that&amp;rsquo;s going to get picked up around the world. And KEK are very much those kinds of people.
Today is one part in a series of Four HPC Tech Shorts dedicated to the work KEK did (yes, it&amp;rsquo;s that awesome).</description>
    </item>
    
    <item>
      <title>Understanding EC2 for HPC users</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/understanding-ec2-for-hpc-users.html</link>
      <pubDate>Thu, 29 Sep 2022 14:59:01 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/understanding-ec2-for-hpc-users.html</guid>
      <description>Amazon EC2 is the engine that powers HPC in the cloud. There&amp;rsquo;s quite a lot that&amp;rsquo;s new if you&amp;rsquo;ve never seen it before. in this Level 1 module in the Tech Shorts Foundations Series, we aim to demystify EC2 for the HPC community.
We&amp;rsquo;ll expand the number of videos in this series over time - but if there&amp;rsquo;s a topic that&amp;rsquo;s really interesting to you that we&amp;rsquo;re overlooking, don&amp;rsquo;t hesitate to contact us.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Welcome to tech Shorts Foundations</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/welcome-to-tech-shorts-foundations.html</link>
      <pubDate>Thu, 15 Sep 2022 15:45:34 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/welcome-to-tech-shorts-foundations.html</guid>
      <description>We get it: AWS has too much stuff going on for any reasonably busy human to catch up with. So we&amp;rsquo;re going back to the foundations of AWS to help the HPC community come up to speed on the stuff that matters to them most.
Today we&amp;rsquo;re starting a whole new series in Tech Shorts called Tech Shorts Foundations.
We&amp;rsquo;ll start high level and gradually peel back the layers so you can get as deep as you like in the areas you&amp;rsquo;re interested in the most.
Think of this as your map to navigate AWS and get to the HPC places quickly, without having to get lost on the way there. Your journey starts here.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Intro to AWS for HPC People - Tech Short Foundations Level 1</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/intro-to-aws-for-hpc-people-tech-short-foundations-level-1.html</link>
      <pubDate>Thu, 15 Sep 2022 15:35:56 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/intro-to-aws-for-hpc-people-tech-short-foundations-level-1.html</guid>
      <description>We get it: AWS has too much stuff going on for any reasonably busy human to catch up with. So we&amp;rsquo;re going back to the foundations of AWS to help the HPC community come up to speed on the stuff that matters to them most.
Today we&amp;rsquo;re starting a whole new series in Tech Shorts called Tech Shorts Foundations. We&amp;rsquo;ll still have shows about advanced topics, new features and performance analysis. But there&amp;rsquo;s now two tracks in Tech Shorts, which will develop over time.
We&amp;rsquo;ll start high level and gradually peel back the layers so you can get as deep as you like in the areas you&amp;rsquo;re interested in the most.
Think of this as your map to navigate AWS and get to the HPC places quickly, without having to get lost on the way there. Your journey starts here.</description>
    </item>
    
    <item>
      <title>Get your HPC codes installed and running in minutes using Spack&#39;s Binary Cache</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/get-your-hpc-codes-installed-and-running-in-minutes-using-spacks-binary-cache.html</link>
      <pubDate>Mon, 08 Aug 2022 18:28:02 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/get-your-hpc-codes-installed-and-running-in-minutes-using-spacks-binary-cache.html</guid>
      <description>Spack has already been removing the ugly work from building HPC codes, but with the announcement of the Spack Binary Cache at ISC&#39;22, build and deploy times for these complicated applications will drop by 95% or more in most cases.
Greg Becker from Livermore came along to show us how it works, and discuss what&amp;rsquo;s behind it.
You can find a blog post about the announcement here: hpc.news/binaryCache
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>New file systems support in ParallelCluster 3.2 (Part 1 of 2)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/new-file-systems-support-in-parallelcluster-32-part-1-of-2.html</link>
      <pubDate>Thu, 28 Jul 2022 14:07:47 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/new-file-systems-support-in-parallelcluster-32-part-1-of-2.html</guid>
      <description>ParallelCluster can now mount lots and lots of file systems that you&amp;rsquo;ve previously created in your AWS account, in addition to the scratch filesystem you can ask it to create for you when you launch your cluster. And as of today, ParallelCluster supports OpenZFS as one of those filesystems, along with Netapp ONTAP - which will help you get access to data on your enterprise filesystems, too.
Olly Perks and Austin Cherian describe all this in detail, as part 1 of a 2-part series covering the new features of ParallelCluster 3.2.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Memory aware scheduling with Slurm in ParallelCluster 3.2 (Part 2 of 2)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/memory-aware-scheduling-with-slurm-in-parallelcluster-32-part-2-of-2.html</link>
      <pubDate>Thu, 28 Jul 2022 14:07:45 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/memory-aware-scheduling-with-slurm-in-parallelcluster-32-part-2-of-2.html</guid>
      <description>If you&amp;rsquo;ve ever had to iteratively guess how much memory is left in a bunch of compute nodes in order to get your memory-hungry jobs running, then this feature will save your sanity.
It&amp;rsquo;s a new integration between ParallelCluster and Slurm that lets you specify how much RAM your jobs need, and gives Slurm the ability to figure out how to place your jobs in order to achieve that - not just counting cores, which is the default behavior for most schedulers.
Olly Perks and Austin Cherian describe this in detail, as part of a 2-part series covering the new features of ParallelCluster 3.2 (part 1 covered new file systems support and you can find it here: https://youtu.be/2JOoMv-K1FY).
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Clusters in the Cloud made easier with PCluster Manager (Part 2)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/clusters-in-the-cloud-made-easier-with-pcluster-manager-part-2.html</link>
      <pubDate>Tue, 19 Jul 2022 17:09:59 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/clusters-in-the-cloud-made-easier-with-pcluster-manager-part-2.html</guid>
      <description>In part 1, we covered all the aspects of designing and creating ParallelClusters in PCluster Manager. Today we delve into some more advanced topics like debugging your stack when something goes wrong, managing access to the cluster via the UI and CLI, and visualization, which is something that&amp;rsquo;s just super hard to do anywhere else.
Sean Smith shows us how this is all a lot easier with the right tools. if you want to install PCluster Manager in your account, head over to hpc.news/pclustermanager and deploy one of the one-click lauchable stacks to get going. There&amp;rsquo;s even an episode with Charlie (from our engineering team) showing how to install it and get it set up (it&amp;rsquo;s here: https://youtu.be/Z1vlpJYb1KQ).
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Clusters in the Cloud made easier with PCluster Manager (Part 1 of 2)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/clusters-in-the-cloud-made-easier-with-pcluster-manager-part-1-of-2.html</link>
      <pubDate>Thu, 14 Jul 2022 16:59:11 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/clusters-in-the-cloud-made-easier-with-pcluster-manager-part-1-of-2.html</guid>
      <description>Today is the first part of a 2-part series and we&amp;rsquo;re covering all the aspects of designing and creating ParallelClusters using PCluster Manager, which is a graphical user interface tool for making all the hard bits of cluster management way less messy and so much easier. All the documentation in the world isn&amp;rsquo;t as useful as this one tool.
In part 2, we&amp;rsquo;ll dive into some advanced topics like debugging your cluster build when something goes wrong, and we&amp;rsquo;ll show you some fancy access methods, and tools for managing jobs - and visualizing the results.
Sean Smith shows us how this is all a lot easier with the right tools. if you want to install PCluster Manager in your account, head over to hpc.news/pclustermanager and deploy one of the one-click lauchable stacks to get going. There&amp;rsquo;s even an episode with Charlie (from our engineering team) showing how to install it and get it set up (it&amp;rsquo;s here: https://youtu.</description>
    </item>
    
    <item>
      <title>NCCL on EFA makes the ML world go around in the cloud</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/nccl-on-efa-makes-the-ml-world-go-around-in-the-cloud.html</link>
      <pubDate>Thu, 30 Jun 2022 17:05:07 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/nccl-on-efa-makes-the-ml-world-go-around-in-the-cloud.html</guid>
      <description>Machine Learning is a huge workload, and one of the most demanding when it comes to scaling to thousands (and thousands) of CPUs. Some of the largest workloads customers run in the cloud are deep learning models, which require huge numbers of GPUs and saturate the networks connecting them.
To make all that work on AWS, NVIDIA&amp;rsquo;s collectives communications library (NCCL) relies on libfabrics to speak to the EFA hardware that makes up EC2&amp;rsquo;s high performance interconnect.
Rashika Kheria leads the team in Annapurna that handles this interface, ensuring your models, using all your favorite frameworks, scale really nicely to as far as your imagination allows (well, maybe a little further). She came to Tech Shorts to tell us how that works.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Bayesian models and half a million cores - what&#39;re you waiting for?</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/bayesian-models-and-half-a-million-cores-whatre-you-waiting-for.html</link>
      <pubDate>Fri, 24 Jun 2022 14:59:17 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/bayesian-models-and-half-a-million-cores-whatre-you-waiting-for.html</guid>
      <description>The Ampersand Data Science team had a challenge: their Bayesian statistical models needed more than half a million core-hours of runtime, regularly, if they were to get an answer fast enough for it to be useful to their customers.
Scaling to a million core or more isn&amp;rsquo;t really a challenge now (thanks to Amazon EC2). The hard part is all the code pipelines and plumbing - and the management of the entire thing when it&amp;rsquo;s in flight.
Daniel Gerlanc (Senior Director for Data Science) and Jeffrey Enos (Senior Machine Learning Engineer) swung by the Tech Shorts virtual watercooler to tell us how it worked, what was most surprising, and which bits made all the difference.
There&amp;rsquo;s also a blog that was posted last week which talks to some of this too. Worth a read: https://aws.amazon.com/blogs/hpc/bayesian-ml-models-at-scale-with-aws-batch/
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>New console features including container insights in AWS Batch</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/new-console-features-including-container-insights-in-aws-batch.html</link>
      <pubDate>Thu, 16 Jun 2022 17:12:05 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/new-console-features-including-container-insights-in-aws-batch.html</guid>
      <description>The AWS Batch team recently added container insights and advanced logging features to the Batch console, which is making a LOT of people very happy.
We decided to catch up with the dev team in Seattle who worked on this, and one of my favotire Amazonians - David Chambers - joined me to help us all understand what this means to Batch users and to taker us for a drive to sere how easy it is.
This is our first ever in-person Tech Short, which is something we&amp;rsquo;re really keen to build on, now that travel is starting to happen more. Hope you like it.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>The Arm64 developer environments - Part 2 of 2</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/the-arm64-developer-environments-part-2-of-2.html</link>
      <pubDate>Wed, 08 Jun 2022 15:15:36 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/the-arm64-developer-environments-part-2-of-2.html</guid>
      <description>Olly Perks - our resident Arm64 architecture expert and all round HPC software guy - walks us through the quite extensive support for HPC developers who are planning to work on AWS Graviton processors.
There&amp;rsquo;s a lot of software and it comes from a number of sources, including open source and commercial groups.
This is part 2 of a two-part series - the first part was published last week, abd you can find it here: https://youtu.be/qFrpmgvN9Xs
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>The Arm64 developer environments - Part 1 (of 2)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/the-arm64-developer-environments-part-1-of-2.html</link>
      <pubDate>Thu, 02 Jun 2022 07:48:02 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/the-arm64-developer-environments-part-1-of-2.html</guid>
      <description>Olly Perks - our resident Arm64 architecture expert and all round HPC software guy - walks us through the quite extensive support for HPC developers who are planning to work on AWS Graviton processors.
There&amp;rsquo;s a lot of software and it comes from a number of sources, including open source and commercial groups.
This is part 1 of a two-part series. The other part will be out early next week.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Winter Invitational Episode II - The Algorithms Strike Back</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/winter-invitational-episode-ii-the-algorithms-strike-back.html</link>
      <pubDate>Thu, 19 May 2022 15:42:17 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/winter-invitational-episode-ii-the-algorithms-strike-back.html</guid>
      <description>This is the second part of a two-part set about the Winter Invitational Student Cluster Competition, which the AWS HPC team supported as one of four mentor teams.
This isn&amp;rsquo;t just about the competition or the results. It&amp;rsquo;s about the skills that are important to getting a job, learning new things and solving tough problems. And it&amp;rsquo;s about working together with other people.
If you&amp;rsquo;re able to support these kinds of events, you should give it a try. it&amp;rsquo;s super rewarding.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Winter Invitational Episode I - New Hopefuls</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/winter-invitational-episode-i-new-hopefuls.html</link>
      <pubDate>Tue, 17 May 2022 13:09:07 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/winter-invitational-episode-i-new-hopefuls.html</guid>
      <description>This is the first part of a two-part set about the Winter Invitational Student Cluster Competition (which the AWS HPC team supported as one of four mentor teams). The competition targeted academic institutions that we need to succeed to bring some balance to the HPC force.
Dan Olds, the Chief Research Officer of Intersect360, and the unofficial historian (and historical figure) of Student Cluster Comps, joined us to walk us through the structure of the comp, and feed us some of the drama leading up to an extraordinary result in 2022.
This isn&amp;rsquo;t just about the competition or the results. It&amp;rsquo;s about the skills that are important to getting a job, learning new things and solving tough problems while working together with other people.
If you&amp;rsquo;re able to support these kinds of events, you should give it a try. it&amp;rsquo;s super rewarding.</description>
    </item>
    
    <item>
      <title>What makes the AWS Graviton 3 so interesting to HPC and AI/ML customers?</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/what-makes-the-aws-graviton-3-so-interesting-to-hpc-and-aiml-customers.html</link>
      <pubDate>Thu, 05 May 2022 16:35:59 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/what-makes-the-aws-graviton-3-so-interesting-to-hpc-and-aiml-customers.html</guid>
      <description>The AWS Graviton 3 will be launched this year as part of the new C7g instance. Inside this chip are some interesting innovations that already have a lot of HPC and AI/ML customers interested.
We sat down with Olly Perks, who recently joined AWS HPC Engineering from Arm, to discuss what&amp;rsquo;s most interesting. This is the first of a series of Tech Shorts covering the Graviton 3 architecture. We&amp;rsquo;ll deep dive on techniques and tools for getting the most out of these CPUs.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>YellowDog make scaling to crazy large workloads easy</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/yellowdog-make-scaling-to-crazy-large-workloads-easy.html</link>
      <pubDate>Fri, 29 Apr 2022 13:00:02 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/yellowdog-make-scaling-to-crazy-large-workloads-easy.html</guid>
      <description>YellowDog (yellowdog.co) cut their teeth in the media and entertainment industry working on orchestrating really large workloads for CGI and FX companies. They&amp;rsquo;ve generalized the platform and now they&amp;rsquo;re able to do the same thing for nearly any kind of workload in the cloud (or hybrid on-prem/cloud scenarios).
They understand the complexities of doing this, and have features and tooling for dealing with the nitty gritty (rationing licenses for ISV applications, data movement and staging etc etc).
Alan Parry, their Director of Engineering showed us how easy it is.
In the show, we mentioned a blog post, which you can find here: https://hpc.news/hawking901
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Protein folding (AlphaFold) at SCALE using a notebook and the cloud</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/protein-folding-alphafold-at-scale-using-a-notebook-and-the-cloud.html</link>
      <pubDate>Wed, 20 Apr 2022 13:33:52 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/protein-folding-alphafold-at-scale-using-a-notebook-and-the-cloud.html</guid>
      <description>Protein folding simulations can consume huge CPU time on supercomputers. AlphaFold changed that by applying ML techniques to the task. Then the problem became &amp;lsquo;how do I run AlphaFold easily?&amp;rsquo;.
The Healthcare AI/ML team at AWS figured that out, built a really accessible solution and open-sourced it to GitHub, which is where you can grab it today.
Brian Loyal and Ujjwal Ratan from the AI/ML specialist team in our global HCLS org joined us to help understand how this fits, why it works, and described some of the awesome science it supports.
They blogged about it here: https://hpc.news/curie264 The CloudFormation stack is here: https://github.com/aws-samples/aws-batch-architecture-for-alphafold
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Spooky-action-at-a-distance As a Service (Quantum Computing)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/spooky-action-at-a-distance-as-a-service-quantum-computing.html</link>
      <pubDate>Fri, 08 Apr 2022 16:16:10 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/spooky-action-at-a-distance-as-a-service-quantum-computing.html</guid>
      <description>Amazon Braket is AWS&amp;rsquo;s Quantum Computing service which gives you access to several Quantum Computers using a serverless programming model and &amp;hellip; YOU ACTUALLY GET TO USE REAL QUANTUM COMPUTERS without needing a $50M budget and a cryo facility.
This is therefore one of the weirdest Tech Shorts we&amp;rsquo;ve done. We even run a Quantum version of &amp;lsquo;Hello, world&amp;rsquo;. No kidding.
You can get more information about the service at aws.amazon.com/braket
But : why not try it yourself?
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Controlling hybrid workflows between on-premises and cloud with EnginFrame</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/controlling-hybrid-workflows-between-on-premises-and-cloud-with-enginframe.html</link>
      <pubDate>Thu, 31 Mar 2022 20:54:58 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/controlling-hybrid-workflows-between-on-premises-and-cloud-with-enginframe.html</guid>
      <description>EnginFrame makes life easy for scientists and engineers so they can use HPC resources without having to understand the complexity.
BUT: EnginFrame also makes the local sysadmin a hero by giving them the ability to embed into simple scripts the decisions that lead to determining how and where and when a job gets run.
The use cases are almost endless:
decide whether your job bursts to the cloud based on congestion conditions in your on&amp;ndash;prem queues figure out if data needs to be moved first before a job can be run decide based on job parameters whether a job is fit for execution in the AWS spot market, or should be run on-prem, or run in on-demand queues. The workshop we discussed is here: https://hpc.news/efConnectorWorkshop
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Genomics workflow set made easy with AWS Genomics CLI</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/genomics-workflow-set-made-easy-with-aws-genomics-cli.html</link>
      <pubDate>Fri, 25 Mar 2022 11:29:03 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/genomics-workflow-set-made-easy-with-aws-genomics-cli.html</guid>
      <description>The AWS Genomics CLI (or AGC) seriously removes all the grunt work involved in setting up bioinformatics pipelines to run in the cloud. We know that Snakemake, Cromwell, NextFlow and miniWDL all work happily in the cloud on AWS Batch, but AGC means you don&amp;rsquo;t have to know to set all that stuff up - it does it for you.
You can have completely separate tool chains using completely different workflow languages all running at the same time, on the same infrastructure (if you like), sharing data and tooling.
Lee Pang from the dev team that built this came along to show us how it works, and - most importantly - how easy it is to get productive. Zero to hero in less than 30 mins - it&amp;rsquo;s really impressive.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Multi-user in ParallelCluster with LDAP and MS Active Directory</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/multi-user-in-parallelcluster-with-ldap-and-ms-active-directory.html</link>
      <pubDate>Thu, 17 Mar 2022 15:34:44 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/multi-user-in-parallelcluster-with-ldap-and-ms-active-directory.html</guid>
      <description>Customers have been asking us to make it easy to configure ParallelCluster to use a corporate directory server, so adding a user to the cloud HPC stack becomes as easy as adding them to (say) the Engineering Group in Active Directory or LDAP.
We&amp;rsquo;re happy to say we&amp;rsquo;ve done that, and released a blog about it a couple weeks ago. In today&amp;rsquo;s show, Giacomo Marciani (one of the developers who built this feature) came along to explain how it works and give us a demo, so you can get to the important steps fast.
During the show we spoke about a blog and a tutorial. They&amp;rsquo; right here:
Blog: https://hpc.news/turing313 Tutorial: https://hpc.news/hawking513 If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>OpenZFS created quickly with checkpoints, backups and big performance</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/openzfs-created-quickly-with-checkpoints-backups-and-big-performance.html</link>
      <pubDate>Thu, 10 Mar 2022 15:47:17 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/openzfs-created-quickly-with-checkpoints-backups-and-big-performance.html</guid>
      <description>We launched Amazon FSx for OpenZFS at re:invent in December, and well let&amp;rsquo;s just say it&amp;rsquo;s been more than a bit popular :-)
ZFS and it&amp;rsquo;s friends have been a long time favorite of the HPC community, so we asked Delwin Olivan, the Snr Product Manager for the service to come and show us how easy it is to get big scale, and big performance with very little effort.
Auto-backups and one-click snapshots are exactly as they&amp;rsquo;re described on the lid. Which is awesome.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Scaling CFD a lot by breaking down a workflow, to speed things up</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/scaling-cfd-a-lot-by-breaking-down-a-workflow-to-speed-things-up.html</link>
      <pubDate>Thu, 03 Mar 2022 17:26:49 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/scaling-cfd-a-lot-by-breaking-down-a-workflow-to-speed-things-up.html</guid>
      <description>Sometimes when we&amp;rsquo;re looking for performance we lose the forrest for all those trees - we miss the huge improvements we can do to unglamorous parts of the overall workflow, while we obsess on the pieces that look hard. It&amp;rsquo;s an engineer thing, I think.
In today&amp;rsquo;s Tech Short, Neil Ashton shows us exactly this kind of example from the world of CFD (using OpenFOAM, but this lesson applies generally) - and shows us how to break the problem down in order to speed it all up - and pretty easily, too.
The workshops we reference in the discussion are all listed here: https://aws.amazon.com/hpc/cfd/
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Using Lustre to build very fast file systems with Amazon Fsx for Lustre</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/using-lustre-to-build-very-fast-file-systems-with-amazon-fsx-for-lustre.html</link>
      <pubDate>Thu, 24 Feb 2022 15:01:26 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/using-lustre-to-build-very-fast-file-systems-with-amazon-fsx-for-lustre.html</guid>
      <description>The Amazon Fsx for Lustre team have managed to turn what used to be a multi-year process to buy and build a Lustre filesystem, into a simple process with a launch button on the front.
It&amp;rsquo;s hard to overstate the pain that this saves, but the most interesting things to ponder are the consequences of it. Jordan Dolman from the FSx Lustre team shows us some of the more unusual things you can do with Fsx Lustre that you just couldn&amp;rsquo;t think about doing any other way.
Storage is one of the foundational layers we covered in the AWS HPC Speeds&amp;rsquo;n&amp;rsquo;Feeds event in early Feb 2022. if you missed it, head over to https://hpc.news/SpeedsFeeds to watch the replay.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>A new GUI for building and managing clusters - PCluster Manager</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/a-new-gui-for-building-and-managing-clusters-pcluster-manager.html</link>
      <pubDate>Wed, 16 Feb 2022 15:00:39 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/a-new-gui-for-building-and-managing-clusters-pcluster-manager.html</guid>
      <description>Charlie Gruenweld, our newest Principal Engineer in the HPC org, took advantage of the new API in ParallelCluster and a few dozen years of web development revolutions to build a snappy UI for creating and managing clusters that makes it nearly impossible to go wrong - and gives you immediate access to some of ParallelCluster&amp;rsquo;s most powerful features.
In less than 20 minutes, you&amp;rsquo;ll grok how to create a cluster, run jobs, spawn elastic (disposable) nodes, and jump into a full graphical desktop to run visual applications on the cluster itself.
This is the project that brings it all together. I hope you enjoy this as much as I did.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>How to explore Slurm&#39;s job management API from a Python notebook - (Part 5)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/how-to-explore-slurms-job-management-api-from-a-python-notebook-part-5.html</link>
      <pubDate>Thu, 27 Jan 2022 16:48:25 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/how-to-explore-slurms-job-management-api-from-a-python-notebook-part-5.html</guid>
      <description>Slurm&amp;rsquo;s job management API, coupled with ParallelCluster&amp;rsquo;s own API for managing cluster infrastructure opens up a lot of doors to create new ways for your users to interact with HPC resources - in this case without having to leave their familiar Jupyter notebook environment.
We made this video as part of the launch of ParallelCluster 3 - we want to make it easy to migrate all your workflows from to Slurm, and we figured it would be easier if you heard abotu it from the source.
So we enlisted the help of SchedMD&amp;rsquo;s Director of Cloud Engineering, Nick Ihli to help us show you how it&amp;rsquo;s a lot easier than it might look.
Joining Nick today is Josiah Bjorgaard from the AWS APN Solution Architecture team, who regularly works with Nick and the team from SchedMD to solve lots of customer problems.</description>
    </item>
    
    <item>
      <title>ParallelCluster 3 - Launch and Cluster Operations</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/parallelcluster-3-launch-and-cluster-operations.html</link>
      <pubDate>Thu, 20 Jan 2022 15:11:08 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/parallelcluster-3-launch-and-cluster-operations.html</guid>
      <description>In a previous show (https://youtu.be/6gAwAK5IJ2w), we talked about &amp;lsquo;Infrastructure as Code&amp;rsquo;, or how the ParallelCluster 3 YAML config translated to an actual cluster running, ready to take jobs.
Today, Austin Cherian joins us again to walk through standing up the cluster using that config, and how we can adjust the cluster on the fly if, for example, we needed to add some new node types into the compute fleets, like if we need some fat memory nodes, or a new GPU type.
The reference guide for doing dynamic updates is here: https://docs.aws.amazon.com/parallelcluster/latest/ug/using-pcluster-update-cluster-v3.html
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>HPC on AWS just got faster and lower-cost with the launch of Hpc6a</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/hpc-on-aws-just-got-faster-and-lower-cost-with-the-launch-of-hpc6a.html</link>
      <pubDate>Thu, 13 Jan 2022 16:00:31 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/hpc-on-aws-just-got-faster-and-lower-cost-with-the-launch-of-hpc6a.html</guid>
      <description>On Jan 10, the new Hpc6a instance became generally available in two regions. This instance is built from the ground up for HPC and comes with a number of interesting innovations. Neil and Heidi from our application performance team came along to show results from real analyses of real codes running at tens of thousands of cores, with some of the hardest applications on Earth.
The instance details are here: https://aws.amazon.com/ec2/instance-types/hpc6/
&amp;hellip; and the blog post talking about them is here: https://hpc.news/Hpc6aLaunch
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Make your HPC users highly productive using EnginFrame with AWS HPC Connector</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/make-your-hpc-users-highly-productive-using-enginframe-with-aws-hpc-connector.html</link>
      <pubDate>Tue, 11 Jan 2022 15:53:44 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/make-your-hpc-users-highly-productive-using-enginframe-with-aws-hpc-connector.html</guid>
      <description>In December, we introduced EnginFrame with AWS HPC Connector, which is a new feature in EnginFrame to help customers leverage managed HPC resources in AWS as well as their on-prem systems - that means a single interface so administrators can make easy to use workflows available no matter where they live.
It means highly specialized users like scientists and engineers can use EnginFrame’s portal interface to run their important workflows without having to understand the detailed operation of infrastructure underneath. HPC is, after all, a tool used by humans. Their productivity is the real measure of success, and we think AWS HPC Connector will make a big difference to them.
In this talk, Fabrizio Chignoli (who leads the EnginFrame dev team) shows us how it all works and what the end user experience looks like.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Recap on all the HPC developments from re:invent 21</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/recap-on-all-the-hpc-developments-from-reinvent-21.html</link>
      <pubDate>Tue, 14 Dec 2021 16:00:02 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/recap-on-all-the-hpc-developments-from-reinvent-21.html</guid>
      <description>reInvent &amp;lsquo;21 is over, but it was a busy week with new tech developments we&amp;rsquo;ve built for customers in pretty much every area of HPC: storage, networking and compute.
We talk with our HPC General Manager, Ian Colle, about ZFS, Lustre, Graviton3, DDR5, EFA at 400 Gb/s on dozens of instances &amp;hellip; and a lot more.
If you didn&amp;rsquo;t get to reinvent, or even if you did and couldn&amp;rsquo;t be in 10 places at once, this is a compact recap for you.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>What supercomputers, scientists and TV stations have in common</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/what-supercomputers-scientists-and-tv-stations-have-in-common.html</link>
      <pubDate>Thu, 09 Dec 2021 16:19:43 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/what-supercomputers-scientists-and-tv-stations-have-in-common.html</guid>
      <description>CDI (Cloud Digital Interface) uses EFA to blast broadcast quality video, uncompressed, from instance to instance in the cloud as live video feeds its way through complex stacks of software and makes its way to your TV set.
It&amp;rsquo;s one of the more unusual use cases we&amp;rsquo;ve heard for a technology that we invented for keeping highly parallel scientific applications in sync when running in HPC environments in the cloud. Yes, that&amp;rsquo;s weird.
Evan Statton from our media and entertainment tech team talks about how this happened and why it&amp;rsquo;s so useful.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>miniWDL workflows with 100% cloud elasticity, and no DevOps geekery</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/miniwdl-workflows-with-100-cloud-elasticity-and-no-devops-geekery.html</link>
      <pubDate>Thu, 02 Dec 2021 16:42:11 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/miniwdl-workflows-with-100-cloud-elasticity-and-no-devops-geekery.html</guid>
      <description>Mike Lin is the creator and maintainer of miniWDL and has been working with our Genomics and ML teams to integrate it with Sagemaker Studio and AWS Batch. The result is a working environment that&amp;rsquo;s completely familiar and easy to use (it&amp;rsquo;s Jupyter, after all) and yet has access to all the elasticity of the cloud, but without ever needing to leave the familiar notebook interface to make that magic happen.
It&amp;rsquo;s a really cool example of bringing the tools to science, rather than forcing the scientists to understand yet another new tool.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Fair share scheduling to maximize user happiness in AWS Batch</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/fair-share-scheduling-to-maximize-user-happiness-in-aws-batch.html</link>
      <pubDate>Wed, 24 Nov 2021 17:55:32 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/fair-share-scheduling-to-maximize-user-happiness-in-aws-batch.html</guid>
      <description>If you’re a frequent user of Batch queues, you have opinions about whether it’s scheduling is “fair” on your users.
But &amp;lsquo;fair&amp;rsquo; means different things to everyone - what’s needed are some levers and dials that allow you to figure out what ‘fair’ means for your organization.
AWS Batch now has Fair Share Scheduling, and it comes with a whole lot of controls.
We’re joined this week on Tech Shorts by Aswin Damodar from the Batch engineering team, and Christian Kniep, our Snr Dev Advocate for HPC &amp;amp; Batch, to run through all these controls and what they mean.
I’m betting this video is going to get watched a LOT. Fair-share scheduling is a complex topic, but the discussion makes it a lot easier to understand that reading the documentation probably will the first time.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Easy migration from SGE to Slurm - Part 4 Slurm Accounting</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/easy-migration-from-sge-to-slurm-part-4-slurm-accounting.html</link>
      <pubDate>Thu, 18 Nov 2021 11:31:59 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/easy-migration-from-sge-to-slurm-part-4-slurm-accounting.html</guid>
      <description>As part of the launch of ParallelCluster 3, we want to make it easy to migrate all your workflows from to Slurm, but we know that it can seem daunting. So we enlisted the help of SchedMD&amp;rsquo;s Director of Cloud Engineering, Nick Ihli to help us show you how it&amp;rsquo;s a lot easier than it might look.
(Seriously, you may need to change very little).
Over the next 5 x Tech Shorts we&amp;rsquo;ll show:
Part 1) Command line syntax and which Slurm commands map to familiar SGE ones. Part 2) Job Scripts - what&amp;rsquo;s the stuff you need to care about to adjust ytour scripts? What&amp;rsquo;s actually easier with Slurm? Part 3) Array Jobs - Slurm has a really elegant way to handle array jobs, and also has some really nice SGE-like commands that will fool you into thinking nothing actually changed.</description>
    </item>
    
    <item>
      <title>Easy migration from SGE to Slurm - Part 3 - Array Jobs</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/easy-migration-from-sge-to-slurm-part-3-array-jobs.html</link>
      <pubDate>Thu, 11 Nov 2021 17:54:45 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/easy-migration-from-sge-to-slurm-part-3-array-jobs.html</guid>
      <description>As part of the launch of ParallelCluster 3, we want to make it easy to migrate all your workflows from to Slurm, but we know that it can seem daunting. So we enlisted the help of SchedMD&amp;rsquo;s Director of Cloud Engineering, Nick Ihli to help us show you how it&amp;rsquo;s a lot easier than it might look.
(Seriously, you may need to change very little).
Over the next 5 x Tech Shorts we&amp;rsquo;ll show:
Part 1) Command line syntax and which Slurm commands map to familiar SGE ones. Part 2) Job Scripts - what&amp;rsquo;s the stuff you need to care about to adjust ytour scripts? What&amp;rsquo;s actually easier with Slurm? Part 3) Array Jobs - Slurm has a really elegant way to handle array jobs, and also has some really nice SGE-like commands that will fool you into thinking nothing actually changed.</description>
    </item>
    
    <item>
      <title>Easy migration from SGE to Slurm - Part 2 - Job Scripts</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/easy-migration-from-sge-to-slurm-part-2-job-scripts.html</link>
      <pubDate>Tue, 09 Nov 2021 16:34:10 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/easy-migration-from-sge-to-slurm-part-2-job-scripts.html</guid>
      <description>As part of the launch of ParallelCluster 3, we want to make it easy to migrate all your workflows from to Slurm, but we know that it can seem daunting. So we enlisted the help of SchedMD&amp;rsquo;s Director of Cloud Engineering, Nick Ihli to help us show you how it&amp;rsquo;s a lot easier than it might look.
(Seriously, you may need to change very little).
Over the next 5 x Tech Shorts we&amp;rsquo;ll show:
Part 1) Command line syntax and which Slurm commands map to familiar SGE ones. Part 2) Job Scripts - what&amp;rsquo;s the stuff you need to care about to adjust ytour scripts? What&amp;rsquo;s actually easier with Slurm? Part 3) Array Jobs - Slurm has a really elegant way to handle array jobs, and also has some really nice SGE-like commands that will fool you into thinking nothing actually changed.</description>
    </item>
    
    <item>
      <title>Easy migrating from SGE to Slurm - Part 1 - Command Line Tools</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/easy-migrating-from-sge-to-slurm-part-1-command-line-tools.html</link>
      <pubDate>Thu, 04 Nov 2021 15:36:14 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/easy-migrating-from-sge-to-slurm-part-1-command-line-tools.html</guid>
      <description>As part of the launch of ParallelCluster 3, we want to make it easy to migrate all your workflows from to Slurm, but we know that it can seem daunting. So we enlisted the help of SchedMD&amp;rsquo;s Director of Cloud Engineering, Nick Ihli to help us show you how it&amp;rsquo;s a lot easier than it might look.
(Seriously, you may need to change very little).
Over the next 5 x Tech Shorts we&amp;rsquo;ll show:
Part 1) Command line syntax and which Slurm commands map to familiar SGE ones. Part 2) Job Scripts - what&amp;rsquo;s the stuff you need to care about to adjust ytour scripts? What&amp;rsquo;s actually easier with Slurm? Part 3) Array Jobs - Slurm has a really elegant way to handle array jobs, and also has some really nice SGE-like commands that will fool you into thinking nothing actually changed.</description>
    </item>
    
    <item>
      <title>CFD performance on Ice Lake CPU with the Amazon EC2 C6i (Part 2)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/cfd-performance-on-ice-lake-cpu-with-the-amazon-ec2-c6i-part-2.html</link>
      <pubDate>Tue, 02 Nov 2021 14:00:09 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/cfd-performance-on-ice-lake-cpu-with-the-amazon-ec2-c6i-part-2.html</guid>
      <description>Today we launched the new Amazon EC2 C6i instance family which is powered by the Intel Xeon Ice Lake processor and comes equipped with our Elastic Fabric Adapter.
Neil Ashton and Nicola Venuti joined us to talk about CFD performance on this new instance family, and spent some time comparing network connectivity options, too.
This is a two part series of Tech Shorts:
Part 1) Discuss C6i and how it&amp;rsquo;s put together. Look at OpenFOAM and Siemens Simcenter StarCCM+ Part 2) Ansys Fluent, under populated cores, and some on-premises comparisons for calibration.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>CFD performance on Ice Lake CPU with the Amazon EC2 C6i (Part 1)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/cfd-performance-on-ice-lake-cpu-with-the-amazon-ec2-c6i-part-1.html</link>
      <pubDate>Fri, 29 Oct 2021 10:48:01 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/cfd-performance-on-ice-lake-cpu-with-the-amazon-ec2-c6i-part-1.html</guid>
      <description>Today we launched the new Amazon EC2 C6i instance family which is powered by the Intel Xeon Ice Lake processor and comes equipped with our Elastic Fabric Adapter.
Neil Ashton and Nicola Venuti joined us to talk about CFD performance on this new instance family, and spent some time comparing network connectivity options, too.
This is a two part series of Tech Shorts:
Part 1) Discuss C6i and how it&amp;rsquo;s put together. Look at OpenFOAM and Siemens Simcenter StarCCM+ Part 2) Ansys Fluent, under populated cores, and some on-premises comparisons for calibration.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>How to deconstruct FSI Grid scheduling and mapping it onto AWS services</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/how-to-deconstruct-fsi-grid-scheduling-and-mapping-it-onto-aws-services.html</link>
      <pubDate>Thu, 21 Oct 2021 16:05:26 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/how-to-deconstruct-fsi-grid-scheduling-and-mapping-it-onto-aws-services.html</guid>
      <description>Last month we talked about the evolution of HPC workloads in the financial world. We agreed with Alex Kimber, our global FSI HPC expert, to come back and talk about how to deconstruct these very complex, high-speed, task scheduling environments and map them onto AWS services. This is cool because it lets us scale them even further in a bunch of new ways. it also means we can solve some tricky organizational problems (like sharing infra and cross-charging between depts) without any fuss at all.
Alex dives into this area in today&amp;rsquo;s discussion.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Getting started with bioinformatics on AWS with Swaine Chen from GIS</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/getting-started-with-bioinformatics-on-aws-with-swaine-chen-from-gis.html</link>
      <pubDate>Fri, 15 Oct 2021 13:01:30 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/getting-started-with-bioinformatics-on-aws-with-swaine-chen-from-gis.html</guid>
      <description>Swaine Chen had a problem: his new group at the Genomics Institute of Singapore needed common tooling involving zillions of utilities, scripts carrying algorithms and techniques, so they could migrate their workloads from their on-premises infrastructure and over to the cloud.
His solution was to build a golden AMI. So much better is that he automated the production of the AMI, and documented it in GitHub, for the world to see, and for the world to use.
Today&amp;rsquo;s discussion shows just a glimpse of what his team has built, and you can get all the goodness in the hands-on training workshops he&amp;rsquo;s offering (for free) to anyone in the world who wants to sign up.
If you want to know more about the workshops, or sign up, go here: https://hpc.news/giswsregister
If you want to plunder all this goodness and use it for the forces of good (not evil!</description>
    </item>
    
    <item>
      <title>Infrastructure as Code - ParallelCluster 3&#39;s config</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/infrastructure-as-code-parallelcluster-3s-config.html</link>
      <pubDate>Thu, 07 Oct 2021 13:56:05 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/infrastructure-as-code-parallelcluster-3s-config.html</guid>
      <description>&amp;lsquo;Infrastructure as Code&amp;rsquo; has a weird meaning in HPC, because it says we can write a script which stands up the entire incredibly complex software stack that is an HPC cluster, complete with MPIs, math libraries, schedulers - and even a parallel file system and visualization server.
Austin Cherian takes us for a walk through the new ParallelCluster 3 config file and shows how the syntax aligns with the significant elements of the cluster architecture. He also talks about a few quirks we found along the way.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>Launch a machine in RONIN</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/launch-a-machine-in-ronin.html</link>
      <pubDate>Thu, 30 Sep 2021 15:40:16 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/launch-a-machine-in-ronin.html</guid>
      <description>RONIN is probably the easiest way for a research scientist or engineer to be able to quickly jump into the cloud on AWS and get stuff done. And they&amp;rsquo;ve made a regular habit of delighting their customers by making hard things easy.
In this Tech Short, Tara from RONIN shows us how to spin up a Linux machine and connect to it securely. All of this happens inside the safety and governance of RONIN&amp;rsquo;s security posture and it&amp;rsquo;s budget management guardrails.
More on all of this is available at www.ronin.cloud.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>HPC in Financial Services is not boring, and has some interesting problems to solve.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/hpc-in-financial-services-is-not-boring-and-has-some-interesting-problems-to-solve.html</link>
      <pubDate>Thu, 23 Sep 2021 15:53:01 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/hpc-in-financial-services-is-not-boring-and-has-some-interesting-problems-to-solve.html</guid>
      <description>HPC in Financial Services is interesting for a lot of reasons that have nothing to do with banking. It&amp;rsquo;s an embarrassingly parallel workload, typically made up from zillions of short&amp;rsquo;ish jobs (seconds to minutes). And the decisions it supports are pretty big ones: whole banks and entire economies might be impacted by the outcomes. It&amp;rsquo;s interesting because there are so many fun ways you can solve the problem of &amp;lsquo;get it done fast, cheaply, and soon enough to help the people who are making really big decisions&amp;rsquo;. And whilst you expect us to say this: the cloud truly is a real game changer for this workload.
Alex Kimber wrote the HPC book at AWS for Financial Services, but what he talks about in this short discussion puts context around the whole thing &amp;hellip; He&amp;rsquo;ll be back to expand on several of the key points in a few weeks.</description>
    </item>
    
    <item>
      <title>Governance and guardrails for researchers with RONIN ISOLATE</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/governance-and-guardrails-for-researchers-with-ronin-isolate.html</link>
      <pubDate>Wed, 15 Sep 2021 15:50:26 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/governance-and-guardrails-for-researchers-with-ronin-isolate.html</guid>
      <description>RONIN (www.ronin.cloud) is one of our favorite HPC partners, because they&amp;rsquo;ve done a really comprehensive job providing research customers the governance and guardrails they need around budget management and data security to make insanely easy to explore and experiment with workloads in the cloud. It helps that they pack hundreds of DevOps tasks into every click so you can launch HPC clusters in less time than it takes to buy your groceries online.
Tara Madhyastha is one of RONIN&amp;rsquo;s Principal Research Scientists based out of Seattle, and she gives us a peek at a new product built on RONIN Core called &amp;lsquo;RONIN Isolate&amp;rsquo;, which takes secure enclaves and project isolation to an obsessive new level to help customers with really extreme (and particular) security and compliance needs.
Isolate launched this week, and you can find more about it at the RONIN Blo at https://blog.</description>
    </item>
    
    <item>
      <title>Introducing ParallelCluster 3 - HPC in the Cloud built by customers</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/introducing-parallelcluster-3-hpc-in-the-cloud-built-by-customers.html</link>
      <pubDate>Fri, 10 Sep 2021 16:19:34 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/introducing-parallelcluster-3-hpc-in-the-cloud-built-by-customers.html</guid>
      <description>Today we’re announcing AWS ParallelCluster 3.
Customers, integrators, and other builders have told us they want to build end-to-end “recipes” for HPC, spanning the whole gamut from infrastructure to middleware, libraries, and runtime codes. They also asked to interact with ParallelCluster programmatically to create interfaces and services for their users. We worked backwards from this feedback, using thousands of conversations with customers to create a shiny new version of ParallelCluster rebuilt from the ground up.
In 15 minutes, we walk through the main features with Nathan Stornetta our Snr Product Manager for ParallelCluster, who also explains what customer feedback lead to each idea.
This is a big release with loads of new features. But the most exciting part is what it sets us up for next.
You can find out more about this in our launch blog at: http://hpc.news/pc3day1 and also by checking out the workshop and documentation that are linked to in the bottom of that blog post.</description>
    </item>
    
    <item>
      <title>What&#39;s the impact of DCV&#39;s pixel streaming on my AWS Bill?</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/whats-the-impact-of-dcvs-pixel-streaming-on-my-aws-bill.html</link>
      <pubDate>Thu, 02 Sep 2021 15:28:09 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/whats-the-impact-of-dcvs-pixel-streaming-on-my-aws-bill.html</guid>
      <description>Since our NICE DCV high performance desktop and application streaming product does such a great job of making it feel like you have a cloud dat center sitting behind your laptop screen somewhere, lots of customers have asked &amp;lsquo;Won&amp;rsquo;t that impact my bill? You guys charge for data, right?&amp;rsquo;.
There&amp;rsquo;s no way to give a precise answer, but what we did do was to put together a range of usage scenarios from fairly pedestrian usage (doing work processing on a remote windows machine) through to high bit-rate gaming and video streaming. We tested them all and measured their consumption. And we show you the results. Bottom line: data charges are&amp;rsquo;t really going to nudge your billing by much unless you&amp;rsquo;re getting close to 4K video streaming.
Jyothi Venkatesh and Boof put DCV to the test, and we walk you through the scenarios and our analysis.</description>
    </item>
    
    <item>
      <title>How to Spack a software package</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/how-to-spack-a-software-package.html</link>
      <pubDate>Thu, 26 Aug 2021 17:26:42 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/how-to-spack-a-software-package.html</guid>
      <description>This is a talk that was delivered at the AWS/Arm Cloud Hackathon in July 2021.
Todd Gamblin (the creator of Spack) and his colleague Greg Becker talk us through the essential skills and concepts needed to understand how to create and deploy Spack recipes to build scientific codes. Spack massively simplifies the task of building scientific applications, which are almost defined by their insane build methods and dependency hierarchies. We made extensive use of Spack in the hackathon, and were extremely grateful for their help.
The Summer Hackathon ran for a week from July 12-16 in 2021. It&amp;rsquo;s purpose was to assemble the HPC community around a common goal of beginning the porting/testing/tuning process for dozens of codes to use Arm-based processors, in this case, AWS&amp;rsquo;s Graviton2&amp;rsquo;s.
During the week we had a series of talks from global leaders and experts in various fields from compilers to networking, profiling debugging and tooling.</description>
    </item>
    
    <item>
      <title>Arm/AWS Cloud Hackathon - Compilers in HPC, their use and abuse</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/armaws-cloud-hackathon-compilers-in-hpc-their-use-and-abuse.html</link>
      <pubDate>Thu, 12 Aug 2021 15:00:23 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/armaws-cloud-hackathon-compilers-in-hpc-their-use-and-abuse.html</guid>
      <description>This is a talk that was delivered at the AWS/Arm Cloud Hackathon in July 2021.
Will Lovett works for Arm on the Arm compiler, and talk about how compilers work, and how to best leverage them to get the result you&amp;rsquo;re looking for when porting a code or working on performance.
The Summer Hackathon ran for a week from July 12-16 in 2021. It&amp;rsquo;s purpose was to assemble the HPC community around a common goal of beginning the porting/testing/tuning process for dozens of codes to use Arm-based processors, in this case, AWS&amp;rsquo;s Graviton2&amp;rsquo;s.
During the week we had a series of talks from global leaders and experts in various fields from compilers to networking, profiling debugging and tooling. We&amp;rsquo;ve published all the talks here in the HPC Tech Shorts channel, in the hope that everyone gets some value from them.</description>
    </item>
    
    <item>
      <title>Arm/AWS Cloud Hackathon - Performance Portability with Tom Deakin</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/armaws-cloud-hackathon-performance-portability-with-tom-deakin.html</link>
      <pubDate>Tue, 10 Aug 2021 15:00:22 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/armaws-cloud-hackathon-performance-portability-with-tom-deakin.html</guid>
      <description>This is a talk that was delivered at the AWS/Arm Cloud Hackathon in July 2021.
Tom Deakin is a computer scientist at the University of Bristol, and speaks to us about how to build for portability, and what that really means in an era of so many hardware technology choices.
The Summer Hackathon ran for a week from July 12-16 in 2021. It&amp;rsquo;s purpose was to assemble the HPC community around a common goal of beginning the porting/testing/tuning process for dozens of codes to use Arm-based processors, in this case, AWS&amp;rsquo;s Graviton2&amp;rsquo;s.
During the week we had a series of talks from global leaders and experts in various fields from compilers to networking, profiling debugging and tooling. We&amp;rsquo;ve published all the talks here in the HPC Tech Shorts channel, in the hope that everyone gets some value from them.
If you want to join in our collective effort to boost the performance of HPC workloads on Arm, DM us at Twitter (@TechHpc) or head to a-hug.</description>
    </item>
    
    <item>
      <title>Arm/AWS Cloud Hackathon - Profiling without printf()</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/armaws-cloud-hackathon-profiling-without-printf.html</link>
      <pubDate>Thu, 05 Aug 2021 15:00:23 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/armaws-cloud-hackathon-profiling-without-printf.html</guid>
      <description>This is a talk that was delivered at the AWS/Arm Cloud Hackathon in July 2021.
John Linford is Arm&amp;rsquo;s director of HPC and in this talk he covers why you should evolve your profiling beyond using print(), and how much time and effort this can save you. He also dives into some reasons to avoid premature optimization.
The Summer Hackathon ran for a week from July 12-16 in 2021. It&amp;rsquo;s purpose was to assemble the HPC community around a common goal of beginning the porting/testing/tuning process for dozens of codes to use Arm-based processors, in this case, AWS&amp;rsquo;s Graviton2&amp;rsquo;s.
During the week we had a series of talks from global leaders and experts in various fields from compilers to networking, profiling debugging and tooling. We&amp;rsquo;ve published all the talks here in the HPC Tech Shorts channel, in the hope that everyone gets some value from them.</description>
    </item>
    
    <item>
      <title>Arm/AWS Cloud Hackathon Talk - Application Scaling with Jeff Hammond</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/armaws-cloud-hackathon-talk-application-scaling-with-jeff-hammond.html</link>
      <pubDate>Tue, 03 Aug 2021 15:00:07 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/armaws-cloud-hackathon-talk-application-scaling-with-jeff-hammond.html</guid>
      <description>This is a talk that was delivered at the AWS/Arm Cloud Hackathon in July 2021.
Jeff Hammond is a computational scientist at NVIDIA and walks through techniques for getting applications to scale in HPC.
The Summer Hackathon ran for a week from July 12-16 in 2021. It&amp;rsquo;s purpose was to assemble the HPC community around a common goal of beginning the porting/testing/tuning process for dozens of codes to use Arm-based processors, in this case, AWS&amp;rsquo;s Graviton2&amp;rsquo;s.
During the week we had a series of talks from global leaders and experts in various fields from compilers to networking, profiling debugging and tooling. We&amp;rsquo;ve published all the talks here in the HPC Tech Shorts channel, in the hope that everyone gets some value from them.
If you want to join in our collective effort to boost the performance of HPC workloads on Arm, DM us at Twitter (@TechHpc) or head to a-hug.</description>
    </item>
    
    <item>
      <title>How Netflix used DCV (and AWS) to distribute their creative workforce (and saved our sanity)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/how-netflix-used-dcv-and-aws-to-distribute-their-creative-workforce-and-saved-our-sanity.html</link>
      <pubDate>Thu, 29 Jul 2021 15:00:22 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/how-netflix-used-dcv-and-aws-to-distribute-their-creative-workforce-and-saved-our-sanity.html</guid>
      <description>Netflix has a really ambitious goal to create more content than practically anyone, but ran into headwinds when Covid-19 forced us all to lock down: it became hard to recruit and support artists, editors and other creatives without finding some really clever solutions to putting powerful compute at their fingertips anywhere in the world : a perfect job for AWS&amp;rsquo;s NICE DCV and globally-distributed infrastructure.
But there&amp;rsquo;s another lesson here - when you listen to how Michelle describes the way Netflix works to support their most valuable assets (clever, talented people), it should strike you as a really good lesson for how we should prosecute our mission to make scientists and engineers more powerful and productive with all these same tools.
You can find Michelle on linkedIn (linkedin.com/in/michellebrenner/) if you want to know more, or you can join her working at Netflix by heading to job.</description>
    </item>
    
    <item>
      <title>Nextflow Tower  and how it makes it easy to manage a lot of infrastructure quickly.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/nextflow-tower-and-how-it-makes-it-easy-to-manage-a-lot-of-infrastructure-quickly.html</link>
      <pubDate>Thu, 22 Jul 2021 15:00:09 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/nextflow-tower-and-how-it-makes-it-easy-to-manage-a-lot-of-infrastructure-quickly.html</guid>
      <description>Nextflow Tower became necessary when lots of Nextflow users started to leave the confines of their existing environments in search of more compute, more storage, more capabilities. Nextflow made their workflows portable, but there was still a lot of relatively complex work to be done standing up cloud infrastructure, setting limits, choosing instances, pondering VPCs, Batch compute environments &amp;hellip; You get the idea.
Nextflow Tower massively simplifies that. It integrates deeply with all these environments (lots of clouds, lots of cluster types, and even in hybrid mode) such that a researcher with a good idea can go from laptop to server to cluster to massive compute farm, step-by-step as their idea and ambition grows.
Evan Floden, the CEO of Seqera Labs and a core developer of Nextflow (along with his friend and business partner Paolo Di Tommaso) steps in to take us for a drive of Tower and explains some of the tech underpinning it.</description>
    </item>
    
    <item>
      <title>NF Core is an example to all compute-intensive scientific fields. They should all watch this.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/nf-core-is-an-example-to-all-compute-intensive-scientific-fields-they-should-all-watch-this.html</link>
      <pubDate>Thu, 15 Jul 2021 20:12:22 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/nf-core-is-an-example-to-all-compute-intensive-scientific-fields-they-should-all-watch-this.html</guid>
      <description>NF Core is an open source repository of Nextflow workflows that can be downloaded, shared, forked, updated, tested and validated. If you&amp;rsquo;ve ever heard that quote from Isaac Newtown about seeing further by standing on the shoulders of giants: well this is the ladder modern day scientists are using to climb up those very big shoulders. Phil Ewels from the SciLife lab at Stockholm University walks us through some of the history of NF Core and gives us a demo of how easy it is to use, and the cool tools he and his project team have built around to just make it &amp;hellip; a no brainer. Every domain of science should be looking at how this works, because every field of science performs a workflow on data - and those workflows are getting increasingly complicated. If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>AWS&#39;s HPC Storage storage options - choose, use and abuse. Here&#39;s how.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/awss-hpc-storage-storage-options-choose-use-and-abuse-heres-how.html</link>
      <pubDate>Fri, 09 Jul 2021 12:43:09 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/awss-hpc-storage-storage-options-choose-use-and-abuse-heres-how.html</guid>
      <description>AWS has a LOT of storage options (block, PIOPS, object, volume, loads of file sytems), pretty much all of which can be used for HPC. That&amp;rsquo;s because we&amp;rsquo;re not forced to come up with one single massively fast storage solution that can cope with a hurricane of worst-case usage - you&amp;rsquo;re not making one-way door decisions on infrastructure. Cloud infrastructure is a two-way door and you can change your mind as many times as you like.
So, yes, you can have 200 GByte/s Lustre, but if you&amp;rsquo;re doing embarrassingly-parallel workloads that just need to stream data through a CPU, you&amp;rsquo;re missing an optimization right there. You can have very specifically tuned storage for very specific workloads (and specific clusters or compute environments, too, if you like). And you can have very specifically lower costs associated with it if you do.</description>
    </item>
    
    <item>
      <title>Nextflow has changed the way science does computing and energized a community. We need this.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/nextflow-has-changed-the-way-science-does-computing-and-energized-a-community-we-need-this.html</link>
      <pubDate>Thu, 24 Jun 2021 17:18:49 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/nextflow-has-changed-the-way-science-does-computing-and-energized-a-community-we-need-this.html</guid>
      <description>In recent years, we’ve all been amazed at how large areas of scientific computing have moved to abstract their workflows away from the details of systems and cluster architecture. Bioinformatics has really lead the way in this regard - exploiting the fact that they came to large scale computing without decades of legacy code.
Nextflow is an open source project that has been hugely impactful in that movement, enabling a community leverage each other&amp;rsquo;s scientific work and - honestly - stand on each others&amp;rsquo; shoulders to reach further. We didn&amp;rsquo;t need a pandemic to drive that point home, but they played their part there too (more on that in a later show).
Today we’re joined by Evan Floden, who is a principle author of Nextflow (along with other leaders like Paolo Di Tommaso who was one of Nextflow’s creators). Evan is also the CEO of Seqera Labs, the company he and Paolo formed to provide extra support to Nextflow users, and to take management of the workflows and integration with the underlying infrastructure to a whole new level.</description>
    </item>
    
    <item>
      <title>Arm HPC Cloud Hackathon</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/arm-hpc-cloud-hackathon.html</link>
      <pubDate>Thu, 17 Jun 2021 14:45:01 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/arm-hpc-cloud-hackathon.html</guid>
      <description>Arm and AWS are jointly sponsoring a summer hackathon that&amp;rsquo;s being run by the Arm HPC User group (A-HUG - and definitely we send a hug to everyone out there). The goal of the hackathon is to get loads of codes with Spack recipes building, running and validating on Arm based systems and identifying the ones that don&amp;rsquo;t work or don&amp;rsquo;t perform well, so we - as a community - can find them and get to work fixing them.
In today&amp;rsquo;s show we cover the logistics of how the hackathon will work, and especially cover the software and hardware we&amp;rsquo;re going to be using. There are a lot of new tools (especially cool things like Spack and reFrame) that we think are awesome - we&amp;rsquo;re pretty sure you&amp;rsquo;ll agree after you see them in action - Olly gives us a demo with a real test case.</description>
    </item>
    
    <item>
      <title>CloudWatch automation to keep your scratch disks humming, and your clusters running.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/cloudwatch-automation-to-keep-your-scratch-disks-humming-and-your-clusters-running.html</link>
      <pubDate>Thu, 10 Jun 2021 15:19:32 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/cloudwatch-automation-to-keep-your-scratch-disks-humming-and-your-clusters-running.html</guid>
      <description>Some workloads generate a LOT of output files and sometimes quite suddenly. For codes like OpenFOAM, this is data that you may not need until later when you run a post-processing job.
Given the amount of data isn’t always predictable, there are a few ways to prepare for this deluge, but most of them involve pre-provisioning too much storage in advance (and hoping you guessed correctly).
We’ve never been fans of guessing like that - we think infrastructure should just expand when you need it.
Stephen Sachs from our HPC Performance Engineering team came up with a great technique for solving this. He’s built some cloud automation with CloudWatch into AWS ParallelCluster so it triggers a “drain” process (a shell script) that pushes all the output files into Amazon S3 whenever the local filesystem on a compute instance reaches 80%. It’s surprisingly easy to do.</description>
    </item>
    
    <item>
      <title>The impact of network conditions on application performance is complicated.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/the-impact-of-network-conditions-on-application-performance-is-complicated.html</link>
      <pubDate>Thu, 03 Jun 2021 15:16:16 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/the-impact-of-network-conditions-on-application-performance-is-complicated.html</guid>
      <description>In the search for HPC application performance, there’s more than one way to build a network.
HPC applications (or “codes” as we usually call them) are often at the mercy of the network underpinning an HPC cluster. If your CPUs aren’t busy, it’s usually because something that’s meant to be feeding them data isn’t doing so at a fast enough rate. And often the culprit is the network.
Peter Mendygral has a lot of years of experience looking at networks and is one of the co-authors of the GPCnet benchmark, which aims to have a more nuanced look at how networks deliver for (or fail) the clusters built on them. In todays’s discussion, Pete speaks about the real world conditions found on many network fabrics, and shows us that when they depart from the idealized scenarios that common micro benchmarks measure, the results are anything but stellar.</description>
    </item>
    
    <item>
      <title>HOWTO make sure EFA is setup correctly (and what to do if it isn&#39;t).</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/howto-make-sure-efa-is-setup-correctly-and-what-to-do-if-it-isnt.html</link>
      <pubDate>Fri, 28 May 2021 14:22:10 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/howto-make-sure-efa-is-setup-correctly-and-what-to-do-if-it-isnt.html</guid>
      <description>By popular request, we’re looking today at the EFA software stack and environment: how to make sure it’s set up correctly (so you get great performance from your codes), how to tell if it’s not, and how to fix that.
Austin Cherian, our performance junkie from the HPC Developer Relations team in AWS Engineering joins us to deep dive into the nitty gritty of the stack and some useful techniques for debugging. We cover both Open MPI and Intel MPI, as well as checking the libfabric providers and the hardware enablement underneath.
In the discussion, we reference a lot of helpful pages, including:
the EFA homepage: https://aws.amazon.com/hpc/efa/ supported instances: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html#efa-instance-types the user guide: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html troubleshooting: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/troubleshooting-ena.html#disable-enhanced-networking-ena-instance-store If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>WRF performance teardown on Graviton vs x86</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/wrf-performance-teardown-on-graviton-vs-x86.html</link>
      <pubDate>Thu, 20 May 2021 16:16:52 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/wrf-performance-teardown-on-graviton-vs-x86.html</guid>
      <description>(A complete teardown of WRF performance on x86 and AWS Graviton, from memory subsystems, compilers and MPI stacks).
Weather simulation is a reliably difficult workload for almost any HPC architecture and is often used as a litmus test by many customers before they look too hard at a novel or different systems. Customers have asked us frequently about our performance for codes like WRF, and that’s been even more the case since we launched our Arm-based processor, the AWS Graviton2, in a range of EC2 instances.
So it’s exciting that Karthik Raman and Matt Koop (two leading engineers from our global HPC solution architecture team) dived deep to look at WRF’s performance across a range of instance types (both Intel and Graviton), with EFA (our fast fabric, as you might remember from last week) as well as investigating the impact of different MPIs and compilers.</description>
    </item>
    
    <item>
      <title>How EFA works and why we don&#39;t use infiniband in the cloud.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/how-efa-works-and-why-we-dont-use-infiniband-in-the-cloud.html</link>
      <pubDate>Thu, 13 May 2021 15:00:17 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/how-efa-works-and-why-we-dont-use-infiniband-in-the-cloud.html</guid>
      <description>AWS’s compute infrastructure is very much not like a ‘normal’ supercomputer (whatever that is). We don’t start with a blank page every few years and design the next big system. It’s more like a city where we have to build on what’s there already, renovate occasionally, and push for bigger and better and faster whilst keeping the lights on at all times.
That leads to a bunch of design decisions that drive our engineers in a very different direction and our Elastic Fabric Adapter is an example of just that. Brian Barrett (one of our Principal Engineers in the HPC team) joins us this week to talk about the genesis of EFA, how it works, and why it convinced us that we could do without specialist fabrics like Infiniband and still deliver the same (or better) application scaling performance that our HPC customers were pushing us for.</description>
    </item>
    
    <item>
      <title>Interesting GPU &amp; CPU performance results from GROMACS</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/interesting-gpu-cpu-performance-results-from-gromacs.html</link>
      <pubDate>Thu, 06 May 2021 16:33:38 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/interesting-gpu-cpu-performance-results-from-gromacs.html</guid>
      <description>Carsten Kutzner is a researcher and scientific software developer at the Max Planck Institute for Biophysical Chemistry in Göttingen in Germany. He&amp;rsquo;s been doing some really diverse benchmarking studies in collaboration with our HPC engineering team, and he joins us today to talk about some of the results of that investigation.
What&amp;rsquo;s most interesting (and soft of unexpected) is the instances he concludes are the best for the job - whether you&amp;rsquo;re optimizing for sheer performance against the clock, or price performance because you need to maximize your budget within a more generous time window. No spoilers here: you need to hear him talk.
Molecular Dynamicists are used to doing these calculations because they measure their progress in nanoseconds of simulation time per DAY of wall clock time. As Carsten points out: if your job runs for days or weeks, it matters whether you can squeeze 10% more from the compute units it&amp;rsquo;s running on.</description>
    </item>
    
    <item>
      <title>What&#39;s New in DCV 2021.0</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/whats-new-in-dcv-20210.html</link>
      <pubDate>Thu, 29 Apr 2021 14:58:00 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/whats-new-in-dcv-20210.html</guid>
      <description>DCV was originally built for supercomputing centers to push pixels over the internet and enable a scientist or aerospace engineer to feel like they had an HPC cluster under their desk when inspecting detailed imagery or manipulating designs.
Now it&amp;rsquo;s getting used way beyond HPC in gaming, software development and &amp;hellip; enabling lots of us to be productive working from home.
All these little things make the desktop virtualization experience more real, so we decided to catch up with Rey Wang (Sr Product Manager for DCV) and Paolo Maggi (the GM for our DCV organization) to find out what&amp;rsquo;s new.
You can find out more about DCV here: aws.amazon.com/hpc/dcv and you can also watch the Tech Short we did last week on DCV&amp;rsquo;s better faster and more amazing streaming capabilities which are driving gaming engines to deliver rich experience in 4K @ 60 FPS (which we all know is essential &amp;hellip; right?</description>
    </item>
    
    <item>
      <title>Supercomputing Visualization good enough for the most demanding gamers.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/supercomputing-visualization-good-enough-for-the-most-demanding-gamers.html</link>
      <pubDate>Thu, 22 Apr 2021 15:16:21 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/supercomputing-visualization-good-enough-for-the-most-demanding-gamers.html</guid>
      <description>DCV was originally created for scientists and engineers who run thing on supercomputers. They needed to visualize massive datasets produced by really large compute workloads crunching petabytes of data. That takes some fast machines and some very fancy GPUs. Since we don’t like putting scientist into freezing data centers our DCV engineers worked out how to push pixels over the internet quickly, without any loss of fidelity.
Fast forward 20 years and a generation of gamers want to do the same thing, only this time there are way more pixels (blame HD and 4k) and it’s 60 frames per second, or bust.
That meant we had to so some extra tricks to make video streams not miss a beat when very normal things happen, like the internet drops a packet or there’s some congestion because your mum is on a zoom call with her team at the office.</description>
    </item>
    
    <item>
      <title>&#39;Making stuff run fast&#39;, starting with GROMACS.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/making-stuff-run-fast-starting-with-gromacs.html</link>
      <pubDate>Thu, 08 Apr 2021 15:00:15 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/making-stuff-run-fast-starting-with-gromacs.html</guid>
      <description>We just published a blog post last week with a deep dive into what makes GROMACS tick. The blog post talked about software stacks and EC2 instances that will deliver the best possible performance for people trying to do some molecular dynamics quickly. This turns out to be pretty much EVERYONE in the MD community, because these are people who measure their progress in nanoseconds of simulation PER DAY of wall clock time.
Austin Cherian is one of our senior engineers in the Developer Relations team, and the one who is the most obsessed with performance and getting the performance testing methodology right. We asked him to join tech shorts this week to show us how he uses and abuses AWS ParallelCluster in his quest of “making stuff go fast”, because his use cases are pretty common, and might help you imagine some better ways of working.</description>
    </item>
    
    <item>
      <title>Containers, Episode II - the Runtimes Strike Back</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/containers-episode-ii-the-runtimes-strike-back.html</link>
      <pubDate>Thu, 01 Apr 2021 16:48:35 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/containers-episode-ii-the-runtimes-strike-back.html</guid>
      <description>Christian Kniep (our senior developer relations engineer) from HPC Engineering is back to finish the conversation we started about containers in HPC. Christian is leading the cause for containerization in HPC, and helping our engineering and product teams focus on enabling that path on AWS.
Today we&amp;rsquo;re talking about how containers work in a shared infrastructure environment, with shared filesystems (like Lustre) and we&amp;rsquo;ll also cover multi-node parallelism, too, since MPI is where it&amp;rsquo;s at for many (most?) people in the HPC community.
(And we completely missed thew opportunity to make any April fools jokes, so everything we say is totally believable) :-)
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>&#39;NO TEARS HPC - honest-to-goodness research-ready HPC clusters in under 10 minutes.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/no-tears-hpc-honest-to-goodness-research-ready-hpc-clusters-in-under-10-minutes.html</link>
      <pubDate>Thu, 25 Mar 2021 16:30:03 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/no-tears-hpc-honest-to-goodness-research-ready-hpc-clusters-in-under-10-minutes.html</guid>
      <description>When COVID-19 struck in 2020, lots of scientists all over the world had to drop EVERYTHING and get busy solving some very big problems, very quickly. AWS became a founding member of the COVID-19 HPC Consortium, which is a White House-led initiative to provide COVID-19 researchers worldwide with free compute time and huge resources on leading industry, government, and academic HPC systems. In short, we all threw the kitchen sink at the problem of speeding up the pace of scientific discovery in the fight to stop the virus.
From AWS’s point of view, global reach and capacity are solved problems. Our challenge was to onboard researchers quickly, with no specialized knowledge necessary &amp;hellip; so they could get computing on AWS as fast as humanly possible. To that end we developed the NoTearsHPC cluster solution for 1-click launches.
Evan Bollig and Sean Smith from the team who built No Tears join us to talk about how it work, what it provides and show us how easy it is to do really complicated things really fast.</description>
    </item>
    
    <item>
      <title>Containers in HPC - what they fix and what they break (and how to fix that, too)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/containers-in-hpc-what-they-fix-and-what-they-break-and-how-to-fix-that-too.html</link>
      <pubDate>Thu, 18 Mar 2021 16:50:52 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/containers-in-hpc-what-they-fix-and-what-they-break-and-how-to-fix-that-too.html</guid>
      <description>Today we&amp;rsquo;re joined by Christian Kniep who is one of our senior developer advocates in HPC Engineering. Christian is leading the cause for containerization in HPC, and helping our engineering and product teams focus on enabling that path on AWS.
We know containers are a &amp;lsquo;mixed&amp;rsquo; topic for HPC folks, and so we&amp;rsquo;re starting a series of Tech Shorts to talk about what they are, what problems they solve, what new problems they create, and what solutions exist to address those.
Today is part 1, where we talk about how containers work generally, how they work on AWS, with things like AWS Nitro and GPUs in play.
Next week, we&amp;rsquo;ll talk about containers in shared infrastructure and multi-node parallelism, too.
The FOSDEM devroom we spoke about is here: https://tinyurl.com/fosdemhpc21 and you can find Christian on twitter here https://twitter.com/CQnib - pick up the conversation with him if you have some thoughts about containers and how they work for you (or not).</description>
    </item>
    
    <item>
      <title>How to accelerate CryoEM Analysis with AWS ParallelCluster and FSx for Lustre</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/how-to-accelerate-cryoem-analysis-with-aws-parallelcluster-and-fsx-for-lustre.html</link>
      <pubDate>Thu, 11 Mar 2021 10:55:25 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/how-to-accelerate-cryoem-analysis-with-aws-parallelcluster-and-fsx-for-lustre.html</guid>
      <description>In today’s show, we’re talking about how to choose the right compute and storage elements to get great performance for CryoSPARC, which is one of the popular codes used in cryogenic electron microscopy (CryoEM) analysis.
This is important because, as you’ll hear in the discussion, breaking the pipeline down into different stages and optimizing the infrastructure for each one can speed up the entire workflow, and save a whole lot of money on that compute, too. All these workloads ran on AWS ParallelCluster, which lets you have multiple queues with different instance types and orchestration options (check out last week’s show: https://www.youtube.com/watch?v=C4iSNjcW5O4). ParallelCluster also makes it easy to construct an FSx for Lustre filesystem on the fly using data from an Amazon S3 bucket.
Steve Litster is our Principal HPC business leader for Healthcare and Lifesciences, based out of Boston, and is a recovering x-ray crystallographer (and still has occasional flashbacks of being in a lab).</description>
    </item>
    
  </channel>
</rss>
