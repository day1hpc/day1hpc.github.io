<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>latency on Day 1 HPC (Staging)</title>
    <link>https://d175uvn6dnkepf.cloudfront.net/tags/latency.html</link>
    <description>Recent content in latency on Day 1 HPC (Staging)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Jun 2021 15:16:16 +0000</lastBuildDate><atom:link href="https://d175uvn6dnkepf.cloudfront.net/tags/latency/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The impact of network conditions on application performance is complicated.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/the-impact-of-network-conditions-on-application-performance-is-complicated.html</link>
      <pubDate>Thu, 03 Jun 2021 15:16:16 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/the-impact-of-network-conditions-on-application-performance-is-complicated.html</guid>
      <description>In the search for HPC application performance, there’s more than one way to build a network.
HPC applications (or “codes” as we usually call them) are often at the mercy of the network underpinning an HPC cluster. If your CPUs aren’t busy, it’s usually because something that’s meant to be feeding them data isn’t doing so at a fast enough rate. And often the culprit is the network.
Peter Mendygral has a lot of years of experience looking at networks and is one of the co-authors of the GPCnet benchmark, which aims to have a more nuanced look at how networks deliver for (or fail) the clusters built on them. In todays’s discussion, Pete speaks about the real world conditions found on many network fabrics, and shows us that when they depart from the idealized scenarios that common micro benchmarks measure, the results are anything but stellar.</description>
    </item>
    
  </channel>
</rss>
