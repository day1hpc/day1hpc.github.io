<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Media on Day 1 HPC (Staging)</title>
    <link>https://d175uvn6dnkepf.cloudfront.net/tags/media.html</link>
    <description>Recent content in Media on Day 1 HPC (Staging)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 05 Jul 2022 00:00:00 -0700</lastBuildDate><atom:link href="https://d175uvn6dnkepf.cloudfront.net/tags/media/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Efficient and cost-effective rendering pipelines with Blender and AWS Batch</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/efficient-and-cost-effective-rendering-pipelines-with-blender-and-aws-batch.html</link>
      <pubDate>Tue, 05 Jul 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/efficient-and-cost-effective-rendering-pipelines-with-blender-and-aws-batch.html</guid>
      <description>This blog post explains how to run parallel rendering workloads and produce an animation in a cost and time effective way using AWS Batch and AWS Step Functions. AWS Batch manages the rendering jobs on Amazon Elastic Compute Cloud (Amazon EC2), and AWS Step Functions coordinates the dependencies across the individual steps of the rendering workflow. Additionally, Amazon EC2 Spot instances can be used to reduce compute costs by up to 90% compared to On-Demand prices.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Bayesian ML Models at Scale with AWS Batch</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/bayesian-ml-models-at-scale-with-aws-batch.html</link>
      <pubDate>Tue, 14 Jun 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/bayesian-ml-models-at-scale-with-aws-batch.html</guid>
      <description>Ampersand is a data-driven TV advertising technology company that provides aggregated TV audience impression insights and planning on 42 million households, in every media market, across more than 165 networks and apps and in all dayparts (broadcast day segments). The Ampersand Data Science team estimated that building their statistical models would require up to 600,000 physical CPU hours to run, which would not be feasible without using a massively parallel and large-scale architecture in the cloud. AWS Batch enabled Ampersand to compress their time of computation over 500x through massive scaling while optimizing their costs using Amazon EC2 Spot. In this blog post, we will provide an overview of how Ampersand built their TV audience impressions (“impressions”) models at scale on AWS, review the architecture they have been using, and discuss optimizations they conducted to run their workload efficiently on AWS Batch.</description>
    </item>
    
    <item>
      <title>How we enabled uncompressed live video with CDI over EFA</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/how-we-enabled-uncompressed-live-video-with-cdi-over-efa.html</link>
      <pubDate>Wed, 24 Nov 2021 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/how-we-enabled-uncompressed-live-video-with-cdi-over-efa.html</guid>
      <description>We’re going to take you into the world of broadcast video, and explain how it led to us announcing today the general availability of EFA on smaller instance sizes. For a range of applications, this is going to save customers a lot of money because they no longer need to use the biggest instances in each instance family to get HPC-style network performance. But the story of how we got there involves our Elastic Fabric Adapter (EFA), some difficult problems presented to us by customers in the entertainment industry, and an invention called the Cloud Digital Interface (CDI). And it started not very far from Hollywood.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Supporting climate model simulations to accelerate climate science</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/supporting-climate-model-simulations-to-accelerate-climate-science.html</link>
      <pubDate>Fri, 03 Sep 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/supporting-climate-model-simulations-to-accelerate-climate-science.html</guid>
      <description>The Amazon Sustainability Data Initiative (ASDI), AWS is donating cloud resources, technical support, and access to scalable infrastructure and fast networking providing high performance computing solutions to support simulations of near-term climate using the National Center for Atmospheric Research (NCAR) Community Earth System Model Version 2 (CESM2) and its Whole Atmosphere Community Climate Model (WACCM). In collaboration with ASDI, AWS, and SilverLining, a nonprofit dedicated to ensuring a safe climate, the National Center for Atmospheric Research (NCAR) will run an ensemble of 30 climate-model simulations on AWS. The climate runs will simulate the Earth system over the period of years 2022-2070 under a median scenario for warming and make them available through the AWS Open Data Program. The simulation work will demonstrate the ability to use cloud infrastructure to advance climate models in support of robust scientific studies by researchers around the world and aims to accelerate and democratize climate science.</description>
    </item>
    
    <item>
      <title>Pushing pixels with NICE DCV</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/pushing-pixels-with-nice-dcv.html</link>
      <pubDate>Fri, 30 Jul 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/pushing-pixels-with-nice-dcv.html</guid>
      <description>NICE DCV, our high-performance, low-latency remote-display protocol, was originally created for scientists and engineers who ran large workloads on far-away supercomputers, but needed to visualize data without moving it. Pushing pixels over limited bandwidth across the globe has been the goal of the DCV team since 2007. DCV was able to make very frugal use of very scarce bandwidth, because it was super lean, used data-compression techniques and quickly adopted cutting-edge technologies of the time from GPUs (this is HPC, after all, we left nothing on the table when it came to exploiting new gadgets). This allowed the team to create a super light-weight visualization package that could stream pixels over almost any network. Fast forward to the 2020s, and a generation of gamers, artists, and film-makers all want to do the same thing as HPC researchers- only this time there are way more pixels, because we now have HD and 4k (and some people have multiple), and for most of them, it’s 60 frames per second, or it’s not worth having.</description>
    </item>
    
  </channel>
</rss>
