<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Modeling on Day 1 HPC (Staging)</title>
    <link>https://d175uvn6dnkepf.cloudfront.net/tags/modeling.html</link>
    <description>Recent content in Modeling on Day 1 HPC (Staging)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 Sep 2022 00:00:00 -0700</lastBuildDate><atom:link href="https://d175uvn6dnkepf.cloudfront.net/tags/modeling/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Rearchitecting AWS Batch managed services to leverage AWS Fargate</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/rearchitecting-aws-batch-managed-services-to-leverage-aws-fargate.html</link>
      <pubDate>Wed, 21 Sep 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/rearchitecting-aws-batch-managed-services-to-leverage-aws-fargate.html</guid>
      <description>AWS service teams continuously improve the underlying infrastructure and operations of managed services, and AWS Batch is no exception. The AWS Batch team recently moved most of their job scheduler fleet to a serverless infrastructure model leveraging AWS Fargate. I had a chance to sit with Devendra Chavan, Senior Software Development Engineer on the AWS Batch team, to discuss the move to AWS Fargate and its impact on the Batch managed scheduler service component.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>A serverless architecture for high performance financial modelling</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/a-serverless-architecture-for-high-performance-financial-modelling.html</link>
      <pubDate>Tue, 06 Sep 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/a-serverless-architecture-for-high-performance-financial-modelling.html</guid>
      <description>Understanding deal and portfolio risk and capital requirements is a computationally expensive process that requires the execution of multiple financial forecasting models every day and in often in real time. This post describes how it works at RenaissanceRe, one of the world’s leading reinsurance companies.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Building a Scalable Predictive Modeling Framework in AWS – Part 3</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/building-a-scalable-predictive-modeling-framework-in-aws-part-3.html</link>
      <pubDate>Thu, 11 Aug 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/building-a-scalable-predictive-modeling-framework-in-aws-part-3.html</guid>
      <description>In this final part of this three-part blog series on building predictive models at scale in AWS, we will use the synthetic dataset and the models generated in the previous post to showcase the model updating and sensitivity analysis capabilities of the aws-do-pm framework.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Building a Scalable Predictive Modeling Framework in AWS – Part 2</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/building-a-scalable-predictive-modeling-framework-in-aws-part-2.html</link>
      <pubDate>Wed, 10 Aug 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/building-a-scalable-predictive-modeling-framework-in-aws-part-2.html</guid>
      <description>In the first part of this three-part blog series, we introduced the aws-do-pm framework for building predictive models at scale in AWS. In this blog, we showcase a sample application for predicting the life of batteries in a fleet of electric vehicles, using the aws-do-pm framework.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Building a Scalable Predictive Modeling Framework in AWS – Part 1</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/building-a-scalable-predictive-modeling-framework-in-aws-part-1.html</link>
      <pubDate>Tue, 09 Aug 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/building-a-scalable-predictive-modeling-framework-in-aws-part-1.html</guid>
      <description>Predictive models have powered the design and analysis of real-world systems such as jet engines, automobiles, and powerplants for decades. These models are used to provide insights on system performance and to run simulations, at a fraction of the cost compared to experiments with physical hardware. In this first post of three, we described the motivation and general architecture of the open-source aws-do-pm framework project for building predictive models at scale in AWS.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Bayesian ML Models at Scale with AWS Batch</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/bayesian-ml-models-at-scale-with-aws-batch.html</link>
      <pubDate>Tue, 14 Jun 2022 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/bayesian-ml-models-at-scale-with-aws-batch.html</guid>
      <description>Ampersand is a data-driven TV advertising technology company that provides aggregated TV audience impression insights and planning on 42 million households, in every media market, across more than 165 networks and apps and in all dayparts (broadcast day segments). The Ampersand Data Science team estimated that building their statistical models would require up to 600,000 physical CPU hours to run, which would not be feasible without using a massively parallel and large-scale architecture in the cloud. AWS Batch enabled Ampersand to compress their time of computation over 500x through massive scaling while optimizing their costs using Amazon EC2 Spot. In this blog post, we will provide an overview of how Ampersand built their TV audience impressions (“impressions”) models at scale on AWS, review the architecture they have been using, and discuss optimizations they conducted to run their workload efficiently on AWS Batch.</description>
    </item>
    
    <item>
      <title>How to Arm a world-leading forecast model with AWS Graviton and Lambda</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/how-to-arm-a-world-leading-forecast-model-with-aws-graviton-and-lambda.html</link>
      <pubDate>Wed, 02 Feb 2022 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/how-to-arm-a-world-leading-forecast-model-with-aws-graviton-and-lambda.html</guid>
      <description>The Met Office is the UK’s National Meteorological Service, providing 24×7 world-renowned scientific excellence in weather, climate and environmental forecasts and severe weather warnings for the protection of life and property. They provide forecasts and guidance for the public, to our government and defence colleagues as well as the private sector. As an example, if you’ve been on a plane over Europe, Middle East, or Africa; that plane took off because the Met Office (as one of two World Aviation Forecast Centres) provided a forecast. This article explains one of the ways they use AWS to collect these observations, which has freed them to focus more on top quality delivery for their customers.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Running a 3.2M vCPU HPC Workload on AWS with YellowDog</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/running-a-32m-vcpu-hpc-workload-on-aws-with-yellowdog.html</link>
      <pubDate>Tue, 23 Nov 2021 00:00:00 -0800</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/running-a-32m-vcpu-hpc-workload-on-aws-with-yellowdog.html</guid>
      <description>OMass Therapeutics, a biotechnology company identifying medicines against highly validated target ecosystems, used Yellowdog on AWS to analyze and screen 337 million compounds in 7 hours, a task which would have taken two months using an on-premises HPC cluster. YellowDog, based in Bristol in the UK, ran the drug discovery application on an extremely large, multi-region cluster in AWS with the AWS ‘pay-as-you-go’ pricing model. It provided a central, unified interface to monitor and manage AWS Region selection, compute provisioning, job allocation and execution. The entire workload completed in 65 minutes, enabling scientists to start work on analysis the same day, significantly accelerating the drug discovery process. In this post, we’ll discuss the AWS and YellowDog services we deployed, and the mechanisms used to scale to 3.2m vCPUs using multiple EC2 instance types across multiple regions in 33 minutes, running at a 95% utilization rate.</description>
    </item>
    
    <item>
      <title>The Convergent Evolution of Grid Computing in Financial Services</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/the-convergent-evolution-of-grid-computing-in-financial-services.html</link>
      <pubDate>Thu, 21 Oct 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/the-convergent-evolution-of-grid-computing-in-financial-services.html</guid>
      <description>The Financial Services industry makes significant use of high performance computing (HPC) but it tends to be in the form of loosely coupled, embarrassingly parallel workloads to support risk modelling. The infrastructure tends to scale out to meet ever increasing demand as the analyses look at more and finer grained data. At AWS we’ve helped many customers tackle scaling challenges are noticing some common themes. In this post we describe how HPC teams are thinking about how they deliver compute capacity today, and highlight how we see the solutions converging for the future.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Running the Harmonie numerical weather prediction model on AWS</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/running-the-harmonie-numerical-weather-prediction-model-on-aws.html</link>
      <pubDate>Thu, 30 Sep 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/running-the-harmonie-numerical-weather-prediction-model-on-aws.html</guid>
      <description>The Danish Meteorological Institute (DMI) is responsible for running atmospheric, climate and ocean models covering the kingdom of Denmark. We worked together with the DMI to port and run a full numerical weather prediction (NWP) cycling dataflow with the Harmonie Numerical Weather Prediction (NWP) model to AWS. You can find a report of the porting and operational experience in the ACCORD community newsletter. In this blog post, we expand on that report to present the initial timing results from running the forecast component of Harmonie model on AWS. We also present these as-is timing results together with as-is timings attained on the supercomputing systems based on Cray XC40 and Intel Xeon based Cray XC50.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Supporting climate model simulations to accelerate climate science</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/supporting-climate-model-simulations-to-accelerate-climate-science.html</link>
      <pubDate>Fri, 03 Sep 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/supporting-climate-model-simulations-to-accelerate-climate-science.html</guid>
      <description>The Amazon Sustainability Data Initiative (ASDI), AWS is donating cloud resources, technical support, and access to scalable infrastructure and fast networking providing high performance computing solutions to support simulations of near-term climate using the National Center for Atmospheric Research (NCAR) Community Earth System Model Version 2 (CESM2) and its Whole Atmosphere Community Climate Model (WACCM). In collaboration with ASDI, AWS, and SilverLining, a nonprofit dedicated to ensuring a safe climate, the National Center for Atmospheric Research (NCAR) will run an ensemble of 30 climate-model simulations on AWS. The climate runs will simulate the Earth system over the period of years 2022-2070 under a median scenario for warming and make them available through the AWS Open Data Program. The simulation work will demonstrate the ability to use cloud infrastructure to advance climate models in support of robust scientific studies by researchers around the world and aims to accelerate and democratize climate science.</description>
    </item>
    
    <item>
      <title>Price-Performance Analysis of Amazon EC2 GPU Instance Types using NVIDIA’s GPU optimized seismic code</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/price-performance-analysis-of-amazon-ec2-gpu-instance-types-using-nvidias-gpu-optimized-seismic-code.html</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/price-performance-analysis-of-amazon-ec2-gpu-instance-types-using-nvidias-gpu-optimized-seismic-code.html</guid>
      <description>Seismic imaging is the process of positioning the Earth’s subsurface reflectors. It transforms the seismic data recorded in time at the Earth’s surface to an image of the Earth’s subsurface. This is done by back-propagating data from time to space in a given velocity model. Kirchhoff depth migration is a well-known technique used in geophysics for seismic imaging. Kirchhoff time and depth migration produce an image with higher resolution and generate an image of the subsurface for a subset class of the data, providing valuable information about the petrophysical properties of the rocks and helps to determine how accurate the velocity model is. This blog post looks at the price-performance characteristics computing Kirchhoff migration methods on GPUs using Nvidia’s GPU-optimized code.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Scalable and Cost-Effective Batch Processing for ML workloads with AWS Batch and Amazon FSx</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/scalable-and-cost-effective-batch-processing-for-ml-workloads-with-aws-batch-and-amazon-fsx.html</link>
      <pubDate>Fri, 23 Jul 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/scalable-and-cost-effective-batch-processing-for-ml-workloads-with-aws-batch-and-amazon-fsx.html</guid>
      <description>Batch processing is a common need across varied machine learning use cases such as video production, financial modeling, drug discovery, or genomic research. The elasticity of the cloud provides efficient ways to scale and simplify batch processing workloads while cutting costs. In this post, you’ll learn a scalable and cost-effective approach to configure AWS Batch Array jobs to process datasets that are stored on Amazon S3 and presented to compute instances with Amazon FSx for Lustre.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
    <item>
      <title>Training forecasters to warn severe hazardous weather on AWS</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/training-forecasters-to-warn-severe-hazardous-weather-on-aws.html</link>
      <pubDate>Tue, 18 May 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/training-forecasters-to-warn-severe-hazardous-weather-on-aws.html</guid>
      <description>Training users on how to use high performance computing resources — and the data that comes out as a result of those analyses — is an essential function of most research organizations. Having a robust, scalable, and easy-to-use platform for on-site and remote training is becoming a requirement for creating a community around your research mission. A great example of this comes from the NOAA National Weather Service Warning Decision Training Division (WDTD), which develops and delivers training on the integrated elements of the hazardous weather warning process within a National Weather Service (NWS) forecast office. In collaboration with the University of Oklahoma’s Cooperative Institute for Mesoscale Meteorological Studies (OU/CIMMS), WDTD conducts its flagship course, the Radar and Applications Course (RAC), for forecasters issuing warnings for flash floods, severe thunderstorms, and tornadoes. Trainees learn the warning process, the science and application of conceptual models, and technical aspects of analyzing radar and other weather data in the Advanced Weather Interactive Processing System (AWIPS).</description>
    </item>
    
    <item>
      <title>Numerical weather prediction on AWS Graviton2</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/numerical-weather-prediction-on-aws-graviton2.html</link>
      <pubDate>Mon, 10 May 2021 00:00:00 -0700</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/numerical-weather-prediction-on-aws-graviton2.html</guid>
      <description>The Weather Research and Forecasting (WRF) model is a numerical weather prediction (NWP) system designed to serve both atmospheric research and operational forecasting needs. With the release of Arm-based AWS Graviton2 Amazon Elastic Compute Cloud (EC2) instances, a common question has been how these instances perform on large-scale NWP workloads. In this blog, we will present results from a standard WRF benchmark simulation and compare across three different instance types.
Read the full post at the AWS HPC Blog.</description>
    </item>
    
  </channel>
</rss>
