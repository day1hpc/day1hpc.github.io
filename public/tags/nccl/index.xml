<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NCCL on Day 1 HPC (Staging)</title>
    <link>https://d175uvn6dnkepf.cloudfront.net/tags/nccl.html</link>
    <description>Recent content in NCCL on Day 1 HPC (Staging)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 May 2021 15:00:17 +0000</lastBuildDate><atom:link href="https://d175uvn6dnkepf.cloudfront.net/tags/nccl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How EFA works and why we don&#39;t use infiniband in the cloud.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/how-efa-works-and-why-we-dont-use-infiniband-in-the-cloud.html</link>
      <pubDate>Thu, 13 May 2021 15:00:17 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/how-efa-works-and-why-we-dont-use-infiniband-in-the-cloud.html</guid>
      <description>AWS’s compute infrastructure is very much not like a ‘normal’ supercomputer (whatever that is). We don’t start with a blank page every few years and design the next big system. It’s more like a city where we have to build on what’s there already, renovate occasionally, and push for bigger and better and faster whilst keeping the lights on at all times.
That leads to a bunch of design decisions that drive our engineers in a very different direction and our Elastic Fabric Adapter is an example of just that. Brian Barrett (one of our Principal Engineers in the HPC team) joins us this week to talk about the genesis of EFA, how it works, and why it convinced us that we could do without specialist fabrics like Infiniband and still deliver the same (or better) application scaling performance that our HPC customers were pushing us for.</description>
    </item>
    
  </channel>
</rss>
