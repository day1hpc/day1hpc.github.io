<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>openFOAM on Day 1 HPC (Staging)</title>
    <link>https://d175uvn6dnkepf.cloudfront.net/tags/openfoam.html</link>
    <description>Recent content in openFOAM on Day 1 HPC (Staging)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Mar 2022 17:26:49 +0000</lastBuildDate><atom:link href="https://d175uvn6dnkepf.cloudfront.net/tags/openfoam/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Scaling CFD a lot by breaking down a workflow, to speed things up</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/scaling-cfd-a-lot-by-breaking-down-a-workflow-to-speed-things-up.html</link>
      <pubDate>Thu, 03 Mar 2022 17:26:49 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/scaling-cfd-a-lot-by-breaking-down-a-workflow-to-speed-things-up.html</guid>
      <description>Sometimes when we&amp;rsquo;re looking for performance we lose the forrest for all those trees - we miss the huge improvements we can do to unglamorous parts of the overall workflow, while we obsess on the pieces that look hard. It&amp;rsquo;s an engineer thing, I think.
In today&amp;rsquo;s Tech Short, Neil Ashton shows us exactly this kind of example from the world of CFD (using OpenFOAM, but this lesson applies generally) - and shows us how to break the problem down in order to speed it all up - and pretty easily, too.
The workshops we reference in the discussion are all listed here: https://aws.amazon.com/hpc/cfd/
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>CFD performance on Ice Lake CPU with the Amazon EC2 C6i (Part 2)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/cfd-performance-on-ice-lake-cpu-with-the-amazon-ec2-c6i-part-2.html</link>
      <pubDate>Tue, 02 Nov 2021 14:00:09 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/cfd-performance-on-ice-lake-cpu-with-the-amazon-ec2-c6i-part-2.html</guid>
      <description>Today we launched the new Amazon EC2 C6i instance family which is powered by the Intel Xeon Ice Lake processor and comes equipped with our Elastic Fabric Adapter.
Neil Ashton and Nicola Venuti joined us to talk about CFD performance on this new instance family, and spent some time comparing network connectivity options, too.
This is a two part series of Tech Shorts:
Part 1) Discuss C6i and how it&amp;rsquo;s put together. Look at OpenFOAM and Siemens Simcenter StarCCM+ Part 2) Ansys Fluent, under populated cores, and some on-premises comparisons for calibration.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>CFD performance on Ice Lake CPU with the Amazon EC2 C6i (Part 1)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/cfd-performance-on-ice-lake-cpu-with-the-amazon-ec2-c6i-part-1.html</link>
      <pubDate>Fri, 29 Oct 2021 10:48:01 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/cfd-performance-on-ice-lake-cpu-with-the-amazon-ec2-c6i-part-1.html</guid>
      <description>Today we launched the new Amazon EC2 C6i instance family which is powered by the Intel Xeon Ice Lake processor and comes equipped with our Elastic Fabric Adapter.
Neil Ashton and Nicola Venuti joined us to talk about CFD performance on this new instance family, and spent some time comparing network connectivity options, too.
This is a two part series of Tech Shorts:
Part 1) Discuss C6i and how it&amp;rsquo;s put together. Look at OpenFOAM and Siemens Simcenter StarCCM+ Part 2) Ansys Fluent, under populated cores, and some on-premises comparisons for calibration.
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
    <item>
      <title>CloudWatch automation to keep your scratch disks humming, and your clusters running.</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/cloudwatch-automation-to-keep-your-scratch-disks-humming-and-your-clusters-running.html</link>
      <pubDate>Thu, 10 Jun 2021 15:19:32 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/cloudwatch-automation-to-keep-your-scratch-disks-humming-and-your-clusters-running.html</guid>
      <description>Some workloads generate a LOT of output files and sometimes quite suddenly. For codes like OpenFOAM, this is data that you may not need until later when you run a post-processing job.
Given the amount of data isn’t always predictable, there are a few ways to prepare for this deluge, but most of them involve pre-provisioning too much storage in advance (and hoping you guessed correctly).
We’ve never been fans of guessing like that - we think infrastructure should just expand when you need it.
Stephen Sachs from our HPC Performance Engineering team came up with a great technique for solving this. He’s built some cloud automation with CloudWatch into AWS ParallelCluster so it triggers a “drain” process (a shell script) that pushes all the output files into Amazon S3 whenever the local filesystem on a compute instance reaches 80%. It’s surprisingly easy to do.</description>
    </item>
    
  </channel>
</rss>
