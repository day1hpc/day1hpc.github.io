<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>scheduling on Day 1 HPC (Staging)</title>
    <link>https://d175uvn6dnkepf.cloudfront.net/tags/scheduling.html</link>
    <description>Recent content in scheduling on Day 1 HPC (Staging)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Jul 2022 14:07:45 +0000</lastBuildDate><atom:link href="https://d175uvn6dnkepf.cloudfront.net/tags/scheduling/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Memory aware scheduling with Slurm in ParallelCluster 3.2 (Part 2 of 2)</title>
      <link>https://d175uvn6dnkepf.cloudfront.net/post/memory-aware-scheduling-with-slurm-in-parallelcluster-32-part-2-of-2.html</link>
      <pubDate>Thu, 28 Jul 2022 14:07:45 +0000</pubDate>
      
      <guid>https://d175uvn6dnkepf.cloudfront.net/post/memory-aware-scheduling-with-slurm-in-parallelcluster-32-part-2-of-2.html</guid>
      <description>If you&amp;rsquo;ve ever had to iteratively guess how much memory is left in a bunch of compute nodes in order to get your memory-hungry jobs running, then this feature will save your sanity.
It&amp;rsquo;s a new integration between ParallelCluster and Slurm that lets you specify how much RAM your jobs need, and gives Slurm the ability to figure out how to place your jobs in order to achieve that - not just counting cores, which is the default behavior for most schedulers.
Olly Perks and Austin Cherian describe this in detail, as part of a 2-part series covering the new features of ParallelCluster 3.2 (part 1 covered new file systems support and you can find it here: https://youtu.be/2JOoMv-K1FY).
If you have ideas for technical topics you&amp;rsquo;d like to see us cover in a future show, let us know by finding us on Twitter (@TechHpc) and DM&amp;rsquo;ing us with your idea.</description>
    </item>
    
  </channel>
</rss>
