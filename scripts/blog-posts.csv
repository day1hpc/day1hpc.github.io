title,URL,image,datePublished,categories,excerpt
Amazon EC2 Hpc6id Instances Optimized for High Performance Computing,https://aws.amazon.com/blogs/aws/new-amazon-ec2-hpc6id-instances-optimized-for-high-performance-computing/,,29-Nov-22,Amazon EC2 | Announcements | High Performance Computing,"Today, we announce the general availability of Amazon EC2 Hpc6id instances, a new instance type that is purpose-built for tightly coupled HPC workloads. Amazon EC2 Hpc6id instances are powered by 3rd Gen Intel Xeon Scalable processors (Ice Lake) that run at frequencies up to 3.5 GHz, 1024 GiB memory, 15.2 TB local SSD disk, 200 Gbps Elastic Fabric Adapter (EFA) network bandwidth, which is 4x higher than R6i instances. Amazon EC2 Hpc6id instances have the best per-vCPU HPC performance when compared to similar x86-based EC2 instances for data-intensive HPC workloads."
Avoid overspending with AWS Batch using a serverless cost guardian monitoring architecture,https://aws.amazon.com/blogs/hpc/avoid-overspending-with-aws-batch-using-a-serverless-cost-guardian-monitoring-architecture/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/10/05/CleanShot-2022-10-05-at-09.08.46-1260x628.png,9-Nov-22,AWS Batch | Compute | High Performance Computing,"Pay-as-you-go resources are a compelling but budget-limited researchers performing HPC workloads need help working within the bounds of their grants. In this post, we show how to build a real-time cost guardian for AWS Batch to help enforce those limits."
Bridging research and HPC to tackle grand challenges,https://aws.amazon.com/blogs/hpc/bridging-research-and-hpc-to-tackle-grand-challenges/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/11/08/CleanShot-2022-11-08-at-10.52.15-1256x630.png,8-Nov-22,High Performance Computing | Sustainability | Thought Leadership,"Today we announced the AWS Impact Computing Project at the Harvard Data Science Initiative (HDSI) to identify potential solutions that can improve the lives of humans, other species, and natural ecosystems. Deb Goldfarb describes its goals and our joint vision."
Support for Instance Allocation Flexibility in AWS ParallelCluster 3.3,https://aws.amazon.com/blogs/hpc/support-for-instance-allocation-flexibility-in-aws-parallelcluster-3-3/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/10/27/CleanShot-2022-10-27-at-10.30.36.png,2-Nov-22,AWS ParallelCluster | Compute | High Performance Computing,"AWS ParallelCluster 3.3.0 now lets you define a list of Amazon EC2 instance types for resourcing a compute queue. This gives you more flexibility to optimize the cost and total time to solution of your HPC jobs, especially when capacity is limited or you’re using Spot Instances."
Hyper Metal: Scaling AWS Instances Up with TidalScale,https://aws.amazon.com/blogs/hpc/hyper-metal-scaling-aws-instances-up-with-tidalscale/,,1-Nov-22,,"In this post we show you how to scale large-memory, high CPU-count single system-image environments on AWS with HyperMetal-powered instances by TidalScale."
How AWS Batch developed support for Amazon Elastic Kubernetes Service,https://aws.amazon.com/blogs/hpc/how-aws-batch-developed-support-for-amazon-elastic-kubernetes-service/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/10/21/CleanShot-2022-10-21-at-15.36.47.png,25-Oct-22,Amazon Elastic Kubernetes Service | AWS Batch | High Performance Computing | Thought Leadership,"Today, we discuss AWS batch on Amazon EKS, and the initial motivation and design choices the team made when we developed the service, and some of the challenges to overcome."
Minimize HPC compute costs with all-or-nothing instance launching,https://aws.amazon.com/blogs/hpc/minimize-hpc-compute-costs-with-all-or-nothing-instance-launching/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/09/28/CleanShot-2022-09-28-at-13.13.27-1260x630.png,18-Oct-22,AWS ParallelCluster | High Performance Computing,"In this post, we highlight a little-known configuration option for Slurm on @awscloud ParallelCluster that can reduce costs and increase your iteration speed by preventing idle batch instances from launching when EC2 capacity is limited."
BioContainers are now available in Amazon ECR Public Gallery,https://aws.amazon.com/blogs/hpc/biocontainers-are-now-available-in-amazon-ecr-public-gallery/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/10/12/hpc-150-header.png,12-Oct-22,Announcements | AWS Batch | Compute | High Performance Computing | Life Sciences,"Today we are excited to announce that all 9000+ applications provided by the BioContainers community are available within ECR Public Gallery! You don’t need an AWS account to access these images, but having one allows many more pulls to the internet, and unmetered usage within AWS. If you perform any sort of bioinformatics analysis on AWS, you should check it out!"
Optimize Protein Folding Costs with OpenFold on AWS Batch,https://aws.amazon.com/blogs/hpc/optimize-protein-folding-costs-with-openfold-on-aws-batch/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/10/03/HPCBlog-159-header.png,4-Oct-22,AWS Batch | High Performance Computing | Life Sciences,"In this post, we describe how to orchestrate protein folding jobs on AWS Batch. We also compare the performance of OpenFold and AlphaFold on a set of public targets. Finally, we will discuss how to optimize your protein folding costs."
Getting the Best Price Performance for Numerical Weather Prediction Workloads on AWS,https://aws.amazon.com/blogs/hpc/best-price-performance-for-nwp-on-aws/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/09/19/hpcblog-131-featured-img.png,27-Sep-22,Best Practices | High Performance Computing,"In this post, we will provide an overview of Numerical Weather Prediction (NWP) workloads, and the AWS HPC-optimized services for it. We’ll test three popular NWP codes: WRF, MPAS, and FV3GFS."
Rearchitecting AWS Batch managed services to leverage AWS Fargate,https://aws.amazon.com/blogs/hpc/rearchitecting-aws-batch-managed-services-to-leverage-aws-fargate/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/09/20/hpcblog-144-featured.png,21-Sep-22,AWS Batch | High Performance Computing | Thought Leadership,"AWS service teams continuously improve the underlying infrastructure and operations of managed services, and AWS Batch is no exception. The AWS Batch team recently moved most of their job scheduler fleet to a serverless infrastructure model leveraging AWS Fargate. I had a chance to sit with Devendra Chavan, Senior Software Development Engineer on the AWS Batch team, to discuss the move to AWS Fargate and its impact on the Batch managed scheduler service component."
Easing your migration from SGE to Slurm in AWS ParallelCluster 3,https://aws.amazon.com/blogs/hpc/easing-your-migration-from-sge-to-slurm-in-aws-parallelcluster-3/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/06/29/CleanShot-2022-06-29-at-14.58.17.png,14-Sep-22,AWS ParallelCluster | High Performance Computing,"This post will help you understand the tools available to ease the stress of migrating your cluster (and your users) from SGE to Slurm, which is necessary since the HPC community is no longer supporting SGE’s open-source codebase."
A serverless architecture for high performance financial modelling,https://aws.amazon.com/blogs/hpc/a-serverless-architecture-for-high-performance-financial-modelling/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/08/03/CleanShot-2022-08-03-at-12.43.36-1260x628.png,6-Sep-22,Financial Services | High Performance Computing,"Understanding deal and portfolio risk and capital requirements is a computationally expensive process that requires the execution of multiple financial forecasting models every day and in often in real time. This post describes how it works at RenaissanceRe, one of the world’s leading reinsurance companies."
Simulating 44-Qubit quantum circuits using AWS ParallelCluster,https://aws.amazon.com/blogs/hpc/simulating-44-qubit-quantum-circuits-using-aws-parallelcluster/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/08/03/CleanShot-2022-08-03-at-12.25.24-1260x627.png,30-Aug-22,AWS Center for Quantum Computing | AWS ParallelCluster | High Performance Computing | Quantum Technologies,"A key part of the development of quantum hardware and quantum algorithms is simulation using existing classical architectures and HPC techniques. In this blog post, we describe how to perform large-scale quantum circuits simulations using AWS ParallelCluster with QuEST, the Quantum Exact Simulation Toolkit. We demonstrate a simple and rapid deployment of computational resources up to 4,096 compute instances to simulate random quantum circuits with up to 44 qubits. We were able to allocate as many as 4096 EC2 instances of c5.18xlarge to simulate a non-trivial 44 qubit quantum circuit in fewer than 3.5 hours."
Accelerating Genomics Pipelines Using Intel’s Open Omics Acceleration Framework on AWS,https://aws.amazon.com/blogs/hpc/accelerating-genomics-pipelines-using-intel-open-omics-on-aws/,,23-Aug-22,Best Practices | Customer Solutions | High Performance Computing | Thought Leadership,"In this blog, we showcase the first version of Open Omics and benchmark three applications that are used in processing NGS data – sequence alignment tools BWA-MEM, minimap2, and single cell ATAC-Seq on Xeon-based Amazon Elastic Compute Cloud (Amazon EC2) Instances."
Building a Scalable Predictive Modeling Framework in AWS – Part 3,https://aws.amazon.com/blogs/hpc/building-a-scalable-predictive-modeling-framework-in-aws-part-3/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/07/28/hpc-132-p3-hreader.png,11-Aug-22,Best Practices | High Performance Computing,"In this final part of this three-part blog series on building predictive models at scale in AWS, we will use the synthetic dataset and the models generated in the previous post to showcase the model updating and sensitivity analysis capabilities of the aws-do-pm framework."
Building a Scalable Predictive Modeling Framework in AWS – Part 2,https://aws.amazon.com/blogs/hpc/building-a-scalable-predictive-modeling-framework-in-aws-part-2/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/07/28/hpc-132-p2-hreader.png,10-Aug-22,Best Practices | High Performance Computing | Technical How-to,"In the first part of this three-part blog series, we introduced the aws-do-pm framework for building predictive models at scale in AWS. In this blog, we showcase a sample application for predicting the life of batteries in a fleet of electric vehicles, using the aws-do-pm framework."
Building a Scalable Predictive Modeling Framework in AWS – Part 1,https://aws.amazon.com/blogs/hpc/building-a-scalable-predictive-modeling-framework-in-aws/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/07/28/hpc-132-p1-hreader.png,9-Aug-22,Best Practices | High Performance Computing,"Predictive models have powered the design and analysis of real-world systems such as jet engines, automobiles, and powerplants for decades. These models are used to provide insights on system performance and to run simulations, at a fraction of the cost compared to experiments with physical hardware. In this first post of three, we described the motivation and general architecture of the open-source aws-do-pm framework project for building predictive models at scale in AWS."
Running large-scale CFD fire simulations on AWS for Amazon.com,https://aws.amazon.com/blogs/hpc/amazon-runs-large-scale-cfd-fire-simulations-on-aws/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/07/06/hpcblog-87-header.png,3-Aug-22,AWS ParallelCluster | Customer Solutions | High Performance Computing,"In this blog post, we discuss the AWS solution that Amazon’s construction division used to conduct large-scale CFD fire simulations as part of their Fire Strategy solutions to demonstrate safety and fire mitigation strategies. We outline the five key steps taken that resulted in simulation times that were 15-20x faster than previous on-premises architectures, reducing the time to complete from up to twenty-one days to less than one day."
Expanded filesystems support in AWS ParallelCluster 3.2,https://aws.amazon.com/blogs/hpc/expanded-filesystems-support-in-aws-parallelcluster-3-2/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/07/28/CleanShot-2022-07-28-at-09.45.46-1260x626.png,28-Jul-22,Amazon FSx | Amazon FSx for Lustre | Amazon FSx for NetApp ONTAP | Amazon FSx for OpenZFS | AWS ParallelCluster | High Performance Computing,"AWS ParallelCluster version 3.2 introduces support for two new Amazon FSx filesystem types (NetApp ONTAP and OpenZFS). It also lifts the limit on the number of filesystem mounts you can have on your cluster. We’ll show you how, and help you with the details for getting this going right away."
Slurm-based memory-aware scheduling in AWS ParallelCluster 3.2,https://aws.amazon.com/blogs/hpc/slurm-based-memory-aware-scheduling-in-aws-parallelcluster-3-2/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/07/28/CleanShot-2022-07-28-at-09.39.30-1260x630.png,28-Jul-22,AWS ParallelCluster | Compute | High Performance Computing,"AWS ParallelCluster version 3.2 now supports memory-aware scheduling in Slurm to give you control over the placement of jobs with specific memory requirements. In this blog post, we’ll show you how it works, and explain why this will be really useful to people with memory-hungry workloads."
Call for participation: RADIUSS Tutorial Series,https://aws.amazon.com/blogs/hpc/call-for-participation-radiuss-tutorial-series/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/07/22/hpcblog-146-header.png,22-Jul-22,Announcements | High Performance Computing,Lawrence Livermore National Laboratory (LLNL) and AWS are joining forces to provide a training opportunity for emerging HPC tools and application. RADIUSS (Rapid Application Development via an Institutional Universal Software Stack) is a broad suite of open-source software projects originating from LLNL. Together we are hosting a tutorial series to give attendees hands-on experience with these cutting-edge technologies. Find out how to participate in these events in this blog post.
Analyzing Genomic Data using Amazon Genomics CLI and Amazon SageMaker,https://aws.amazon.com/blogs/hpc/analyzing-genomic-data-using-amazon-genomics-cli-and-amazon-sagemaker/,,19-Jul-22,AWS Batch | High Performance Computing,"In this blog post, we demonstrate how to leverage the AWS Genomics Command line and Amazon SageMaker to analyze large-scale exome sequences and derive meaningful insights. We use the bioinformatics workflow manager Nextflow, it’s open source library of pipelines, NF-Core, and AWS Batch."
How Thermo Fisher Scientific Accelerated Cryo-EM using AWS ParallelCluster,https://aws.amazon.com/blogs/hpc/how-thermo-fisher-scientific-accelerated-cryo-em-using-aws-parallelcluster/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/07/12/CleanShot-2022-07-12-at-14.52.54-1260x630.png,12-Jul-22,AWS ParallelCluster | High Performance Computing,"In this blog post, we’ll walk you through the process of building a successful Cryo-EM benchmarking pilot using AWS ParallelCluster, Amazon FSx for Lustre, and cryoSPARC (from Structura Biotechnology) and explain some of our design decisions along the way."
Efficient and cost-effective rendering pipelines with Blender and AWS Batch,https://aws.amazon.com/blogs/hpc/efficient-and-cost-effective-rendering-pipelines-with-blender-and-aws-batch/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/06/17/CleanShot-2022-06-17-at-16.15.49.png,5-Jul-22,AWS Batch | High Performance Computing,"This blog post explains how to run parallel rendering workloads and produce an animation in a cost and time effective way using AWS Batch and AWS Step Functions. AWS Batch manages the rendering jobs on Amazon Elastic Compute Cloud (Amazon EC2), and AWS Step Functions coordinates the dependencies across the individual steps of the rendering workflow. Additionally, Amazon EC2 Spot instances can be used to reduce compute costs by up to 90% compared to On-Demand prices."
Getting Started with NVIDIA Clara Parabricks on AWS Batch using AWS CloudFormation,https://aws.amazon.com/blogs/hpc/getting-started-with-nvidia-parabricks-on-aws-batch-using-aws-cloudformation/,,28-Jun-22,AWS Batch | High Performance Computing,"In this blog post, we’ll show how you can run NVIDIA Parabricks on AWS Batch leveraging AWS CloudFormation templates. Parabricks is a GPU-accelerated tool for secondary genomic analysis. It reduces the runtime of variant calling on a 30x human genome from 30 hours to just 30 minutes, and leverages AWS Batch to provide an interface that scales compute jobs across multiple instances in the cloud."
Understanding the AWS Batch termination process,https://aws.amazon.com/blogs/hpc/understanding-the-aws-batch-termination-process/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/06/16/hpcblog-121-header-v2.png,21-Jun-22,AWS Batch | Best Practices | High Performance Computing,"In this blog post, we help you understand the AWS Batch job termination process and how you may take actions to gracefully terminate a job by capturing SIGTERM signal inside the application. It provides you with an efficient way to exit your Batch jobs. You also get to know about how job timeouts occur, and how the retry operation works with both traditional AWS Batch jobs and array jobs."
Bayesian ML Models at Scale with AWS Batch,https://aws.amazon.com/blogs/hpc/bayesian-ml-models-at-scale-with-aws-batch/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/06/10/hpcblog-124-header.png,14-Jun-22,AWS Batch | Customer Solutions | High Performance Computing,"Ampersand is a data-driven TV advertising technology company that provides aggregated TV audience impression insights and planning on 42 million households, in every media market, across more than 165 networks and apps and in all dayparts (broadcast day segments). The Ampersand Data Science team estimated that building their statistical models would require up to 600,000 physical CPU hours to run, which would not be feasible without using a massively parallel and large-scale architecture in the cloud. AWS Batch enabled Ampersand to compress their time of computation over 500x through massive scaling while optimizing their costs using Amazon EC2 Spot. In this blog post, we will provide an overview of how Ampersand built their TV audience impressions (“impressions”) models at scale on AWS, review the architecture they have been using, and discuss optimizations they conducted to run their workload efficiently on AWS Batch."
Running cost-effective GROMACS simulations using Amazon EC2 Spot Instances with AWS ParallelCluster,https://aws.amazon.com/blogs/hpc/running-gromacs-on-spot-with-checkpointing/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/06/08/hpcblog-69-header.png,9-Jun-22,AWS ParallelCluster | High Performance Computing,"In this blog post, we cover how to run GROMACS – a popular open source designed for simulations of proteins, lipids, and nucleic acids – cost effectively by leveraging EC2 Spot Instances within AWS ParallelCluster. We also show how to checkpoint GROMACS to recover gracefully from possible Spot Instance interruptions."
Introducing the Spack Rolling Binary Cache hosted on AWS,https://aws.amazon.com/blogs/hpc/introducing-the-spack-rolling-binary-cache/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/05/24/hpblog-129-header.png,31-May-22,Announcements | AWS ParallelCluster | High Performance Computing,"Today we’re excited to announce the availability of a new public Spack Binary Cache. In a collaboration, between AWS, E4S, Kitware, and the Lawrence Livermore National Laboratory (LLNL), Spack users now have access to a public build cache hosted on Amazon S3. The use of this Binary Cache will result in up to 20x faster install times for common Spack packages."
Encoding workflow dependencies in AWS Batch,https://aws.amazon.com/blogs/hpc/encoding-workflow-dependencies-in-aws-batch/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/05/11/hpcblog-31-header-1260x630.png,11-May-22,AWS Batch | Best Practices | High Performance Computing,This post covers the different ways you can encode a dependency between basic and array jobs in AWS Batch. We also cover why you may want to encode dependencies outside of Batch altogether using a workflow system like AWS Step Functions or Apache Airflow.
"AWS Batch updates: higher compute utilization, AWS PrivateLink support, and updatable compute environments",https://aws.amazon.com/blogs/hpc/aws-batch-updates-higher-compute-utilization-aws-privatelink-support-and-updatable-compute-environments/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/04/22/hpc-blog-125-header-1260x630.png,25-Apr-22,AWS Batch | High Performance Computing,"In this post, I cover some of the recent updates to AWS Batch, including improvements to job placement, addition of AWS PrivateLink support, and the new capabilities to update your AWS Batch compute environments."
Benchmarking NVIDIA Clara Parabricks Somatic Variant Calling Pipeline on AWS,https://aws.amazon.com/blogs/hpc/benchmarking-nvidia-clara-parabricks-somatic-variant-calling-pipeline-on-aws/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/04/19/hpcblog-114-header-1260x630.png,20-Apr-22,Customer Solutions | High Performance Computing | Life Sciences,"Somatic variants are genetic alterations which are not inherited but acquired during one’s lifespan, for example those that are present in cancer tumors. In this post, we will demonstrate how to perform somatic variant calling from matched tumor and normal genome sequence data, as well as tumor-only whole genome and whole exome datasets using an NVIDIA GPU-accelerated Parabricks pipeline, and compare the results with baseline CPU-based workflows."
AI-based drug discovery with Atomwise and WEKA Data Platform,https://aws.amazon.com/blogs/hpc/ai-based-drug-discovery-with-atomwise-and-weka-data-platform/,,12-Apr-22,Customer Solutions | High Performance Computing,"Drug discovery is an expensive proposition, with a $2.6 billion cost over 10 years and just a 12% success rate. AI promises to significantly improve the success rate by finding small molecule hits for undruggable targets. On the forefront of using AI in drug discovery is Atomwise, with its AtomNet® platform. In this blog, we will lay out the challenges of the drug discovery process, and show how AI/ML startups are solving these challenges using solutions from Atomwise, AWS, and WEKA."
Simcenter STAR-CCM+ price-performance on AWS,https://aws.amazon.com/blogs/hpc/simcenter-star-ccm-price-performance-on-aws/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/03/30/hpcblog-49-fig1.png,5-Apr-22,Amazon EC2 | Customer Solutions | High Performance Computing,"Organizations such as Amazon Prime Air and Joby Aviation use Simcenter STAR-CCM+ for running CFD simulations on AWS so they can reduce product manufacturing cycles and achieve faster times to market. In this post today, we describe the performance and price analysis of running Computational Fluid Dynamics (CFD) simulations using Siemens SimcenterTM STAR-CCM+TM software on AWS HPC clusters."
Data Science workflows at insitro: how redun uses the advanced service features from AWS Batch and AWS Glue,https://aws.amazon.com/blogs/hpc/how-insitro-redun-uses-advanced-aws-features/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/03/23/hpcblog-116-p2-fig2-831x630.png,31-Mar-22,Artificial Intelligence | AWS Batch | AWS Glue | Customer Solutions | High Performance Computing | Life Sciences,"Matt Rasmussen, VP of Software Engineering at insitro, expands on his first post on redun, insitro’s data science tool for bioinformatics, to describe how redun makes use of advanced AWS features. Specifically, Matt describes how AWS Batch’s Array Jobs is used to support workflows with large fan-out, and how AWS Glue’s DynamicFrame is used to run computationally heterogenous workflows with different back-end needs such as Spark, all in the same workflow definition."
Data Science workflows at insitro: using redun on AWS Batch,https://aws.amazon.com/blogs/hpc/data-science-workflows-at-insitro-using-redun-on-aws-batch/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/03/23/hpcblog-116-p1-fig1.png,31-Mar-22,Artificial Intelligence | AWS Batch | AWS Glue | Customer Solutions | High Performance Computing | Life Sciences,"Matt Rasmussen, VP of Software Engineering at insitro describes their recently released, open-source data science framework, redun, which allows data scientists to define complex scientific workflows that scale from their laptop to large-scale distributed runs on serverless platforms like AWS Batch and AWS Glue. I this post, Matt shows how redun lends itself to Bioinformatics workflows which typically involve wrapping Unix-based programs that require file staging to and from object storage. In the next blog post, Matt describes how redun scales to large and heterogenous workflows by leveraging AWS Batch features such as Array Jobs and AWS Glue features such as Glue DynamicFrame."
Creating a digital map of COVID-19 virus for discovery of new treatment compounds,https://aws.amazon.com/blogs/hpc/creating-a-digital-map-of-covid-19-virus-for-discovery-of-new-treatment-compounds/,,22-Mar-22,Customer Solutions | High Performance Computing | Life Sciences,Quantum physics and high-performance computing have slashed research times for a consortium of researchers led by Qubit Pharmaceuticals. This post describes the discovery of chemical substances that may lead to new COVID-19 treatments in only six months using cloud technology.
Migrating to AWS ParallelCluster v3 – Updated CLI interactions,https://aws.amazon.com/blogs/hpc/aws-parallelcluster-v3-updated-cli/,,17-Mar-22,AWS ParallelCluster | High Performance Computing,The AWS ParallelCluster version 3 CLI differs significantly from ParallelCluster version 2. This post provides some guidance on mapping between versions to help you with migrating to ParallelCluster 3. We also summarize new CLI features in ParallelCluster 3 to expose the things you just couldn’t do previously.
Choosing between AWS Batch or AWS ParallelCluster for your HPC Workloads,https://aws.amazon.com/blogs/hpc/choosing-between-batch-or-parallelcluster-for-hpc/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/03/08/hpcblog-78-header-1260x630.png,10-Mar-22,AWS Batch | AWS ParallelCluster | High Performance Computing | Thought Leadership,"It’s an understatement that AWS has a lot of services (more than 200 at the time of this post!). We’re usually the first to point out that there’s more than one way to solve a problem. HPC is no different in this regard, because we offer a choice: customers can run their HPC workloads using AWS […]"
Getting the best OpenFOAM Performance on AWS,https://aws.amazon.com/blogs/hpc/getting-the-best-openfoam-performance-on-aws/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/03/01/hpcblog-14-header-1260x630.png,2-Mar-22,Best Practices | High Performance Computing,"OpenFOAM is one the most widely used Computational Fluid Dynamics (CFD) packages and helps companies in a broad range of sectors (automotive, aerospace, energy, and life-sciences) to conduct research and design new products. In this post, we’ll discuss six practical things you can do as an OpenFOAM user to run your simulations faster and more cost effectively."
"Cloud-native, high throughput grid computing using the AWS HTC-Grid solution",https://aws.amazon.com/blogs/hpc/cloud-native-high-throughput-computing-with-aws-htc-grid/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/02/03/hpcblog-57-fig-2-981x630.png,28-Feb-22,Customer Solutions | High Performance Computing,"We worked with our financial services customers to develop an open-source, scalable, cloud-native, high throughput computing solution on AWS — AWS HTC-Grid. HTC-Grid allows you to submit large volumes of short and long running tasks and scale environments dynamically. In this first blog of a two-part series, we describe the structure of HTC-Grid and its objective to provide a configurable blueprint for HPC grid scheduling on the cloud."
Optimize your Monte Carlo simulations using AWS Batch,https://aws.amazon.com/blogs/hpc/optimizing-monte-carlo-simulations-using-aws-batch/,,23-Feb-22,AWS Batch | High Performance Computing | Technical How-to,"Introduction Monte Carlo methods are a class of methods based on the idea of sampling to study mathematical problems for which analytical solutions may be unavailable. The basic idea is to create samples through repeated simulations that can be used to derive approximations about a quantity we’re interested in, and its probability distribution. In this […]"
Integrating OKTA identity service provider with NICE EnginFrame,https://aws.amazon.com/blogs/hpc/integrating-okta-identity-service-provider-with-nice-enginframe/,,17-Feb-22,High Performance Computing | Technical How-to,"This post by Roberto Meda and Salvo Maccarone covers how you can configure NICE EnginFrame to leverage OKTA  as an identity service provider to support SAML 2.0 single sign on authentication and several other features like multi-factor verification, API access management and multi-device support."
GROMACS performance on Amazon EC2 with Intel Ice Lake processors,https://aws.amazon.com/blogs/hpc/gromacs-performance-on-amazon-ec2-with-intel-ice-lake-processors/,,16-Feb-22,Amazon EC2 | High Performance Computing,"We recently launched two new Amazon EC2 instance families based on Intel’s Ice Lake – the C6i and M6i. These instances provide higher core counts and take advantage of generational performance improvements on Intel’s Xeon scalable processor family architectures. In this post we show how GROMACS performs on these new instance families. We use similar methodologies as for previous posts where we characterized price-performance for CPU-only and GPU instances (Part 1, Part 2, Part 3), providing instance recommendations for different workload sizes."
Introducing AWS ParallelCluster multiuser support via Active Directory,https://aws.amazon.com/blogs/hpc/introducing-aws-parallelcluster-multiuser-support-via-active-directory/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/01/25/hpcblog-115-header-1260x630.png,10-Feb-22,AWS ParallelCluster | High Performance Computing | Launch,"Today we’re announcing the release of AWS ParallelCluster 3.1 which now supports multiuser authentication based on Active Directory (AD). Starting with v3.1.1 clusters can be configured to use an AD domain managed via one of the AWS Directory Service options like Simple AD or AWS Managed Microsoft AD (MSAD). This blog post describes the new feature, and gives an example of a configuration block for ParallelCluster 3 configuration files."
How to Arm a world-leading forecast model with AWS Graviton and Lambda,https://aws.amazon.com/blogs/hpc/how-to-arm-a-world-leading-forecast-model-with-aws-graviton-and-lambda/,,2-Feb-22,AWS Lambda | Customer Solutions | Graviton | High Performance Computing,"The Met Office is the UK’s National Meteorological Service, providing 24×7 world-renowned scientific excellence in weather, climate and environmental forecasts and severe weather warnings for the protection of life and property. They provide forecasts and guidance for the public, to our government and defence colleagues as well as the private sector. As an example, if you’ve been on a plane over Europe, Middle East, or Africa; that plane took off because the Met Office (as one of two World Aviation Forecast Centres) provided a forecast. This article explains one of the ways they use AWS to collect these observations, which has freed them to focus more on top quality delivery for their customers."
Join us for our HPC “Speeds n’ Feeds” event on Feb. 9,https://aws.amazon.com/blogs/hpc/aws-hpc-speeds-n-feeds-event-feb-9/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/01/28/AdobeStock_323435178-2.jpeg,28-Jan-22,Announcements | High Performance Computing,It’s often difficult to keep track of all the announcements AWS is making around HPC. Come and join us on Feb. 9th for a quick overview of the latest and greatest AWS HPC products and services launched over the past year. You will hear directly from the AWS HPC engineers and product managers who have built these exciting new offerings.
Using the ParallelCluster 3 Configuration Converter,https://aws.amazon.com/blogs/hpc/using-the-parallelcluster-3-configuration-converter/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/09/10/pc3-header-1260x630.png,20-Jan-22,AWS ParallelCluster | High Performance Computing,"ParallelCluster 3 was a major release with several changes and a lot of new features. To help get you started migrating your clusters, we describe the config file converter tool which is part of the ParallelCluster (>= v3.0.1) command line interface (CLI)."
Using Spot Instances with AWS ParallelCluster and Amazon FSx for Lustre,https://aws.amazon.com/blogs/hpc/using-spot-instances-with-aws-parallelcluster-and-amazon-fsx-for-lustre/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/01/19/hpcblog-77-header-1260x630.png,19-Jan-22,Amazon FSx for Lustre | AWS ParallelCluster | High Performance Computing,"Processing large amounts of complex data often requires leveraging a mix of different Amazon EC2 instance types. These types of computations also benefit from shared, high performance, scalable storage like Amazon FSx for Lustre. A way to save costs on your analysis is to use Amazon EC2 Spot Instances, which can help to reduce EC2 costs up to 90% compared to On-Demand Instance pricing. This post will guide you in the creation of a fault-tolerant cluster using AWS ParallelCluster. We will explain how to configure ParallelCluster to automatically unmount the Amazon FSx for Lustre filesystem and resubmit the interrupted jobs back into the queue in the case of Spot interruption events."
Custom AMIs with ParallelCluster 3,https://aws.amazon.com/blogs/hpc/custom-amis-with-parallelcluster-3/,,13-Jan-22,AWS ParallelCluster | High Performance Computing,"This blog post shows how you can create and manage custom AMI images for AWS ParallelCluster 3 using the new AMI creation and management process, which is built using EC2 Image Builder."
Running Windows HPC Workloads using HPC Pack in AWS,https://aws.amazon.com/blogs/hpc/running-windows-hpc-workloads-using-hpc-pack-in-aws/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/01/07/hpcblog-59-header-1260x630.png,11-Jan-22,High Performance Computing | Technical How-to | Windows on AWS,"This blog post shows you how to deploy an HPC cluster for Windows workloads. We have provided an AWS CloudFormation template that automates the creation process to deploy an HPC Pack 2019 Windows cluster. This will help you get started quickly to run Windows-based HPC workloads, while leveraging highly scalable, resilient, and secure AWS infrastructure. As an example, we show how to run a sample parametric sweep for EnergyPlus, an open source energy simulation tool maintained by the U.S. Department of Energy’s Building Technology Office."
Accelerating drug discovery with Amazon EC2 Spot Instances,https://aws.amazon.com/blogs/hpc/accelerating-drug-discovery-with-amazon-ec2-spot-instances/,,5-Jan-22,AWS Batch | Customer Solutions | High Performance Computing,"We have been working with a team of researchers at the Max Planck Institute, helping them adopt the AWS cloud for drug research applications in the pharmaceutical industry. In this post, we’ll focus on how the team at Max Planck obtained thousands of EC2 Spot Instances spread across multiple AWS Regions for running their compute intensive simulations in a cost-effective manner, and how their solution will be enhanced further using the new Spot Placement Score API."
Introducing AWS HPC Connector for NICE EnginFrame,https://aws.amazon.com/blogs/hpc/introducing-aws-hpc-connector/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/12/06/hpcblog-111-header-1260x630.png,6-Dec-21,Announcements | High Performance Computing,"Today we’re introducing AWS HPC Connector, a new feature in NICE EnginFrame that allows customers to leverage managed HPC resources on AWS. With this release, EnginFrame provides a unified interface for administrators to make hybrid HPC resources available to their users both on-premises and within AWS. In this post, we’ll provide some context around EnginFrame’s typical use cases, and show how you can use AWS HPC Connector to stand up HPC compute resources on AWS."
How we enabled uncompressed live video with CDI over EFA,https://aws.amazon.com/blogs/hpc/how-we-enabled-uncompressed-live-video-with-cdi-over-efa/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/11/16/hpcblog-75-f2.png,24-Nov-21,AWS Cloud Digital Interface | Customer Solutions | High Performance Computing | Media & Entertainment,"We’re going to take you into the world of broadcast video, and explain how it led to us announcing today the general availability of EFA on smaller instance sizes. For a range of applications, this is going to save customers a lot of money because they no longer need to use the biggest instances in each instance family to get HPC-style network performance. But the story of how we got there involves our Elastic Fabric Adapter (EFA), some difficult problems presented to us by customers in the entertainment industry, and an invention called the Cloud Digital Interface (CDI). And it started not very far from Hollywood."
Benchmarking the NVIDIA Clara Parabricks germline pipeline on AWS,https://aws.amazon.com/blogs/hpc/benchmarking-the-nvidia-clara-parabricks-germline-pipeline-on-aws/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/11/23/hpcblog-50-header-1260x630.png,23-Nov-21,AWS Marketplace | AWS Partner Network | High Performance Computing,This blog provides an overview of NVIDIA’s Clara Parabricks along with a guide on how to use Parabricks within the AWS Marketplace. It focuses on germline analysis for whole genome and whole exome applications using GPU accelerated bwa-mem and GATK’s HaplotypeCaller.
Running a 3.2M vCPU HPC Workload on AWS with YellowDog,https://aws.amazon.com/blogs/hpc/running-a-3-2m-vcpu-hpc-workload-on-aws-with-yellowdog/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/11/22/hpcblog-102-header-1260x630.png,23-Nov-21,High Performance Computing | Life Sciences,"OMass Therapeutics, a biotechnology company identifying medicines against highly validated target ecosystems, used Yellowdog on AWS to analyze and screen 337 million compounds in 7 hours, a task which would have taken two months using an on-premises HPC cluster. YellowDog, based in Bristol in the UK, ran the drug discovery application on an extremely large, multi-region cluster in AWS with the AWS ‘pay-as-you-go’ pricing model. It provided a central, unified interface to monitor and manage AWS Region selection, compute provisioning, job allocation and execution. The entire workload completed in 65 minutes, enabling scientists to start work on analysis the same day, significantly accelerating the drug discovery process. In this post, we’ll discuss the AWS and YellowDog services we deployed, and the mechanisms used to scale to 3.2m vCPUs using multiple EC2 instance types across multiple regions in 33 minutes, running at a 95% utilization rate."
Coming soon: dedicated HPC instances and hybrid functionality,https://aws.amazon.com/blogs/hpc/coming-soon-dedicated-hpc-instances-and-hybrid-functionality/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/11/18/Hpc6a-and-EF-blog.png,18-Nov-21,Amazon EC2 | Announcements | High Performance Computing,"This year, we’ve launched a lot of new capabilities for HPC customers, making AWS the best place for the length and breadth of their workflows. EFA went mainstream and is now available in sixteen instance families for fast fabric capabilities for scaling MPI and NCCL codes. We’ve written deep-dive studies to explore and explain the optimizations that will drive your workloads faster in the cloud than elsewhere. We released a major new version of AWS ParallelCluster with its own API for controlling the cluster lifecycle. AWS Batch became deeply integrated into AWS Step Functions and now supports fair-share scheduling, with multiple levers to control the experience. Today we’re signaling the arrival of a new HPC-dedicated instance family – the Hpc6a – and an enhanced EnginFrame that will bring the best of the cloud and on-premises together in a single interface."
How to manage HPC jobs using a serverless API,https://aws.amazon.com/blogs/hpc/how-to-manage-hpc-jobs-using-a-serverless-api/,,17-Nov-21,Amazon API Gateway | AWS Lambda | AWS ParallelCluster | AWS Serverless Application Model | AWS Systems Manager | High Performance Computing,"HPC systems are traditionally access through a Command Line Interface (CLI) where the users submit and manage their computational jobs. Depending on their experience and sophistication, the CLI can be a daunting experience for users not accustomed in using it. Fortunately, the cloud offers many other options for users to submit and manage their computational jobs. In this blog post we will cover how to create a serverless API to interact with an HPC system in the the cloud built with AWS ParallelCluster."
Using the Slurm REST API to integrate with distributed architectures on AWS,https://aws.amazon.com/blogs/hpc/using-the-slurm-rest-api-to-integrate-with-distributed-architectures-on-aws/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/11/03/hpcblog-53-fig1.png,17-Nov-21,AWS Amplify | AWS ParallelCluster | Customer Solutions | High Performance Computing,"The Slurm Workload Manager by SchedMD is a popular HPC scheduler and is supported by AWS ParallelCluster, an elastic HPC cluster management service offered by AWS. Traditional HPC workflows involve logging into a head node and running shell commands to submit jobs to a scheduler and check job status. Modern distributed systems often use representational […]"
Deep dive into the AWS ParallelCluster 3 configuration file,https://aws.amazon.com/blogs/hpc/deep-dive-into-the-aws-parallelcluster-3-configuration-file/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/09/10/pc3-header-1260x630.png,15-Nov-21,AWS ParallelCluster | High Performance Computing,"In September, we announced the release of AWS ParallelCluster 3, a major release with lots of changes and new features. To help get you started migrating your clusters, we provided the Moving from AWS ParallelCluster 2.x to 3.x guide. We know moving versions can be a quite an undertaking, so we’re augmenting that official documentation with additional color and context on a few key areas. With this blog post, we’ll focus on the configuration file format changes for ParallelCluster 3, and how they map back to the same configuration sections for ParallelCluster 2."
Introducing fair-share scheduling for AWS Batch,https://aws.amazon.com/blogs/hpc/introducing-fair-share-scheduling-for-aws-batch/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/11/09/hpcblog-72-f6.gif,9-Nov-21,Announcements | AWS Batch | High Performance Computing | Launch,"Today we are announcing fair-share scheduling (FSS) for AWS Batch, which provides fine-grain control of the scheduling behavior by using a scheduling policy. With FSS, customers can prevent “unfair” situations caused by strict first-in, first-out scheduling where high priority jobs can’t “jump the queue” without draining other jobs first. You can now balance resource consumption between groups of workloads and have confidence that the shared compute environment is not dominated by a single workload. In this post, we’ll explain how fair-share scheduling works in more detail. You’ll also find a link to a step-by-step workshop at the end of this post, so you can try it out yourself."
"Scaling a read-intensive, low-latency file system to 10M+ IOPs",https://aws.amazon.com/blogs/hpc/scaling-a-read-intensive-low-latency-file-system-to-10m-iops/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/11/02/scaling-read-fs-f1.png,4-Nov-21,Amazon EC2 | Best Practices | High Performance Computing | Storage | Technical How-to,"Many shared file systems are used in supporting read-intensive applications, like financial backtesting. These applications typically exploit copies of datasets whose authoritative copy resides somewhere else. For small datasets, in-memory databases and caching techniques can yield impressive results. However, low latency flash-based scalable shared file systems can provide both massive IOPs and bandwidth. They’re also easy to adopt because of their use of a file-level abstraction. In this post, I’ll share how to easily create and scale a shared, distributed POSIX compatible file system that performs at local NVMe speeds for files opened read-only."
Running 20k simulations in 3 days to accelerate early stage drug discovery with AWS Batch,https://aws.amazon.com/blogs/hpc/running-20k-simulations-in-3-days-with-aws-batch/,,2-Nov-21,AWS Batch | High Performance Computing,"In this blog post, we’ll describe an ensemble run of 20K simulations to accelerate the drug discovery process, while also optimizing for run time and cost. We used two popular open-source packages — GROMACS, which does a molecular dynamics simulations, and pmx, a free-energy calculation package from the Computational Biomolecular Dynamics Group at Max Planck Institute in Germany."
Using AWS Batch Console Support for Step Functions Workflows,https://aws.amazon.com/blogs/hpc/using-aws-batch-console-support-for-step-functions/,,27-Oct-21,AWS Batch | AWS Step Functions | High Performance Computing | Technical How-to,"Last year, we published the Genomics Secondary Analysis Using AWS Step Functions and AWS Batch solution as a companion solution to the Genomics Data Transfer, Analytics, and Machine Learning Using AWS Services whitepaper. Since then, many customers have used the secondary analysis solution to automate their bioinformatics pipelines in AWS. A common pain point expressed […]"
The Convergent Evolution of Grid Computing in Financial Services,https://aws.amazon.com/blogs/hpc/the-convergent-evolution-of-grid-computing-in-financial-services/,,21-Oct-21,Financial Services | High Performance Computing | Thought Leadership,"The Financial Services industry makes significant use of high performance computing (HPC) but it tends to be in the form of loosely coupled, embarrassingly parallel workloads to support risk modelling. The infrastructure tends to scale out to meet ever increasing demand as the analyses look at more and finer grained data. At AWS we’ve helped many customers tackle scaling challenges are noticing some common themes. In this post we describe how HPC teams are thinking about how they deliver compute capacity today, and highlight how we see the solutions converging for the future."
Putting bitrates into perspective,https://aws.amazon.com/blogs/hpc/putting-bitrates-into-perspective/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/09/27/dcv-bitrates-header-1260x630.png,19-Oct-21,Best Practices | High Performance Computing,"Recently, we talked about the advances NICE DCV has made to push pixels from cloud-hosted desktops or applications over the internet even more efficiently than before. Since we published that post on this blog channel, we’ve been asked by several customers whether all this efficient pixel-pushing could lead to outbound data charges moving up on their AWS bill. We decided to try it on your behalf, and share the details with you in this post. The bottom line? The charges are unlikely to be significant unless you’re doing intensive streaming (such as gaming) and other cost optimizations (like AWS Instance Savings Plans) that will have more impact on your bill."
Running GROMACS on GPU instances: multi-node price-performance,https://aws.amazon.com/blogs/hpc/running-gromacs-on-gpu-instances-multi-node-price-performance/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/10/14/gromacs-gpu-p3-f4.png,14-Oct-21,Amazon EC2 | AWS ParallelCluster | Best Practices | High Performance Computing,This three-part series of posts cover the price performance characteristics of running GROMACS on Amazon Elastic Compute Cloud (Amazon EC2) GPU instances. Part 1 covered some background no GROMACS and how it utilizes GPUs for acceleration. Part 2 covered the price performance of GROMACS on a particular GPU instance family running on a single instance. […]
Running GROMACS on GPU instances: single-node price-performance,https://aws.amazon.com/blogs/hpc/running-gromacs-on-gpu-instances-single-node-price-performance/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/10/13/gromacs-gpu-p2-f4-1.png,13-Oct-21,Amazon EC2 | Best Practices | High Performance Computing,This three-part series of posts cover the price performance characteristics of running GROMACS on Amazon Elastic Compute Cloud (Amazon EC2) GPU instances. Part 1 covered some background no GROMACS and how it utilizes GPUs for acceleration. This post (Part 2) covers the price performance of GROMACS on a particular GPU instance family running on a […]
Running GROMACS on GPU instances,https://aws.amazon.com/blogs/hpc/running-gromacs-on-gpu-instances/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/10/06/gromacs-gpu-p1-f2.png,12-Oct-21,Amazon EC2 | Best Practices | High Performance Computing,"Comparing the performance of real applications across different Amazon Elastic Compute Cloud (Amazon EC2) instance types is the best way we’ve found for finding optimal configurations for HPC applications here at AWS. Previously, we wrote about price-performance optimizations for GROMACS that showed how the GROMACS molecular dynamics simulation runs on single instances, and how it […]"
AWS Batch Dos and Don’ts: Best Practices in a Nutshell,https://aws.amazon.com/blogs/hpc/aws-batch-best-practices/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/09/27/batch-best-practices-header-1260x630.png,4-Oct-21,AWS Batch | Best Practices | High Performance Computing,"AWS Batch is a service that enables scientists and engineers to run computational workloads at virtually any scale without requiring them to manage a complex architecture. In this blog post, we share a set of best practices and practical guidance devised from our experience working with customers in running and optimizing their computational workloads. The readers will learn how to optimize their costs with Amazon EC2 Spot on AWS Batch, how to troubleshoot their architecture should an issue arise and how to tune their architecture and containers layout to run at scale."
Running the Harmonie numerical weather prediction model on AWS,https://aws.amazon.com/blogs/hpc/running-the-harmonie-numerical-weather-prediction-on-aws/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/09/28/harmonie-header-1260x630.png,30-Sep-21,AWS ParallelCluster | Customer Solutions | High Performance Computing,"The Danish Meteorological Institute (DMI) is responsible for running atmospheric, climate and ocean models covering the kingdom of Denmark. We worked together with the DMI to port and run a full numerical weather prediction (NWP) cycling dataflow with the Harmonie Numerical Weather Prediction (NWP) model to AWS. You can find a report of the porting and operational experience in the ACCORD community newsletter. In this blog post, we expand on that report to present the initial timing results from running the forecast component of Harmonie model on AWS. We also present these as-is timing results together with as-is timings attained on the supercomputing systems based on Cray XC40 and Intel Xeon based Cray XC50."
Cost-optimization on Spot Instances using checkpoint for Ansys LS-DYNA,https://aws.amazon.com/blogs/hpc/cost-optimization-on-spot-instances-using-checkpoints-for-ansys-ls-dyna/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/06/11/ls-dyna-spot-header-1260x630.png,27-Sep-21,Amazon EC2 | AWS ParallelCluster | High Performance Computing | Technical How-to,"A major portion of the costs incurred for running Finite Element Analyses (FEA) workloads on AWS comes from the usage of Amazon EC2 instances. Amazon EC2 Spot Instances offer a cost-effective architectural choice, allowing you to take advantage of unused EC2 capacity for up to a 90% discount compared to On-Demand Instance prices. In this post, we describe how you 0can run fault-tolerant FEA workloads on Spot Instances using Ansys LS-DYNA’s checkpointing and auto-restart utility."
Quantum Chemistry Calculation with FHI-aims code on AWS,https://aws.amazon.com/blogs/hpc/quantum-chemistry-calculation-on-aws/,,24-Sep-21,AWS ParallelCluster | Customer Solutions | Graviton | High Performance Computing | Research,"This article was contributed by Dr. Fabio Baruffa, Sr. HPC and QC Solutions Architect at AWS, and Dr. Jesús Pérez Ríos, Group Leader at the Fritz Haber Institute, Max-Planck Society.   Introduction Quantum chemistry – the study of the inherently quantum interactions between atoms forming part of molecules – is a cornerstone of modern chemistry. […]"
Virtual Screening of Novel Active Drug Compounds on AWS with Orion®,https://aws.amazon.com/blogs/hpc/virtual-screening-of-drug-compounds-with-orion/,,23-Sep-21,AWS Partner Network | High Performance Computing | Life Sciences,"Computer-aided drug discovery (CADD) has been a key player in lowering the cost and speeding up the timeline for drug development. CADD uses high performance computing (HPC) resources to virtually screen databases with billions of molecules. It can speed up the searching of potential drug molecules, and filter out molecules and compounds that are unsuitable. OpenEye Scientific developed Orion®, a cloud-based molecular design platform for CADD. Orion provides computational chemists with virtually unlimited HPC resources. These include data visualization, collaboration, and workflow management tools that help them perform calculations more efficiently. In this post, we describe the Orion architecture on AWS, and it’s capabilities to address the challenges in drug development."
Call for participation: PRACE Winter School,https://aws.amazon.com/blogs/hpc/call-for-participation-prace-winter-school/,,16-Sep-21,Announcements | Education | High Performance Computing | Research,"The Inter University Computing Centre (IUCC) in Israel and AWS have joined forces to train Researchers and Research Software Engineers (RSEs) in the use of AWS for High Performance Computing (HPC) at the PRACE Winter School, 7-9 December 2021, and we’re calling for interested groups to sign up and join us."
New: Introducing AWS ParallelCluster 3,https://aws.amazon.com/blogs/hpc/introducing-aws-parallelcluster-3/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/09/10/pc3-header-1260x630.png,10-Sep-21,Announcements | AWS ParallelCluster | High Performance Computing,"Running HPC workloads, like computational fluid dynamics (CFD), molecular dynamics, or weather forecasting typically involves a lot of moving parts. You need a hundreds or thousands of compute cores, a job scheduler for keeping them fed, a shared file system that’s tuned for throughput or IOPS (or both), loads of libraries, a fast network, and […]"
Supporting climate model simulations to accelerate climate science,https://aws.amazon.com/blogs/hpc/supporting-climate-model-simulations-to-accelerate-climate-science/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/09/02/asdi-header-1260x630.png,3-Sep-21,Amazon FSx for Lustre | AWS ParallelCluster | Customer Solutions | High Performance Computing | Public Sector | Research | Sustainability | Thought Leadership,"The Amazon Sustainability Data Initiative (ASDI), AWS is donating cloud resources, technical support, and access to scalable infrastructure and fast networking providing high performance computing solutions to support simulations of near-term climate using the National Center for Atmospheric Research (NCAR) Community Earth System Model Version 2 (CESM2) and its Whole Atmosphere Community Climate Model (WACCM). In collaboration with ASDI, AWS, and SilverLining, a nonprofit dedicated to ensuring a safe climate, the National Center for Atmospheric Research (NCAR) will run an ensemble of 30 climate-model simulations on AWS. The climate runs will simulate the Earth system over the period of years 2022-2070 under a median scenario for warming and make them available through the AWS Open Data Program. The simulation work will demonstrate the ability to use cloud infrastructure to advance climate models in support of robust scientific studies by researchers around the world and aims to accelerate and democratize climate science."
High Burst CPU Compute for Monte Carlo Simulations on AWS,https://aws.amazon.com/blogs/hpc/high-burst-cpu-compute-for-monte-carlo-simulations-on-aws/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/08/26/mc-playtech-header-1260x630.png,1-Sep-21,Amazon API Gateway | AWS Lambda | Customer Solutions | High Performance Computing | Serverless,"Playtech mathematicians and game designers need accurate, detailed game play simulation results to create fun experiences for players. While software developers have been able to iterate on code in an agile manner for many years, for non-analytical solutions, mathematicians have had to rely on slow CPU-bound Monte-Carlo simulations, waiting, as software engineers once did, many hours or overnight to get the results of their latest changes. These statistics are also required as evidence of game fairness in the highly regulated online gaming business. Playtech has developed an AWS Lambda Serverless based solution that provides massive burst compute performance that allows game simulations in minutes rather than hours. This post goes into the details of the architecture, as well as some examples of using the system in our development and operations."
Stion – a Software as a Service for Cryo-EM data processing on AWS,https://aws.amazon.com/blogs/hpc/stion-a-saas-for-cryo-em-data-processing-on-aws/,,17-Aug-21,Customer Solutions | High Performance Computing,"This post was written by Swapnil Bhatkar, Cloud Engineer, NREL in collaboration with Edward Eng Ph.D. and Micah Rapp Ph.D, both SEMC/NYSBC, and Evan Bollig Ph.D. and Aniket Deshpande, both AWS. Introduction Cryo-electron microscopy (Cryo-EM) technology allows biomedical researchers to image frozen biological molecules, such as proteins, viruses and nucleic acids, and obtain structures of […]"
Price-Performance Analysis of Amazon EC2 GPU Instance Types using NVIDIA’s GPU optimized seismic code,https://aws.amazon.com/blogs/hpc/price-performance-analysis-of-gpu-instance-types-using-nvidias-gpu-optimized-seismic-code/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/08/09/hpc-blog-header-price-perf-1-1260x630.png,12-Aug-21,Energy (Oil & Gas) | High Performance Computing,"Seismic imaging is the process of positioning the Earth’s subsurface reflectors. It transforms the seismic data recorded in time at the Earth’s surface to an image of the Earth’s subsurface. This is done by back-propagating data from time to space in a given velocity model. Kirchhoff depth migration is a well-known technique used in geophysics for seismic imaging. Kirchhoff time and depth migration produce an image with higher resolution and generate an image of the subsurface for a subset class of the data, providing valuable information about the petrophysical properties of the rocks and helps to determine how accurate the velocity model is. This blog post looks at the price-performance characteristics computing Kirchhoff migration methods on GPUs using Nvidia’s GPU-optimized code."
Bare metal performance with the AWS Nitro System,https://aws.amazon.com/blogs/hpc/bare-metal-performance-with-the-aws-nitro-system/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/08/05/bm-nitro-header-1260x620.png,5-Aug-21,Amazon EC2 | Best Practices | Compute | High Performance Computing,"High Performance Computing (HPC) is known as a domain where applications are well-optimized to get the highest performance possible on a platform. Unsurprisingly, a common question when moving a workload to AWS is what performance difference there may be from an existing on-premises “bare metal” platform. This blog will show the performance differential between “bare metal” instances and instances that use the AWS Nitro hypervisor is negligible for the evaluated HPC workloads."
Pushing pixels with NICE DCV,https://aws.amazon.com/blogs/hpc/pushing-pixels-with-nice-dcv/,,30-Jul-21,High Performance Computing | Thought Leadership,"NICE DCV, our high-performance, low-latency remote-display protocol, was originally created for scientists and engineers who ran large workloads on far-away supercomputers, but needed to visualize data without moving it. Pushing pixels over limited bandwidth across the globe has been the goal of the DCV team since 2007. DCV was able to make very frugal use of very scarce bandwidth, because it was super lean, used data-compression techniques and quickly adopted cutting-edge technologies of the time from GPUs (this is HPC, after all, we left nothing on the table when it came to exploiting new gadgets). This allowed the team to create a super light-weight visualization package that could stream pixels over almost any network. Fast forward to the 2020s, and a generation of gamers, artists, and film-makers all want to do the same thing as HPC researchers- only this time there are way more pixels, because we now have HD and 4k (and some people have multiple), and for most of them, it’s 60 frames per second, or it’s not worth having. Today we have around 12x the number of pixels, and around 3x the frame rate compared to TV of circa 2007. Fortunately, networking improved a lot in that time: a high-end user’s broadband connection grew around 60x in bandwidth, but the 120x growth in computing power really tipped the balance in favor of bringing remote streaming to the masses. Still, physics remains, meaning the latency forced on us by the curvature of the earth and the speed of light, is still a challenge. We still haven’t beaten physics, but we’re making up for it by building our own global fiber network and adding more machinery (and in local and wavelength zones) to get closer to more customers as soon as we can."
Scalable and Cost-Effective Batch Processing for ML workloads with AWS Batch and Amazon FSx,https://aws.amazon.com/blogs/hpc/ml-training-with-aws-batch-and-amazon-fsx/,,23-Jul-21,Amazon Machine Learning | AWS Batch | High Performance Computing | Technical How-to,"Batch processing is a common need across varied machine learning use cases such as video production, financial modeling, drug discovery, or genomic research. The elasticity of the cloud provides efficient ways to scale and simplify batch processing workloads while cutting costs. In this post, you’ll learn a scalable and cost-effective approach to configure AWS Batch Array jobs to process datasets that are stored on Amazon S3 and presented to compute instances with Amazon FSx for Lustre."
"In the search for performance, there’s more than one way to build a network",https://aws.amazon.com/blogs/hpc/in-the-search-for-performance-theres-more-than-one-way-to-build-a-network/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/06/14/efa-header-1260x630.png,22-Jun-21,High Performance Computing | Thought Leadership,"AWS worked backwards from an essential problem in HPC networking (MPI ranks need to exchange lots of data quickly) and found a different solution for our unique circumstances, without trading off the things customers love the most about cloud: that you can run virtually any application, at scale, and right away. Find out more about how Elastic Fabric Adapter (EFA) can help your HPC workloads scale on AWS."
Getting started with containers in HPC at ISC’21,https://aws.amazon.com/blogs/hpc/containers-in-hpc-workshop-at-isc21/,,18-Jun-21,Announcements | High Performance Computing,"Containers are rapidly maturing within the high performance computing (HPC) community and we’re excited to be part of the movement: listening to what customers have to say and feeding this back to both the community and our own product and service teams. Containerization has the potential to unblock HPC environments, so AWS ParallelCluster and container-native schedulers like AWS Batch are moving quickly to reflect the best practices developed by the community and our customers. This year is the seventh consecutive year we are hosting the ‘High Performance Container Workshop’ at ISC High Performance 2021 conference (ISC’21). The workshop will be taking place on July 2nd at 2PM CEST (7AM CST). The full program for the workshop is available on the High Performance Container Workshop page at https://hpcw.github.io/"
Building highly-available HPC infrastructure on AWS,https://aws.amazon.com/blogs/hpc/highly-available-hpc-infrastructure-on-aws/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/06/03/ha-hpc-header-1260x630.png,3-Jun-21,Amazon Aurora | AWS ParallelCluster | High Performance Computing,"In this blog post, we will explain how to launch highly available HPC clusters across an AWS Region. The solution is deployed using the AWS Cloud Developer Kit (AWS CDK), a software development framework for defining cloud infrastructure in code and provisioning it through AWS CloudFormation, hiding the complexity of integration between the components."
Accelerating research and development of new medical treatments with HPC on AWS,https://aws.amazon.com/blogs/hpc/accelerating-research-and-development-of-new-medical-treatments-with-hpc-on-aws/,,28-May-21,Compliance | Customer Solutions | High Performance Computing | Public Sector,"Today, more than 290,000 researchers in France are working to provide better support and care for patients through modern medical treatment. To fulfill their mission, these researchers must be equipped with powerful tools. At AWS, we believe that technology has a critical role to play in medical research. Why? Because technology can take advantage of the significant amount of data generated in the healthcare system and in the research community to enable opportunities for more accurate diagnoses, and better treatments for many existing and future diseases. To support elite research in France, we are proud to be a sponsor of two French organizations:  Gustave Roussy and Sorbonne University. AWS is providing them with the computing power and machine learning technologies needed to accelerate cancer research and develop a treatment for COVID-19."
Training forecasters to warn severe hazardous weather on AWS,https://aws.amazon.com/blogs/hpc/training-forecasters-to-warn-severe-hazardous-weather-on-aws/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/05/18/RITC-CollabMap-1101x630.png,18-May-21,Customer Solutions | High Performance Computing,"Training users on how to use high performance computing resources — and the data that comes out as a result of those analyses — is an essential function of most research organizations. Having a robust, scalable, and easy-to-use platform for on-site and remote training is becoming a requirement for creating a community around your research mission. A great example of this comes from the NOAA National Weather Service Warning Decision Training Division (WDTD), which develops and delivers training on the integrated elements of the hazardous weather warning process within a National Weather Service (NWS) forecast office. In collaboration with the University of Oklahoma’s Cooperative Institute for Mesoscale Meteorological Studies (OU/CIMMS), WDTD conducts its flagship course, the Radar and Applications Course (RAC), for forecasters issuing warnings for flash floods, severe thunderstorms, and tornadoes. Trainees learn the warning process, the science and application of conceptual models, and technical aspects of analyzing radar and other weather data in the Advanced Weather Interactive Processing System (AWIPS). "
AWS joins Arm to support Arm-HPC hackathon this summer,https://aws.amazon.com/blogs/hpc/aws-arm-hpc-hackathon-2021/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/05/11/arm-aws-hackathon-1260x630.png,12-May-21,Announcements | Graviton | High Performance Computing,"Arm and AWS are calling all grad students and post-docs who want to gain experience advancing the adoption of the Arm architecture in HPC to join a world-wide community effort lead by the Arm HPC User’s Group (A-HUG). The event will take the form of a hackathon this summer and is aimed at getting open-source HPC codes to build and run fast on Arm-based processors, specifically AWS Graviton2. To make it a bit more exciting, A-HUG will be awarding an Apple M1 MacBook to each member of the team (max. 4 people) that contributes the most back to the Arm HPC community."
Numerical weather prediction on AWS Graviton2,https://aws.amazon.com/blogs/hpc/numerical-weather-prediction-on-aws-graviton2/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/05/10/wfr-g2-header-1260x630.png,10-May-21,Amazon FSx for Lustre | AWS ParallelCluster | Best Practices | Graviton | High Performance Computing,"The Weather Research and Forecasting (WRF) model is a numerical weather prediction (NWP) system designed to serve both atmospheric research and operational forecasting needs. With the release of Arm-based AWS Graviton2 Amazon Elastic Compute Cloud (EC2) instances, a common question has been how these instances perform on large-scale NWP workloads. In this blog, we will present results from a standard WRF benchmark simulation and compare across three different instance types."
Reader Question: What is the difference between canceling and terminating a job in AWS Batch?,https://aws.amazon.com/blogs/hpc/reader-question-what-is-the-difference-between-canceling-and-terminating-a-job-in-aws-batch/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/05/03/batch-cancel_term-header-1260x630.png,3-May-21,AWS Batch | Best Practices | High Performance Computing,"A customer asked us what is the difference between the CancelJob and TerminateJob API calls in AWS Batch. This post provides an overview of AWS Batch job states, and how these two API calls effect the job requests that you have submitted."
A VDI solution with EnginFrame and NICE DCV Session Manager built with AWS CDK,https://aws.amazon.com/blogs/hpc/a-vdi-solution-with-enginframe-and-nice-dcv-session-manager-built-with-aws-cdk/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/04/26/vdi-header-1-1260x630.png,23-Apr-21,High Performance Computing | Technical How-to,"This post was written by Dario La Porta, AWS Professional Services Senior Consultant for HPC. Customers across a wide range of industries such as energy, life sciences, and design and engineering are facing challenges in managing and analyzing their data. Not only is the amount and velocity of data increasing, but so is the complexity and […]"
Introducing support for per-job Amazon EFS volumes in AWS Batch,https://aws.amazon.com/blogs/hpc/introducing-support-for-per-job-amazon-efs-volumes-in-aws-batch/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/04/05/batch-efs-header-1260x630.png,5-Apr-21,Amazon Elastic Container Service | Amazon Elastic File System (EFS) | Announcements | AWS Batch | AWS Fargate | High Performance Computing | Technical How-to,Large-scale data analysis usually involves some multi-step process where the output of one job acts as the input of subsequent jobs. Customers using AWS Batch for data analysis want a simple and performant storage solution to share with and between jobs. We are excited to announce that customers can now use Amazon Elastic File System (Amazon […]
GROMACS price-performance optimizations on AWS,https://aws.amazon.com/blogs/hpc/gromacs-price-performance-optimizations-on-aws/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/04/01/gromacs-header-image-1-1260x630.png,31-Mar-21,Advanced (300) | Amazon EC2 | AWS ParallelCluster | Best Practices | Customer Solutions | Graviton | High Performance Computing | Life Sciences,"Molecular dynamics (MD) is a simulation method for analyzing the movement and tracing trajectories of atoms and molecules where the dynamics of a system evolve over time. MD simulations are used across various domains such as material sciences, biochemistry, biophysics and are typically used in two broad ways to study a system. The importance of […]"
Running finite element analysis using Simcenter Nastran on AWS,https://aws.amazon.com/blogs/hpc/running-finite-element-analysis-using-simcenter-nastran-on-aws/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/03/31/nastran-header-1260x630.png,31-Mar-21,Amazon EC2 | AWS Cloud9 | AWS ParallelCluster | Best Practices | Customer Solutions | High Performance Computing,"This post was written by Dnyanesh Digraskar, Sr. Partner Solutions Architect for HPC at AWS and co-authored by Wei Zhang and Ravi Gupta, Sr Software Engineers for Simcenter Nastran at Siemens. Introduction In this blog, we demonstrate the deployment, performance, and price comparisons of Simcenter Nastran for three finite element analysis (FEA) based use cases […]"
Simplify HPC cluster usage with AWS Cloud9 and AWS ParallelCluster,https://aws.amazon.com/blogs/hpc/simplify-hpc-cluster-usage-with-aws-cloud9-and-aws-parallelcluster/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/03/31/cloud9-pcl-header-1260x630.png,31-Mar-21,AWS Cloud9 | AWS ParallelCluster | Customer Solutions | High Performance Computing | Technical How-to,"This post was written by Benjamin Meyer, AWS Solutions Architect When companies and labs start their high performance computing (HPC) journey in the AWS Cloud, it’s not only because they’re in search of elasticity and scale – they’re also in search of new tools and environments. Initially this can appear challenging as there are many […]"
Welcome to the AWS HPC Blog,https://aws.amazon.com/blogs/hpc/welcome-to-the-aws-hpc-blog/,https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2021/03/31/intro-header-1-1260x630.png,31-Mar-21,Amazon EC2 | Announcements | AWS Batch | AWS ParallelCluster | Compute | High Performance Computing,"This post is written by Deepak Singh, Vice President of Compute Services. At AWS, we love working with customers to solve their toughest challenges. High performance computing (HPC) is one of those challenges that pushes against the boundaries of AWS performance at scale. HPC is also a personal interest of mine, as I came to […]"